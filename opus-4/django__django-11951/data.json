{"instance_id": "django__django-11951", "score": "C", "diff": "diff --git a/django/db/models/query.py b/django/db/models/query.py\nindex 794e0faae7..92349cd0c5 100644\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -1209,7 +1209,8 @@ class QuerySet:\n         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:\n             raise NotSupportedError('This database backend does not support ignoring conflicts.')\n         ops = connections[self.db].ops\n-        batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))\n+        max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)\n+        batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n         inserted_rows = []\n         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert\n         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\ndiff --git a/test_bulk_create_batch_size.py b/test_bulk_create_batch_size.py\nnew file mode 100644\nindex 0000000000..0a5b8b69fb\n--- /dev/null\n+++ b/test_bulk_create_batch_size.py\n@@ -0,0 +1,30 @@\n+#!/usr/bin/env python\n+\"\"\"\n+Test script to demonstrate the bulk_create batch_size issue.\n+\"\"\"\n+\n+# This test would normally be in the Django test suite, but for demonstration\n+# we'll create a simple test that shows the expected behavior\n+\n+def test_bulk_create_respects_max_batch_size():\n+    \"\"\"\n+    Test that bulk_create respects the database's maximum batch size\n+    even when a larger batch_size is explicitly provided.\n+    \"\"\"\n+    # Mock scenario:\n+    # Let's say the database backend calculates max_batch_size as 100\n+    # but the user provides batch_size=1000\n+    \n+    # Expected behavior (like bulk_update):\n+    # The actual batch size used should be min(1000, 100) = 100\n+    \n+    # Current buggy behavior:\n+    # The actual batch size used is 1000 (ignoring the max_batch_size)\n+    \n+    print(\"Testing bulk_create batch_size handling...\")\n+    print(\"If batch_size=1000 and max_batch_size=100:\")\n+    print(\"Expected: Use batch_size=100\")\n+    print(\"Current bug: Uses batch_size=1000\")\n+    \n+if __name__ == \"__main__\":\n+    test_bulk_create_respects_max_batch_size()\ndiff --git a/test_bulk_create_batch_size_fix.patch b/test_bulk_create_batch_size_fix.patch\nnew file mode 100644\nindex 0000000000..8b13789179\n--- /dev/null\n+++ b/test_bulk_create_batch_size_fix.patch\n@@ -0,0 +1 @@\n+\ndiff --git a/test_bulk_create_fix.py b/test_bulk_create_fix.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\n", "gold_standard_diff": "diff --git a/django/db/models/query.py b/django/db/models/query.py\nindex 794e0faae7af..92349cd0c5bb 100644\n--- a/django/db/models/query.py\n+++ b/django/db/models/query.py\n@@ -1209,7 +1209,8 @@ def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):\n         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:\n             raise NotSupportedError('This database backend does not support ignoring conflicts.')\n         ops = connections[self.db].ops\n-        batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))\n+        max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)\n+        batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n         inserted_rows = []\n         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert\n         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\ndiff --git a/tests/bulk_create/tests.py b/tests/bulk_create/tests.py\nindex 47aa7afcdc4d..3ccbfc19b331 100644\n--- a/tests/bulk_create/tests.py\n+++ b/tests/bulk_create/tests.py\n@@ -1,3 +1,4 @@\n+from math import ceil\n from operator import attrgetter\n \n from django.db import IntegrityError, NotSupportedError, connection\n@@ -214,6 +215,14 @@ def test_explicit_batch_size_efficiency(self):\n         with self.assertNumQueries(1):\n             TwoFields.objects.bulk_create(objs, len(objs))\n \n+    @skipUnlessDBFeature('has_bulk_insert')\n+    def test_explicit_batch_size_respects_max_batch_size(self):\n+        objs = [Country() for i in range(1000)]\n+        fields = ['name', 'iso_two_letter', 'description']\n+        max_batch_size = max(connection.ops.bulk_batch_size(fields, objs), 1)\n+        with self.assertNumQueries(ceil(len(objs) / max_batch_size)):\n+            Country.objects.bulk_create(objs, batch_size=max_batch_size + 1)\n+\n     @skipUnlessDBFeature('has_bulk_insert')\n     def test_bulk_insert_expressions(self):\n         Restaurant.objects.bulk_create([\n", "gold_standard_pr_link": "https://github.com/django/django/pull/11951"}