{"instance_id": "scikit-learn__scikit-learn-14087", "score": "I", "diff": "diff --git a/debug_issue_deeper.py b/debug_issue_deeper.py\nnew file mode 100644\nindex 000000000..85afb99d4\n--- /dev/null\n+++ b/debug_issue_deeper.py\n@@ -0,0 +1,46 @@\n+import numpy as np\n+from sklearn.linear_model import LogisticRegressionCV\n+from sklearn.linear_model.logistic import _check_multi_class\n+import traceback\n+\n+# Reproduce the issue\n+np.random.seed(29)\n+X = np.random.normal(size=(1000, 3))\n+beta = np.random.normal(size=3)\n+intercept = np.random.normal(size=None)\n+y = np.sign(intercept + X @ beta)\n+\n+# Check what multi_class resolves to  \n+print(\"For binary classification with saga solver:\")\n+print(\"multi_class='auto' resolves to:\", _check_multi_class('auto', 'saga', 2))\n+\n+# Let's manually check the flow by inspecting the fitted object\n+print(\"\\nTesting with explicit multi_class='ovr'...\")\n+try:\n+    clf = LogisticRegressionCV(\n+        cv=5,\n+        solver='saga',\n+        tol=1e-2,\n+        refit=False,\n+        multi_class='ovr'\n+    )\n+    clf.fit(X, y)\n+    print(\"SUCCESS with ovr!\")\n+except Exception as e:\n+    print(\"ERROR with ovr:\", type(e).__name__, str(e))\n+    traceback.print_exc()\n+\n+print(\"\\nTesting with explicit multi_class='multinomial'...\")\n+try:\n+    clf = LogisticRegressionCV(\n+        cv=5,\n+        solver='saga',\n+        tol=1e-2,\n+        refit=False,\n+        multi_class='multinomial'  # This should trigger the error\n+    )\n+    clf.fit(X, y)\n+    print(\"SUCCESS with multinomial!\")\n+except Exception as e:\n+    print(\"ERROR with multinomial:\", type(e).__name__, str(e))\n+    traceback.print_exc()\ndiff --git a/debug_shapes.py b/debug_shapes.py\nnew file mode 100644\nindex 000000000..daf4f49b7\n--- /dev/null\n+++ b/debug_shapes.py\n@@ -0,0 +1,54 @@\n+import numpy as np\n+from sklearn.linear_model import LogisticRegressionCV\n+from sklearn.model_selection import StratifiedKFold\n+\n+# Reproduce the issue\n+np.random.seed(29)\n+X = np.random.normal(size=(1000, 3))\n+beta = np.random.normal(size=3)\n+intercept = np.random.normal(size=None)\n+y = np.sign(intercept + X @ beta)\n+\n+# Check binary vs multiclass\n+print(\"Binary classification test:\")\n+print(\"Unique classes:\", np.unique(y))\n+\n+# Reproduce the path taken internally\n+from sklearn.preprocessing import LabelEncoder\n+label_encoder = LabelEncoder().fit(y)\n+y_encoded = label_encoder.transform(y)\n+classes = label_encoder.classes_\n+encoded_labels = label_encoder.transform(label_encoder.classes_)\n+n_classes = len(encoded_labels)\n+\n+print(\"n_classes before adjustment:\", n_classes)\n+\n+# This is what happens internally for binary case\n+if n_classes == 2:\n+    n_classes = 1\n+    encoded_labels = encoded_labels[1:]\n+    classes = classes[1:]\n+    \n+print(\"n_classes after adjustment:\", n_classes)\n+print(\"encoded_labels:\", encoded_labels)\n+\n+# Test multinomial case with 3 classes\n+print(\"\\n\\nMulticlass test:\")\n+y_multi = np.random.choice([0, 1, 2], size=1000)\n+print(\"Unique classes:\", np.unique(y_multi))\n+\n+print(\"\\nTesting multiclass LogisticRegressionCV with refit=False...\")\n+try:\n+    clf = LogisticRegressionCV(\n+        cv=5,\n+        solver='saga',\n+        tol=1e-2,\n+        refit=False,\n+        multi_class='multinomial'\n+    )\n+    clf.fit(X, y_multi)\n+    print(\"SUCCESS: No error thrown!\")\n+    print(\"Coefficients shape:\", clf.coef_.shape)\n+except Exception as e:\n+    print(\"ERROR:\", type(e).__name__)\n+    print(\"Message:\", str(e))\ndiff --git a/test_fix.py b/test_fix.py\nnew file mode 100644\nindex 000000000..69619f321\n--- /dev/null\n+++ b/test_fix.py\n@@ -0,0 +1,35 @@\n+import numpy as np\n+\n+# Check if Cs_ indexing works because it's an array\n+Cs_ = np.array([0.1, 1.0, 10.0])\n+best_indices = np.array([1, 2, 0, 1, 2])  # 5 folds, different best indices\n+best_indices_C = best_indices % len(Cs_)\n+\n+print(\"Cs_:\", Cs_)\n+print(\"best_indices_C:\", best_indices_C)\n+print(\"Cs_[best_indices_C]:\", Cs_[best_indices_C])\n+print(\"np.mean(Cs_[best_indices_C]):\", np.mean(Cs_[best_indices_C]))\n+\n+# Now check l1_ratios_ issue\n+print(\"\\n--- l1_ratios_ issue ---\")\n+l1_ratios_ = [None]\n+best_indices_l1 = best_indices // len(Cs_)\n+\n+print(\"l1_ratios_:\", l1_ratios_)\n+print(\"best_indices_l1:\", best_indices_l1)\n+\n+# This is what causes the error\n+try:\n+    result = np.mean(l1_ratios_[best_indices_l1])\n+    print(\"Direct indexing result:\", result)\n+except Exception as e:\n+    print(\"ERROR with direct indexing:\", type(e).__name__, str(e))\n+\n+# Fix for when l1_ratios_ = [None]\n+if l1_ratios_ == [None]:\n+    print(\"Fix: Since no l1_ratio is used, using None\")\n+    result = None\n+else:\n+    l1_ratios_array = np.asarray(l1_ratios_)\n+    result = np.mean(l1_ratios_array[best_indices_l1])\n+print(\"Fixed result:\", result)\ndiff --git a/test_logistic_cv_refit_false.py b/test_logistic_cv_refit_false.py\nnew file mode 100644\nindex 000000000..303fc6e02\n--- /dev/null\n+++ b/test_logistic_cv_refit_false.py\n@@ -0,0 +1,32 @@\n+import numpy as np\n+from sklearn.linear_model import LogisticRegressionCV\n+\n+# Reproduce the issue\n+np.random.seed(29)\n+X = np.random.normal(size=(1000, 3))\n+beta = np.random.normal(size=3)\n+intercept = np.random.normal(size=None)\n+y = np.sign(intercept + X @ beta)\n+\n+# First let's check what multi_class is being used\n+clf_test = LogisticRegressionCV()\n+print(\"Default multi_class:\", clf_test.multi_class)\n+print(\"Number of unique classes:\", len(np.unique(y)))\n+\n+print(\"\\nTesting LogisticRegressionCV with refit=False...\")\n+try:\n+    clf = LogisticRegressionCV(\n+        cv=5,\n+        solver='saga',\n+        tol=1e-2,\n+        refit=False\n+    )\n+    clf.fit(X, y)\n+    print(\"SUCCESS: No error thrown!\")\n+    print(\"Coefficients shape:\", clf.coef_.shape)\n+    print(\"Intercept shape:\", clf.intercept_.shape)\n+except Exception as e:\n+    print(\"ERROR:\", type(e).__name__)\n+    print(\"Message:\", str(e))\n+    import traceback\n+    traceback.print_exc()\n", "gold_standard_diff": "diff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex e588121b9928c..b945496953f36 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -12,6 +12,12 @@ Version 0.21.3\n Changelog\n ---------\n \n+:mod:`sklearn.linear_model`\n+...........................\n+- |Fix| Fixed a bug in :class:`linear_model.LogisticRegressionCV` where\n+  ``refit=False`` would fail depending on the ``'multiclass'`` and\n+  ``'penalty'`` parameters (regression introduced in 0.21). :pr:`14087` by\n+  `Nicolas Hug`_.\n \n .. _changes_0_21_2:\n \ndiff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py\nindex 1999f1dae7ba3..a8b83759ba75c 100644\n--- a/sklearn/linear_model/logistic.py\n+++ b/sklearn/linear_model/logistic.py\n@@ -2170,7 +2170,7 @@ def fit(self, X, y, sample_weight=None):\n                 # Take the best scores across every fold and the average of\n                 # all coefficients corresponding to the best scores.\n                 best_indices = np.argmax(scores, axis=1)\n-                if self.multi_class == 'ovr':\n+                if multi_class == 'ovr':\n                     w = np.mean([coefs_paths[i, best_indices[i], :]\n                                  for i in range(len(folds))], axis=0)\n                 else:\n@@ -2180,8 +2180,11 @@ def fit(self, X, y, sample_weight=None):\n                 best_indices_C = best_indices % len(self.Cs_)\n                 self.C_.append(np.mean(self.Cs_[best_indices_C]))\n \n-                best_indices_l1 = best_indices // len(self.Cs_)\n-                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n+                if self.penalty == 'elasticnet':\n+                    best_indices_l1 = best_indices // len(self.Cs_)\n+                    self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n+                else:\n+                    self.l1_ratio_.append(None)\n \n             if multi_class == 'multinomial':\n                 self.C_ = np.tile(self.C_, n_classes)\ndiff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py\nindex 6ad9a4ec99d77..c6829e51369e4 100644\n--- a/sklearn/linear_model/tests/test_logistic.py\n+++ b/sklearn/linear_model/tests/test_logistic.py\n@@ -1532,8 +1532,9 @@ def test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr():\n     assert (lrcv.predict(X_test) == gs.predict(X_test)).mean() >= .8\n \n \n-@pytest.mark.parametrize('multi_class', ('ovr', 'multinomial'))\n-def test_LogisticRegressionCV_no_refit(multi_class):\n+@pytest.mark.parametrize('penalty', ('l2', 'elasticnet'))\n+@pytest.mark.parametrize('multi_class', ('ovr', 'multinomial', 'auto'))\n+def test_LogisticRegressionCV_no_refit(penalty, multi_class):\n     # Test LogisticRegressionCV attribute shapes when refit is False\n \n     n_classes = 3\n@@ -1543,9 +1544,12 @@ def test_LogisticRegressionCV_no_refit(multi_class):\n                                random_state=0)\n \n     Cs = np.logspace(-4, 4, 3)\n-    l1_ratios = np.linspace(0, 1, 2)\n+    if penalty == 'elasticnet':\n+        l1_ratios = np.linspace(0, 1, 2)\n+    else:\n+        l1_ratios = None\n \n-    lrcv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',\n+    lrcv = LogisticRegressionCV(penalty=penalty, Cs=Cs, solver='saga',\n                                 l1_ratios=l1_ratios, random_state=0,\n                                 multi_class=multi_class, refit=False)\n     lrcv.fit(X, y)\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/14087"}