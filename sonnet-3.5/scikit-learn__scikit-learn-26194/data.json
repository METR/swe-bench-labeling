{"instance_id": "scikit-learn__scikit-learn-26194", "score": "I", "diff": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..7b3c3d5e9 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1083,7 +1083,10 @@ def roc_curve(\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    if thresholds[0] <= 1:\n+        thresholds = np.r_[min(thresholds[0] + 1, 1), thresholds]\n+    else:\n+        thresholds = np.r_[thresholds[0] + 1, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..911cd8ee0 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -1160,6 +1160,14 @@ def test_average_precision_constant_values():\n     assert average_precision_score(y_true, y_score) == 0.25\n \n \n+def test_roc_curve_with_probability_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    assert np.all(np.logical_or(thresholds <= 1, thresholds == thresholds[0] + 1))\n+    assert thresholds[0] <= 1\n+\n def test_average_precision_score_pos_label_errors():\n     # Raise an error when pos_label is not in binary y_true\n     y_true = np.array([0, 1])\n", "gold_standard_diff": "From cd1c8e18b1a852a8fcb57a1eaf310b3f02cec945 Mon Sep 17 00:00:00 2001\nFrom: Guillaume Lemaitre <g.lemaitre58@gmail.com>\nDate: Mon, 17 Apr 2023 18:31:28 +0200\nSubject: [PATCH 01/10] FIX thresholds should not exceed 1.0 with probabilities\n in\n\n---\n doc/whats_new/v1.3.rst                |  4 ++++\n sklearn/metrics/_ranking.py           |  5 ++++-\n sklearn/metrics/tests/test_ranking.py | 18 ++++++++++++++++--\n 3 files changed, 24 insertions(+), 3 deletions(-)\n\ndiff --git a/doc/whats_new/v1.3.rst b/doc/whats_new/v1.3.rst\nindex e47c74a54edd6..f4288fdb80ed8 100644\n--- a/doc/whats_new/v1.3.rst\n+++ b/doc/whats_new/v1.3.rst\n@@ -366,6 +366,10 @@ Changelog\n - |API| The `eps` parameter of the :func:`log_loss` has been deprecated and will be\n   removed in 1.5. :pr:`25299` by :user:`Omar Salman <OmarManzoor>`.\n \n+- |Fix| Fixes a bug where `thresholds` provided by :func:`metrics.roc_curve` could be\n+  larger than `1.0` even for `y_score` being a probability estimate.\n+  :pr:`xxx` by :user:`Guillaume Lemaitre <glemaitre>`.\n+\n :mod:`sklearn.model_selection`\n ..............................\n \ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead9233898..d9b66f72c1c91 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1083,7 +1083,10 @@ def roc_curve(\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    # make sure to not have a thresholds exceeding 1 for what could look like a\n+    # probability estimate and not a decision function\n+    max_threshold = 1 if thresholds.max() <= 1 else thresholds[0] + 1\n+    thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c38c0a..b1c13f4473947 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,13 +418,13 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n \n def test_roc_curve_fpr_tpr_increasing():\n@@ -2199,3 +2199,17 @@ def test_ranking_metric_pos_label_types(metric, classes):\n         assert not np.isnan(metric_1).any()\n         assert not np.isnan(metric_2).any()\n         assert not np.isnan(thresholds).any()\n+\n+\n+def test_roc_curve_with_probablity_estimates():\n+    \"\"\"Check that thresholds do not exceed 1.0 when `y_score` is a probability\n+    estimate.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/26193\n+    \"\"\"\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n\nFrom add5d6d94245cc6a5e7dab4a4025ef37eacd1090 Mon Sep 17 00:00:00 2001\nFrom: Guillaume Lemaitre <g.lemaitre58@gmail.com>\nDate: Mon, 17 Apr 2023 18:32:38 +0200\nSubject: [PATCH 02/10] iter\n\n---\n sklearn/metrics/_ranking.py | 4 +++-\n 1 file changed, 3 insertions(+), 1 deletion(-)\n\ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex d9b66f72c1c91..55f0303dffe22 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1085,7 +1085,9 @@ def roc_curve(\n     fps = np.r_[0, fps]\n     # make sure to not have a thresholds exceeding 1 for what could look like a\n     # probability estimate and not a decision function\n-    max_threshold = 1 if thresholds.max() <= 1 else thresholds[0] + 1\n+    max_threshold = (\n+        min(1, thresholds[0] + 1) if thresholds.max() <= 1 else thresholds[0] + 1\n+    )\n     thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n\nFrom 026b643446042c0379a04493d385e06c889130aa Mon Sep 17 00:00:00 2001\nFrom: Guillaume Lemaitre <g.lemaitre58@gmail.com>\nDate: Mon, 17 Apr 2023 18:33:44 +0200\nSubject: [PATCH 03/10] DOC update changelog\n\n---\n doc/whats_new/v1.3.rst | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/doc/whats_new/v1.3.rst b/doc/whats_new/v1.3.rst\nindex f4288fdb80ed8..eb91072fddaba 100644\n--- a/doc/whats_new/v1.3.rst\n+++ b/doc/whats_new/v1.3.rst\n@@ -368,7 +368,7 @@ Changelog\n \n - |Fix| Fixes a bug where `thresholds` provided by :func:`metrics.roc_curve` could be\n   larger than `1.0` even for `y_score` being a probability estimate.\n-  :pr:`xxx` by :user:`Guillaume Lemaitre <glemaitre>`.\n+  :pr:`26194` by :user:`Guillaume Lemaitre <glemaitre>`.\n \n :mod:`sklearn.model_selection`\n ..............................\n\nFrom 07592c5f5ac5462cb28198a71d2ea460d376f075 Mon Sep 17 00:00:00 2001\nFrom: Guillaume Lemaitre <g.lemaitre58@gmail.com>\nDate: Tue, 18 Apr 2023 10:22:03 +0200\nSubject: [PATCH 04/10] Update sklearn/metrics/_ranking.py\n\nCo-authored-by: Olivier Grisel <olivier.grisel@ensta.org>\n---\n sklearn/metrics/_ranking.py | 13 ++++++++-----\n 1 file changed, 8 insertions(+), 5 deletions(-)\n\ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 55f0303dffe22..8c9a844147969 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1083,11 +1083,14 @@ def roc_curve(\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    # make sure to not have a thresholds exceeding 1 for what could look like a\n-    # probability estimate and not a decision function\n-    max_threshold = (\n-        min(1, thresholds[0] + 1) if thresholds.max() <= 1 else thresholds[0] + 1\n-    )\n+    # _binary_clf_curve returns decreasing thresholds, hence:\n+    max_threshold, min_threshold = thresholds[0], thresholds[-1]\n+    if min_threshold >=0 and max_threshold <= 1:\n+         # Ensure that probability thresholds stay in the [0-1] range.\n+         max_threshold = min(1, max_threshold + 1)\n+    else:\n+         # Unbounded range for decision_function threshold values.\n+         max_threshold = max_threshold + 1\n     thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n\nFrom ba9e358c583de5dbe07ebe2e7bff068eb89cb578 Mon Sep 17 00:00:00 2001\nFrom: Guillaume Lemaitre <g.lemaitre58@gmail.com>\nDate: Tue, 18 Apr 2023 10:26:15 +0200\nSubject: [PATCH 05/10] update doctest\n\n---\n sklearn/metrics/_ranking.py | 12 ++++++------\n 1 file changed, 6 insertions(+), 6 deletions(-)\n\ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 8c9a844147969..c09325f0b581d 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1056,7 +1056,7 @@ def roc_curve(\n     >>> tpr\n     array([0. , 0.5, 0.5, 1. , 1. ])\n     >>> thresholds\n-    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n+    array([1.0 , 0.8 , 0.4 , 0.35, 0.1 ])\n     \"\"\"\n     fps, tps, thresholds = _binary_clf_curve(\n         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight\n@@ -1085,12 +1085,12 @@ def roc_curve(\n     fps = np.r_[0, fps]\n     # _binary_clf_curve returns decreasing thresholds, hence:\n     max_threshold, min_threshold = thresholds[0], thresholds[-1]\n-    if min_threshold >=0 and max_threshold <= 1:\n-         # Ensure that probability thresholds stay in the [0-1] range.\n-         max_threshold = min(1, max_threshold + 1)\n+    if min_threshold >= 0 and max_threshold <= 1:\n+        # Ensure that probability thresholds stay in the [0-1] range.\n+        max_threshold = min(1, max_threshold + 1)\n     else:\n-         # Unbounded range for decision_function threshold values.\n-         max_threshold = max_threshold + 1\n+        # Unbounded range for decision_function threshold values.\n+        max_threshold = max_threshold + 1\n     thresholds = np.r_[max_threshold, thresholds]\n \n     if fps[-1] <= 0:\n\nFrom 8d85771ab82f857270d2d12a8952114267323b7f Mon Sep 17 00:00:00 2001\nFrom: Guillaume Lemaitre <g.lemaitre58@gmail.com>\nDate: Tue, 18 Apr 2023 11:59:39 +0200\nSubject: [PATCH 06/10] more conservative\n\n---\n doc/whats_new/v1.3.rst      |  5 +++--\n sklearn/metrics/_ranking.py | 19 +++++++++----------\n 2 files changed, 12 insertions(+), 12 deletions(-)\n\ndiff --git a/doc/whats_new/v1.3.rst b/doc/whats_new/v1.3.rst\nindex eb91072fddaba..d8c3016e247be 100644\n--- a/doc/whats_new/v1.3.rst\n+++ b/doc/whats_new/v1.3.rst\n@@ -366,8 +366,9 @@ Changelog\n - |API| The `eps` parameter of the :func:`log_loss` has been deprecated and will be\n   removed in 1.5. :pr:`25299` by :user:`Omar Salman <OmarManzoor>`.\n \n-- |Fix| Fixes a bug where `thresholds` provided by :func:`metrics.roc_curve` could be\n-  larger than `1.0` even for `y_score` being a probability estimate.\n+- |Fix| Fixes a bug in :func:`metrics.roc_curve` where we add `eps` instead of 1.0\n+  to `max(y_score)` for the starting point `tpr=0` and `fpr=0`. It avoids to have values\n+  outised of [0, 1] range for probability estimates.\n   :pr:`26194` by :user:`Guillaume Lemaitre <glemaitre>`.\n \n :mod:`sklearn.model_selection`\ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex c09325f0b581d..eb609547b986f 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1036,6 +1036,10 @@ def roc_curve(\n     are reversed upon returning them to ensure they correspond to both ``fpr``\n     and ``tpr``, which are sorted in reversed order during their calculation.\n \n+    An arbritrary threshold is added for the case `tpr=0` and `fpr=0` to\n+    ensure that the curve starts at `(0, 0)`. This threshold corresponds to the\n+    `max(y_score) + eps`.\n+\n     References\n     ----------\n     .. [1] `Wikipedia entry for the Receiver operating characteristic\n@@ -1056,7 +1060,7 @@ def roc_curve(\n     >>> tpr\n     array([0. , 0.5, 0.5, 1. , 1. ])\n     >>> thresholds\n-    array([1.0 , 0.8 , 0.4 , 0.35, 0.1 ])\n+    array([0.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n     \"\"\"\n     fps, tps, thresholds = _binary_clf_curve(\n         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight\n@@ -1083,15 +1087,10 @@ def roc_curve(\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    # _binary_clf_curve returns decreasing thresholds, hence:\n-    max_threshold, min_threshold = thresholds[0], thresholds[-1]\n-    if min_threshold >= 0 and max_threshold <= 1:\n-        # Ensure that probability thresholds stay in the [0-1] range.\n-        max_threshold = min(1, max_threshold + 1)\n-    else:\n-        # Unbounded range for decision_function threshold values.\n-        max_threshold = max_threshold + 1\n-    thresholds = np.r_[max_threshold, thresholds]\n+    # get dtype of `y_score` even if it is an array-like\n+    score_dtype = getattr(y_score, \"dtype\", np.float64)\n+    dtype = score_dtype if score_dtype in (np.float32, np.float64) else np.float64\n+    thresholds = np.r_[thresholds[0] + np.finfo(dtype).eps, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\n\nFrom 973ee1a4477d02a3fa62c0616c50722f1e3ea3ff Mon Sep 17 00:00:00 2001\nFrom: Guillaume Lemaitre <g.lemaitre58@gmail.com>\nDate: Tue, 18 Apr 2023 18:30:10 +0200\nSubject: [PATCH 07/10] Update the documentation\n\n---\n doc/modules/model_evaluation.rst      | 2 +-\n sklearn/metrics/_ranking.py           | 4 ++--\n sklearn/metrics/tests/test_ranking.py | 8 +++++---\n 3 files changed, 8 insertions(+), 6 deletions(-)\n\ndiff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst\nindex 537f23e49d2dc..15eee7cde726e 100644\n--- a/doc/modules/model_evaluation.rst\n+++ b/doc/modules/model_evaluation.rst\n@@ -1366,7 +1366,7 @@ function::\n     >>> tpr\n     array([0. , 0.5, 0.5, 1. , 1. ])\n     >>> thresholds\n-    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n+    array([0.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n \n Compared to metrics such as the subset accuracy, the Hamming loss, or the\n F1 score, ROC doesn't require optimizing a threshold for each label.\ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex eb609547b986f..ead6be2d85138 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1016,10 +1016,10 @@ def roc_curve(\n         Increasing true positive rates such that element `i` is the true\n         positive rate of predictions with score >= `thresholds[i]`.\n \n-    thresholds : ndarray of shape = (n_thresholds,)\n+    thresholds : ndarray of shape (n_thresholds,)\n         Decreasing thresholds on the decision function used to compute\n         fpr and tpr. `thresholds[0]` represents no instances being predicted\n-        and is arbitrarily set to `max(y_score) + 1`.\n+        and is arbitrarily set to `max(y_score) + eps`.\n \n     See Also\n     --------\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex b1c13f4473947..35f5073b580e0 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -2201,15 +2201,17 @@ def test_ranking_metric_pos_label_types(metric, classes):\n         assert not np.isnan(thresholds).any()\n \n \n-def test_roc_curve_with_probablity_estimates():\n+def test_roc_curve_with_probablity_estimates(global_random_seed):\n     \"\"\"Check that thresholds do not exceed 1.0 when `y_score` is a probability\n     estimate.\n \n     Non-regression test for:\n     https://github.com/scikit-learn/scikit-learn/issues/26193\n     \"\"\"\n-    rng = np.random.RandomState(42)\n+    rng = np.random.RandomState(global_random_seed)\n     y_true = rng.randint(0, 2, size=10)\n     y_score = rng.rand(10)\n     _, _, thresholds = roc_curve(y_true, y_score)\n-    assert np.logical_or(thresholds <= 1, thresholds >= 0).all()\n+    assert np.logical_or(\n+        thresholds <= 1 + np.info(y_score.dtype).eps, thresholds >= 0\n+    ).all()\n\nFrom 326da9739c6f8e879ab53756f5fc672d373f5b09 Mon Sep 17 00:00:00 2001\nFrom: Olivier Grisel <olivier.grisel@ensta.org>\nDate: Fri, 21 Apr 2023 17:29:55 +0200\nSubject: [PATCH 08/10] Wording\n\n---\n doc/whats_new/v1.3.rst | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/doc/whats_new/v1.3.rst b/doc/whats_new/v1.3.rst\nindex d8c3016e247be..12b302bbfd287 100644\n--- a/doc/whats_new/v1.3.rst\n+++ b/doc/whats_new/v1.3.rst\n@@ -367,8 +367,8 @@ Changelog\n   removed in 1.5. :pr:`25299` by :user:`Omar Salman <OmarManzoor>`.\n \n - |Fix| Fixes a bug in :func:`metrics.roc_curve` where we add `eps` instead of 1.0\n-  to `max(y_score)` for the starting point `tpr=0` and `fpr=0`. It avoids to have values\n-  outised of [0, 1] range for probability estimates.\n+  to `max(y_score)` for the starting point `tpr=0` and `fpr=0`. It avoids getting\n+  values significantly outside of the [0, 1] range for probability estimates.\n   :pr:`26194` by :user:`Guillaume Lemaitre <glemaitre>`.\n \n :mod:`sklearn.model_selection`\n\nFrom eb57cb0bb0672383e26a6bf23b810b53be5d95cc Mon Sep 17 00:00:00 2001\nFrom: Guillaume Lemaitre <g.lemaitre58@gmail.com>\nDate: Tue, 2 May 2023 14:53:59 +0200\nSubject: [PATCH 09/10] use np.inf\n\n---\n doc/whats_new/v1.3.rst                |  6 +++---\n sklearn/metrics/_ranking.py           | 10 ++++------\n sklearn/metrics/tests/test_ranking.py |  8 +++-----\n 3 files changed, 10 insertions(+), 14 deletions(-)\n\ndiff --git a/doc/whats_new/v1.3.rst b/doc/whats_new/v1.3.rst\nindex 12b302bbfd287..6a5c0812a8622 100644\n--- a/doc/whats_new/v1.3.rst\n+++ b/doc/whats_new/v1.3.rst\n@@ -366,9 +366,9 @@ Changelog\n - |API| The `eps` parameter of the :func:`log_loss` has been deprecated and will be\n   removed in 1.5. :pr:`25299` by :user:`Omar Salman <OmarManzoor>`.\n \n-- |Fix| Fixes a bug in :func:`metrics.roc_curve` where we add `eps` instead of 1.0\n-  to `max(y_score)` for the starting point `tpr=0` and `fpr=0`. It avoids getting\n-  values significantly outside of the [0, 1] range for probability estimates.\n+- |Fix| In :func:`metrics.roc_curve`, use the threshold value `np.inf` instead of\n+  arbritrary `max(y_score) + 1`. This threshold is associated with the ROC curve point\n+  `tpr=0` and `fpr=0`.\n   :pr:`26194` by :user:`Guillaume Lemaitre <glemaitre>`.\n \n :mod:`sklearn.model_selection`\ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex ead6be2d85138..7a3b7f0cc2663 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1019,7 +1019,7 @@ def roc_curve(\n     thresholds : ndarray of shape (n_thresholds,)\n         Decreasing thresholds on the decision function used to compute\n         fpr and tpr. `thresholds[0]` represents no instances being predicted\n-        and is arbitrarily set to `max(y_score) + eps`.\n+        and is arbitrarily set to `np.inf`.\n \n     See Also\n     --------\n@@ -1038,7 +1038,7 @@ def roc_curve(\n \n     An arbritrary threshold is added for the case `tpr=0` and `fpr=0` to\n     ensure that the curve starts at `(0, 0)`. This threshold corresponds to the\n-    `max(y_score) + eps`.\n+    `np.inf`.\n \n     References\n     ----------\n@@ -1060,7 +1060,7 @@ def roc_curve(\n     >>> tpr\n     array([0. , 0.5, 0.5, 1. , 1. ])\n     >>> thresholds\n-    array([0.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n+    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\n     \"\"\"\n     fps, tps, thresholds = _binary_clf_curve(\n         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight\n@@ -1088,9 +1088,7 @@ def roc_curve(\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n     # get dtype of `y_score` even if it is an array-like\n-    score_dtype = getattr(y_score, \"dtype\", np.float64)\n-    dtype = score_dtype if score_dtype in (np.float32, np.float64) else np.float64\n-    thresholds = np.r_[thresholds[0] + np.finfo(dtype).eps, thresholds]\n+    thresholds = np.r_[np.inf, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex 35f5073b580e0..d40e48456fe9d 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,13 +418,13 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n+    assert_array_almost_equal(thresholds, [np.inf, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+    assert_array_almost_equal(thresholds, [np.inf, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n \n def test_roc_curve_fpr_tpr_increasing():\n@@ -2212,6 +2212,4 @@ def test_roc_curve_with_probablity_estimates(global_random_seed):\n     y_true = rng.randint(0, 2, size=10)\n     y_score = rng.rand(10)\n     _, _, thresholds = roc_curve(y_true, y_score)\n-    assert np.logical_or(\n-        thresholds <= 1 + np.info(y_score.dtype).eps, thresholds >= 0\n-    ).all()\n+    assert np.isinf(thresholds[0])\n\nFrom 5cfc9b40e47cbcb37913c473a163eff53b383ce6 Mon Sep 17 00:00:00 2001\nFrom: Guillaume Lemaitre <g.lemaitre58@gmail.com>\nDate: Tue, 2 May 2023 16:03:19 +0200\nSubject: [PATCH 10/10] iter\n\n---\n doc/modules/model_evaluation.rst | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst\nindex 15eee7cde726e..c0b0ab67de786 100644\n--- a/doc/modules/model_evaluation.rst\n+++ b/doc/modules/model_evaluation.rst\n@@ -1366,7 +1366,7 @@ function::\n     >>> tpr\n     array([0. , 0.5, 0.5, 1. , 1. ])\n     >>> thresholds\n-    array([0.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n+    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\n \n Compared to metrics such as the subset accuracy, the Hamming loss, or the\n F1 score, ROC doesn't require optimizing a threshold for each label.\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/26194"}