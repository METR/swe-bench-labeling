{"instance_id": "scikit-learn__scikit-learn-10297", "score": "C", "diff": "diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex 911c74ae3..966af8a5a 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1301,6 +1301,10 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n         weights inversely proportional to class frequencies in the input data\n         as ``n_samples / (n_classes * np.bincount(y))``\n \n+    store_cv_values : boolean, default=False\n+        Flag indicating if the cross-validation values corresponding to\n+        each alpha should be stored in the `cv_values_` attribute.\n+\n     Attributes\n     ----------\n     cv_values_ : array, shape = [n_samples, n_alphas] or \\\n@@ -1333,11 +1337,13 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n     advantage of the multi-variate response support in Ridge.\n     \"\"\"\n     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n-                 normalize=False, scoring=None, cv=None, class_weight=None):\n+                 normalize=False, scoring=None, cv=None, class_weight=None,\n+                 store_cv_values=False):\n         super(RidgeClassifierCV, self).__init__(\n             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n-            scoring=scoring, cv=cv)\n+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)\n         self.class_weight = class_weight\n+        self.store_cv_values = store_cv_values\n \n     def fit(self, X, y, sample_weight=None):\n         \"\"\"Fit the ridge classifier.\n@@ -1373,7 +1379,7 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n             sample_weight = (sample_weight *\n                              compute_sample_weight(self.class_weight, y))\n \n-        _BaseRidgeCV.fit(self, X, Y, sample_weight=sample_weight)\n+        super(RidgeClassifierCV, self).fit(X, Y, sample_weight=sample_weight)\n         return self\n \n     @property\n", "gold_standard_diff": "From 1745347be6d62fd6d00118878cec56edb74b06fe Mon Sep 17 00:00:00 2001\nFrom: Mabel Villalba <mabelvj@gmail.com>\nDate: Tue, 12 Dec 2017 23:03:16 +0100\nSubject: [PATCH 1/9] Fixex #10284. Added store_cv_values to RidgeClassifierCV\n and a test. ISSUE: Linear_model.RidgeClassifierCV's Parameter store_cv_values\n\n---\n sklearn/linear_model/ridge.py            | 12 ++++++--\n sklearn/linear_model/tests/test_ridge.py | 38 +++++++++++++++++++++---\n 2 files changed, 44 insertions(+), 6 deletions(-)\n\ndiff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex c46cdff7da2d3..32bbeae736e81 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1301,6 +1301,12 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n         weights inversely proportional to class frequencies in the input data\n         as ``n_samples / (n_classes * np.bincount(y))``\n \n+    store_cv_values : boolean, default=False\n+        Flag indicating if the cross-validation values corresponding to\n+        each alpha should be stored in the `cv_values_` attribute (see\n+        below). This flag is only compatible with `cv=None` (i.e. using\n+        Generalized Cross-Validation).\n+\n     Attributes\n     ----------\n     cv_values_ : array, shape = [n_samples, n_alphas] or \\\n@@ -1332,11 +1338,13 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n     a one-versus-all approach. Concretely, this is implemented by taking\n     advantage of the multi-variate response support in Ridge.\n     \"\"\"\n+\n     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n-                 normalize=False, scoring=None, cv=None, class_weight=None):\n+                 normalize=False, scoring=None, cv=None, class_weight=None,\n+                 store_cv_values=False):\n         super(RidgeClassifierCV, self).__init__(\n             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n-            scoring=scoring, cv=cv)\n+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)\n         self.class_weight = class_weight\n \n     def fit(self, X, y, sample_weight=None):\ndiff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\nindex ee44da5d56b86..fd4f6b1edae82 100644\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -49,8 +49,11 @@\n X_iris = sp.csr_matrix(iris.data)\n y_iris = iris.target\n \n-DENSE_FILTER = lambda X: X\n-SPARSE_FILTER = lambda X: sp.csr_matrix(X)\n+\n+def DENSE_FILTER(X): return X\n+\n+\n+def SPARSE_FILTER(X): return sp.csr_matrix(X)\n \n \n def test_ridge():\n@@ -352,7 +355,7 @@ def _test_ridge_loo(filter_):\n     assert_equal(ridge_gcv2.alpha_, alpha_)\n \n     # check that we get same best alpha with custom score_func\n-    func = lambda x, y: -mean_squared_error(x, y)\n+    def func(x, y): return -mean_squared_error(x, y)\n     scoring = make_scorer(func)\n     ridge_gcv3 = RidgeCV(fit_intercept=False, scoring=scoring)\n     f(ridge_gcv3.fit)(filter_(X_diabetes), y_diabetes)\n@@ -576,7 +579,7 @@ def test_class_weights_cv():\n \n def test_ridgecv_store_cv_values():\n     # Test _RidgeCV's store_cv_values attribute.\n-    rng = rng = np.random.RandomState(42)\n+    rng = np.random.RandomState(42)\n \n     n_samples = 8\n     n_features = 5\n@@ -598,6 +601,33 @@ def test_ridgecv_store_cv_values():\n     assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))\n \n \n+def test_ridge_classifier_cv_store_cv_values():\n+    # Test RidgeClassifier\n+    x = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n+                  [1.0, 1.0], [1.0, 0.0]])\n+    y = np.array([1, 1, 1, -1, -1])\n+\n+    n_samples = x.shape[0]\n+\n+    alphas = [1e-1, 1e0, 1e1]\n+    n_alphas = len(alphas)\n+\n+    r = RidgeClassifierCV(alphas=alphas, store_cv_values=True)\n+\n+    # with len(y.shape) == 1\n+    n_targets = 1\n+    r.fit(x, y)\n+    assert_equal(r.cv_values_.shape, (n_samples, n_targets, n_alphas))\n+\n+    # with len(y.shape) == 2\n+    y = np.array([[1, 1, 1, -1, -1],\n+                  [1, -1, 1, -1, 1],\n+                  [-1, -1, 1, -1, -1]]).transpose()\n+    n_targets = y.shape[1]\n+    r.fit(x, y)\n+    assert_equal(r.cv_values_.shape, (n_samples, n_targets, n_alphas))\n+\n+\n def test_ridgecv_sample_weight():\n     rng = np.random.RandomState(0)\n     alphas = (0.1, 1.0, 10.0)\n\nFrom eb59462764dc0e02eb25ffd4fd04761671ce7470 Mon Sep 17 00:00:00 2001\nFrom: Mabel Villalba <mabelvj@gmail.com>\nDate: Tue, 19 Dec 2017 13:10:33 +0100\nSubject: [PATCH 2/9] Fixes #10284 Changed test nomenclature in\n test_ridgecv_store_cv_values\n\n---\n sklearn/linear_model/tests/test_ridge.py | 6 +++---\n 1 file changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\nindex fd4f6b1edae82..3a77373bea9e8 100644\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -595,10 +595,10 @@ def test_ridgecv_store_cv_values():\n     assert_equal(r.cv_values_.shape, (n_samples, n_alphas))\n \n     # with len(y.shape) == 2\n-    n_responses = 3\n-    y = rng.randn(n_samples, n_responses)\n+    n_targets = 3\n+    y = rng.randn(n_samples, n_targets)\n     r.fit(x, y)\n-    assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))\n+    assert_equal(r.cv_values_.shape, (n_samples, n_targets, n_alphas))\n \n \n def test_ridge_classifier_cv_store_cv_values():\n\nFrom 90a7205ee4715509dc2e9a5e2d31de8c370eec05 Mon Sep 17 00:00:00 2001\nFrom: Mabel Villalba <mabelvj@gmail.com>\nDate: Wed, 20 Dec 2017 07:52:46 +0100\nSubject: [PATCH 3/9] Fixes #10284 Updated RidgeClassifierCV documentation\n\n---\n sklearn/linear_model/ridge.py | 3 +--\n 1 file changed, 1 insertion(+), 2 deletions(-)\n\ndiff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex 32bbeae736e81..1f773a0208c0e 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1309,8 +1309,7 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n \n     Attributes\n     ----------\n-    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n-    shape = [n_samples, n_responses, n_alphas], optional\n+    cv_values_ : array, shape = [n_samples, n_targets, n_alphas], optional\n         Cross-validation values for each alpha (if `store_cv_values=True` and\n     `cv=None`). After `fit()` has been called, this attribute will contain \\\n     the mean squared errors (by default) or the values of the \\\n\nFrom f6130fef2c6834bb52bc31c539761d6eaffd5f97 Mon Sep 17 00:00:00 2001\nFrom: Mabel Villalba <mabelvj@gmail.com>\nDate: Mon, 25 Dec 2017 12:39:21 +0100\nSubject: [PATCH 4/9] Fixes #10284 Updated RidgeClassifierCV documentation -\n line formatting fixing\n\n---\n sklearn/linear_model/ridge.py | 6 +++---\n 1 file changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex 1f773a0208c0e..bbccba574c582 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1311,9 +1311,9 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n     ----------\n     cv_values_ : array, shape = [n_samples, n_targets, n_alphas], optional\n         Cross-validation values for each alpha (if `store_cv_values=True` and\n-    `cv=None`). After `fit()` has been called, this attribute will contain \\\n-    the mean squared errors (by default) or the values of the \\\n-    `{loss,score}_func` function (if provided in the constructor).\n+        `cv=None`). After `fit()` has been called, this attribute will contain\n+        the mean squared errors (by default) or the values of the\n+        `{loss,score}_func` function (if provided in the constructor).\n \n     coef_ : array, shape = [n_features] or [n_targets, n_features]\n         Weight vector(s).\n\nFrom 980ea0e4dea51eb065e3fd5794b1deeb8167c207 Mon Sep 17 00:00:00 2001\nFrom: Mabel Villalba <mabelvj@gmail.com>\nDate: Wed, 27 Dec 2017 02:25:40 +0100\nSubject: [PATCH 5/9] Fixes #10284 Updated RidgeClassifierCV added credits\n\n---\n doc/whats_new/v0.20.rst | 6 ++++++\n 1 file changed, 6 insertions(+)\n\ndiff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst\nindex defa3bdc4c792..e51d5246d066c 100644\n--- a/doc/whats_new/v0.20.rst\n+++ b/doc/whats_new/v0.20.rst\n@@ -155,6 +155,12 @@ Classifiers and regressors\n   arrays are converted to C-ordered arrays in the dense case. :issue:`9991` by\n   :user:`Guillaume Lemaitre <glemaitre>`.\n \n+- Fixed a bug in :class: `linear_model.RidgeClassifierCV` where\n+  the parameter `store_cv_values` was not immplemented though it was documented\n+  in `cv_values_` as a way to set up the storage of cross-validation\n+  values for different alphas. :issue:`10297` by \n+  :user:`Isabel Mar\u00eda Villalba-Jim\u00e9nez <mabelvj>`. \n+\n Decomposition, manifold learning and clustering\n \n - Fix for uninformative error in :class:`decomposition.IncrementalPCA`:\n\nFrom 1d18ca24885dc29df8cb8f629897e9f28844d6a9 Mon Sep 17 00:00:00 2001\nFrom: Mabel Villalba <mabelvj@gmail.com>\nDate: Tue, 2 Jan 2018 14:08:27 +0100\nSubject: [PATCH 6/9] Fixes #10284 added double bacticks to documentation of\n ridge\n\n---\n doc/whats_new/v0.20.rst       | 10 +++++-----\n sklearn/linear_model/ridge.py | 22 +++++++++++-----------\n 2 files changed, 16 insertions(+), 16 deletions(-)\n\ndiff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst\nindex af222235c8444..a4ec288e3225e 100644\n--- a/doc/whats_new/v0.20.rst\n+++ b/doc/whats_new/v0.20.rst\n@@ -22,7 +22,7 @@ random sampling procedures.\n - :class:`metrics.roc_auc_score` (bug fix)\n - :class:`metrics.roc_curve` (bug fix)\n - :class:`neural_network.BaseMultilayerPerceptron` (bug fix)\n-- :class:`neural_network.MLPRegressor` (bug fix)\n+- :class:`neural_network.MLPRidgeClassifierCV_store_cv_values_issue_10284Regressor` (bug fix)\n - :class:`neural_network.MLPClassifier` (bug fix)\n \n Details are listed in the changelog below.\n@@ -196,10 +196,10 @@ Classifiers and regressors\n   :issue:`9579` by :user:`Thomas Kober <tttthomasssss>`.\n \n - Fixed a bug in :class:`linear_model.RidgeClassifierCV` where\n-  the parameter `store_cv_values` was not immplemented though it was documented\n-  in `cv_values_` as a way to set up the storage of cross-validation\n-  values for different alphas. :issue:`10297` by \n-  :user:`Isabel Mar\u00eda Villalba-Jim\u00e9nez <mabelvj>`. \n+  the parameter ``store_cv_values`` was not immplemented though\n+  it was documented in ``cv_values_`` as a way to set up the storage\n+  of cross-validation values for different alphas. :issue:`10297` by \n+  :user:`Mabel Villalba-Jim\u00e9nez <mabelvj>`. \n \n Decomposition, manifold learning and clustering\n \ndiff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex 09dce8567e656..90e370dbc4e7a 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1212,18 +1212,18 @@ class RidgeCV(_BaseRidgeCV, RegressorMixin):\n \n     store_cv_values : boolean, default=False\n         Flag indicating if the cross-validation values corresponding to\n-        each alpha should be stored in the `cv_values_` attribute (see\n-        below). This flag is only compatible with `cv=None` (i.e. using\n+        each alpha should be stored in the ``cv_values_`` attribute (see\n+        below). This flag is only compatible with ``cv=None`` (i.e. using\n         Generalized Cross-Validation).\n \n     Attributes\n     ----------\n     cv_values_ : array, shape = [n_samples, n_alphas] or \\\n         shape = [n_samples, n_targets, n_alphas], optional\n-        Cross-validation values for each alpha (if `store_cv_values=True` and \\\n-        `cv=None`). After `fit()` has been called, this attribute will \\\n+        Cross-validation values for each alpha (if ``store_cv_values=True``\\\n+        and ``cv=None``). After ``fit()`` has been called, this attribute will \\\n         contain the mean squared errors (by default) or the values of the \\\n-        `{loss,score}_func` function (if provided in the constructor).\n+        ``{loss,score}_func`` function (if provided in the constructor).\n \n     coef_ : array, shape = [n_features] or [n_targets, n_features]\n         Weight vector(s).\n@@ -1303,17 +1303,17 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n \n     store_cv_values : boolean, default=False\n         Flag indicating if the cross-validation values corresponding to\n-        each alpha should be stored in the `cv_values_` attribute (see\n-        below). This flag is only compatible with `cv=None` (i.e. using\n+        each alpha should be stored in the ``cv_values_`` attribute (see\n+        below). This flag is only compatible with ``cv=None`` (i.e. using\n         Generalized Cross-Validation).\n \n     Attributes\n     ----------\n     cv_values_ : array, shape = [n_samples, n_targets, n_alphas], optional\n-        Cross-validation values for each alpha (if `store_cv_values=True` and\n-        `cv=None`). After `fit()` has been called, this attribute will contain\n-        the mean squared errors (by default) or the values of the\n-        `{loss,score}_func` function (if provided in the constructor).\n+        Cross-validation values for each alpha (if ``store_cv_values=True`` and\n+        ``cv=None``). After ``fit()`` has been called, this attribute will\n+        contain the mean squared errors (by default) or the values of the\n+        ``{loss,score}_func`` function (if provided in the constructor).\n \n     coef_ : array, shape = [n_features] or [n_targets, n_features]\n         Weight vector(s).\n\nFrom cf4076f240781dedf82902751ff56745bb36b729 Mon Sep 17 00:00:00 2001\nFrom: Mabel Villalba <mabelvj@gmail.com>\nDate: Thu, 4 Jan 2018 00:12:54 +0100\nSubject: [PATCH 7/9] Fixes #10284 : Fixed documentation line exceeding 80\n\n---\n sklearn/linear_model/ridge.py | 6 +++---\n 1 file changed, 3 insertions(+), 3 deletions(-)\n\ndiff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex 90e370dbc4e7a..d65ec6429212b 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1221,9 +1221,9 @@ class RidgeCV(_BaseRidgeCV, RegressorMixin):\n     cv_values_ : array, shape = [n_samples, n_alphas] or \\\n         shape = [n_samples, n_targets, n_alphas], optional\n         Cross-validation values for each alpha (if ``store_cv_values=True``\\\n-        and ``cv=None``). After ``fit()`` has been called, this attribute will \\\n-        contain the mean squared errors (by default) or the values of the \\\n-        ``{loss,score}_func`` function (if provided in the constructor).\n+        and ``cv=None``). After ``fit()`` has been called, this attribute \\\n+        will contain the mean squared errors (by default) or the values \\\n+        of the ``{loss,score}_func`` function (if provided in the constructor).\n \n     coef_ : array, shape = [n_features] or [n_targets, n_features]\n         Weight vector(s).\n\nFrom 771ab2c406a4ef36033becb49c050015ad3f8b46 Mon Sep 17 00:00:00 2001\nFrom: Mabel Villalba <mabelvj@gmail.com>\nDate: Fri, 9 Feb 2018 10:21:32 +0100\nSubject: [PATCH 8/9] [WIP] Fixes #10284 Converted assert_equal to assert and\n corrected issue with doc v0.20\n\n---\n doc/whats_new/v0.20.rst                  |  2 +-\n sklearn/linear_model/tests/test_ridge.py | 20 ++++++++------------\n 2 files changed, 9 insertions(+), 13 deletions(-)\n\ndiff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst\nindex 6bd087dad1cb1..6855c8ab7276b 100644\n--- a/doc/whats_new/v0.20.rst\n+++ b/doc/whats_new/v0.20.rst\n@@ -27,7 +27,7 @@ random sampling procedures.\n - :class:`metrics.roc_auc_score` (bug fix)\n - :class:`metrics.roc_curve` (bug fix)\n - :class:`neural_network.BaseMultilayerPerceptron` (bug fix)\n-- :class:`neural_network.MLPRidgeClassifierCV_store_cv_values_issue_10284Regressor` (bug fix)\n+- :class:`neural_network.MLPRegressor` (bug fix)\n - :class:`neural_network.MLPClassifier` (bug fix)\n \n Details are listed in the changelog below.\ndiff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\nindex 3a77373bea9e8..8ce02899d0a1a 100644\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -50,10 +50,8 @@\n y_iris = iris.target\n \n \n-def DENSE_FILTER(X): return X\n-\n-\n-def SPARSE_FILTER(X): return sp.csr_matrix(X)\n+DENSE_FILTER = lambda X: X\n+SPARSE_FILTER = lambda X: sp.csr_matrix(X)\n \n \n def test_ridge():\n@@ -355,7 +353,7 @@ def _test_ridge_loo(filter_):\n     assert_equal(ridge_gcv2.alpha_, alpha_)\n \n     # check that we get same best alpha with custom score_func\n-    def func(x, y): return -mean_squared_error(x, y)\n+    func = lambda x, y: -mean_squared_error(x, y)\n     scoring = make_scorer(func)\n     ridge_gcv3 = RidgeCV(fit_intercept=False, scoring=scoring)\n     f(ridge_gcv3.fit)(filter_(X_diabetes), y_diabetes)\n@@ -578,7 +576,6 @@ def test_class_weights_cv():\n \n \n def test_ridgecv_store_cv_values():\n-    # Test _RidgeCV's store_cv_values attribute.\n     rng = np.random.RandomState(42)\n \n     n_samples = 8\n@@ -592,17 +589,16 @@ def test_ridgecv_store_cv_values():\n     # with len(y.shape) == 1\n     y = rng.randn(n_samples)\n     r.fit(x, y)\n-    assert_equal(r.cv_values_.shape, (n_samples, n_alphas))\n+    assert r.cv_values_.shape == (n_samples, n_alphas)\n \n     # with len(y.shape) == 2\n     n_targets = 3\n     y = rng.randn(n_samples, n_targets)\n     r.fit(x, y)\n-    assert_equal(r.cv_values_.shape, (n_samples, n_targets, n_alphas))\n+    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)\n \n \n def test_ridge_classifier_cv_store_cv_values():\n-    # Test RidgeClassifier\n     x = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n                   [1.0, 1.0], [1.0, 0.0]])\n     y = np.array([1, 1, 1, -1, -1])\n@@ -617,7 +613,7 @@ def test_ridge_classifier_cv_store_cv_values():\n     # with len(y.shape) == 1\n     n_targets = 1\n     r.fit(x, y)\n-    assert_equal(r.cv_values_.shape, (n_samples, n_targets, n_alphas))\n+    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)\n \n     # with len(y.shape) == 2\n     y = np.array([[1, 1, 1, -1, -1],\n@@ -625,7 +621,7 @@ def test_ridge_classifier_cv_store_cv_values():\n                   [-1, -1, 1, -1, -1]]).transpose()\n     n_targets = y.shape[1]\n     r.fit(x, y)\n-    assert_equal(r.cv_values_.shape, (n_samples, n_targets, n_alphas))\n+    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)\n \n \n def test_ridgecv_sample_weight():\n@@ -648,7 +644,7 @@ def test_ridgecv_sample_weight():\n         gs = GridSearchCV(Ridge(), parameters, cv=cv)\n         gs.fit(X, y, sample_weight=sample_weight)\n \n-        assert_equal(ridgecv.alpha_, gs.best_estimator_.alpha)\n+        assert ridgecv.alpha_ == gs.best_estimator_.alpha\n         assert_array_almost_equal(ridgecv.coef_, gs.best_estimator_.coef_)\n \n \n\nFrom 992fd6e962606e1c64ecd1ba4fd8147d3fd4a29a Mon Sep 17 00:00:00 2001\nFrom: Hanmin Qin <qinhanmin2005@sina.com>\nDate: Thu, 15 Mar 2018 17:39:16 +0800\nSubject: [PATCH 9/9] revert some unrelevant changes\n\n---\n doc/whats_new/v0.20.rst                  | 2 +-\n sklearn/linear_model/ridge.py            | 1 -\n sklearn/linear_model/tests/test_ridge.py | 2 --\n 3 files changed, 1 insertion(+), 4 deletions(-)\n\ndiff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst\nindex 65268f48ccfa8..b01ac19a4b6c4 100644\n--- a/doc/whats_new/v0.20.rst\n+++ b/doc/whats_new/v0.20.rst\n@@ -222,7 +222,7 @@ Classifiers and regressors\n   :issue:`9579` by :user:`Thomas Kober <tttthomasssss>`.\n \n - Fixed a bug in :class:`linear_model.RidgeClassifierCV` where\n-  the parameter ``store_cv_values`` was not immplemented though\n+  the parameter ``store_cv_values`` was not implemented though\n   it was documented in ``cv_values`` as a way to set up the storage\n   of cross-validation values for different alphas. :issue:`10297` by \n   :user:`Mabel Villalba-Jim\u00e9nez <mabelvj>`.\ndiff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex d5bc82c0d49d3..1d80cacac9a1d 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1337,7 +1337,6 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n     a one-versus-all approach. Concretely, this is implemented by taking\n     advantage of the multi-variate response support in Ridge.\n     \"\"\"\n-\n     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n                  normalize=False, scoring=None, cv=None, class_weight=None,\n                  store_cv_values=False):\ndiff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\nindex 8ce02899d0a1a..d1865dfe72d85 100644\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -49,7 +49,6 @@\n X_iris = sp.csr_matrix(iris.data)\n y_iris = iris.target\n \n-\n DENSE_FILTER = lambda X: X\n SPARSE_FILTER = lambda X: sp.csr_matrix(X)\n \n@@ -604,7 +603,6 @@ def test_ridge_classifier_cv_store_cv_values():\n     y = np.array([1, 1, 1, -1, -1])\n \n     n_samples = x.shape[0]\n-\n     alphas = [1e-1, 1e0, 1e1]\n     n_alphas = len(alphas)\n \n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/10297"}