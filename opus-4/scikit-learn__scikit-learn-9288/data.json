{"instance_id": "scikit-learn__scikit-learn-9288", "score": "C", "diff": "diff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py\nindex b7fbdf7da..efa70d459 100644\n--- a/sklearn/cluster/k_means_.py\n+++ b/sklearn/cluster/k_means_.py\n@@ -363,13 +363,15 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',\n     if effective_n_jobs(n_jobs) == 1:\n         # For a single thread, less memory is needed if we just store one set\n         # of the best results (as opposed to one set per run per thread).\n+        # Generate seeds for deterministic behavior\n+        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n         for it in range(n_init):\n             # run a k-means once\n             labels, inertia, centers, n_iter_ = kmeans_single(\n                 X, sample_weight, n_clusters, max_iter=max_iter, init=init,\n                 verbose=verbose, precompute_distances=precompute_distances,\n                 tol=tol, x_squared_norms=x_squared_norms,\n-                random_state=random_state)\n+                random_state=seeds[it])\n             # determine if these results are the best so far\n             if best_inertia is None or inertia < best_inertia:\n                 best_labels = labels.copy()\ndiff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py\nindex 362b0a914..d129d6afe 100644\n--- a/sklearn/cluster/tests/test_k_means.py\n+++ b/sklearn/cluster/tests/test_k_means.py\n@@ -951,3 +951,25 @@ def test_minibatch_kmeans_partial_fit_int_data():\n     km = MiniBatchKMeans(n_clusters=2)\n     km.partial_fit(X)\n     assert km.cluster_centers_.dtype.kind == \"f\"\n+\n+\n+\n+def test_kmeans_n_jobs_consistency():\n+    # Test that KMeans gives consistent results regardless of n_jobs\n+    # Regression test for issue where n_jobs=1 gave different results than n_jobs>1\n+    X, y = make_blobs(n_samples=1000, centers=10, n_features=2, random_state=2)\n+    \n+    # Run KMeans with n_jobs=1\n+    kmeans_sequential = KMeans(n_clusters=10, random_state=2, n_jobs=1, n_init=10)\n+    kmeans_sequential.fit(X)\n+    inertia_sequential = kmeans_sequential.inertia_\n+    \n+    # Run KMeans with various n_jobs values\n+    for n_jobs in [2, 3, 4, -1]:\n+        kmeans_parallel = KMeans(n_clusters=10, random_state=2, n_jobs=n_jobs, n_init=10)\n+        kmeans_parallel.fit(X)\n+        assert_almost_equal(kmeans_parallel.inertia_, inertia_sequential)\n+        # Also check that cluster centers are the same\n+        assert_allclose(np.sort(kmeans_parallel.cluster_centers_.ravel()),\n+                        np.sort(kmeans_sequential.cluster_centers_.ravel()),\n+                        rtol=1e-7, atol=1e-7)\ndiff --git a/test_kmeans_algorithms.py b/test_kmeans_algorithms.py\nnew file mode 100644\nindex 000000000..01262aab6\n--- /dev/null\n+++ b/test_kmeans_algorithms.py\n@@ -0,0 +1,13 @@\n+from sklearn.cluster import KMeans\n+from sklearn.datasets import make_blobs\n+\n+# Generate some data\n+X, y = make_blobs(n_samples=1000, centers=5, n_features=10, random_state=42)\n+\n+# Test both algorithms\n+for algorithm in ['full', 'elkan']:\n+    print(f\"\\nTesting algorithm: {algorithm}\")\n+    for n_jobs in range(1, 4):\n+        kmeans = KMeans(n_clusters=5, random_state=42, n_jobs=n_jobs, algorithm=algorithm, n_init=5)\n+        kmeans.fit(X)\n+        print(f'  (n_jobs={n_jobs}) kmeans.inertia_ = {kmeans.inertia_:.10f}')\ndiff --git a/test_kmeans_n_jobs_consistency.py b/test_kmeans_n_jobs_consistency.py\nnew file mode 100644\nindex 000000000..0c471a64a\n--- /dev/null\n+++ b/test_kmeans_n_jobs_consistency.py\n@@ -0,0 +1,48 @@\n+\"\"\"Test that KMeans gives consistent results regardless of n_jobs\"\"\"\n+import numpy as np\n+from sklearn.cluster import KMeans\n+from sklearn.datasets import make_blobs\n+\n+def test_kmeans_consistency_n_jobs():\n+    # Generate some data\n+    X, y = make_blobs(n_samples=10000, centers=10, n_features=2, random_state=2)\n+    \n+    # Run KMeans with n_jobs=1\n+    kmeans_sequential = KMeans(n_clusters=10, random_state=2, n_jobs=1)\n+    kmeans_sequential.fit(X)\n+    inertia_sequential = kmeans_sequential.inertia_\n+    \n+    # Run KMeans with various n_jobs values\n+    for n_jobs in [2, 3, 4, -1]:\n+        kmeans_parallel = KMeans(n_clusters=10, random_state=2, n_jobs=n_jobs)\n+        kmeans_parallel.fit(X)\n+        assert kmeans_parallel.inertia_ == inertia_sequential, \\\n+            f\"Inertia mismatch: n_jobs=1 gives {inertia_sequential}, n_jobs={n_jobs} gives {kmeans_parallel.inertia_}\"\n+    \n+    print(\"Test passed: KMeans gives consistent results regardless of n_jobs\")\n+\n+def test_kmeans_consistency_different_algorithms():\n+    # Generate some data\n+    X, y = make_blobs(n_samples=1000, centers=5, n_features=10, random_state=42)\n+    \n+    for algorithm in ['full', 'elkan']:\n+        # Skip elkan for sparse data\n+        if algorithm == 'elkan':\n+            continue\n+            \n+        inertia_sequential = None\n+        for n_jobs in [1, 2, -1]:\n+            kmeans = KMeans(n_clusters=5, random_state=42, n_jobs=n_jobs, algorithm=algorithm, n_init=10)\n+            kmeans.fit(X)\n+            \n+            if inertia_sequential is None:\n+                inertia_sequential = kmeans.inertia_\n+            else:\n+                assert kmeans.inertia_ == inertia_sequential, \\\n+                    f\"Algorithm {algorithm}: Inertia mismatch with n_jobs={n_jobs}\"\n+        \n+        print(f\"Test passed for algorithm={algorithm}\")\n+\n+if __name__ == \"__main__\":\n+    test_kmeans_consistency_n_jobs()\n+    test_kmeans_consistency_different_algorithms()\ndiff --git a/test_kmeans_reproducibility.py b/test_kmeans_reproducibility.py\nnew file mode 100644\nindex 000000000..72a9eb7ff\n--- /dev/null\n+++ b/test_kmeans_reproducibility.py\n@@ -0,0 +1,11 @@\n+from sklearn.cluster import KMeans\n+from sklearn.datasets import make_blobs\n+\n+# Generate some data\n+X, y = make_blobs(n_samples=10000, centers=10, n_features=2, random_state=2)\n+\n+# Run KMeans with various n_jobs values\n+for n_jobs in range(1, 5):\n+    kmeans = KMeans(n_clusters=10, random_state=2, n_jobs=n_jobs)\n+    kmeans.fit(X)\n+    print(f'(n_jobs={n_jobs}) kmeans.inertia_ = {kmeans.inertia_}')\n", "gold_standard_diff": "diff --git a/doc/whats_new/v0.22.rst b/doc/whats_new/v0.22.rst\nindex 7be27894abe0f..0e4cee72c4c1e 100644\n--- a/doc/whats_new/v0.22.rst\n+++ b/doc/whats_new/v0.22.rst\n@@ -26,6 +26,8 @@ random sampling procedures.\n \n - :class:`linear_model.Ridge` when `X` is sparse. |Fix|\n \n+- :class:`cluster.KMeans` when `n_jobs=1`. |Fix|\n+\n Details are listed in the changelog below.\n \n (While we are trying to better inform users by providing this information, we\n@@ -283,6 +285,10 @@ Changelog\n   match `spectral_clustering`.\n   :pr:`13726` by :user:`Shuzhe Xiao <fdas3213>`.\n \n+- |Fix| Fixed a bug where :class:`cluster.KMeans` produced inconsistent results\n+  between `n_jobs=1` and `n_jobs>1` due to the handling of the random state.\n+  :pr:`9288` by :user:`Bryan Yang <bryanyang0528>`.\n+\n :mod:`sklearn.feature_selection`\n ................................\n \ndiff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py\nindex b7fbdf7da3ad1..4a76a40cc87c1 100644\n--- a/sklearn/cluster/k_means_.py\n+++ b/sklearn/cluster/k_means_.py\n@@ -360,16 +360,18 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',\n     else:\n         raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\"\n                          \" %s\" % str(algorithm))\n+\n+    seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n     if effective_n_jobs(n_jobs) == 1:\n         # For a single thread, less memory is needed if we just store one set\n         # of the best results (as opposed to one set per run per thread).\n-        for it in range(n_init):\n+        for seed in seeds:\n             # run a k-means once\n             labels, inertia, centers, n_iter_ = kmeans_single(\n                 X, sample_weight, n_clusters, max_iter=max_iter, init=init,\n                 verbose=verbose, precompute_distances=precompute_distances,\n                 tol=tol, x_squared_norms=x_squared_norms,\n-                random_state=random_state)\n+                random_state=seed)\n             # determine if these results are the best so far\n             if best_inertia is None or inertia < best_inertia:\n                 best_labels = labels.copy()\n@@ -378,7 +380,6 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',\n                 best_n_iter = n_iter_\n     else:\n         # parallelisation of k-means runs\n-        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n         results = Parallel(n_jobs=n_jobs, verbose=0)(\n             delayed(kmeans_single)(X, sample_weight, n_clusters,\n                                    max_iter=max_iter, init=init,\ndiff --git a/sklearn/cluster/tests/test_k_means.py b/sklearn/cluster/tests/test_k_means.py\nindex 362b0a9145fca..2b3d2a0642ba7 100644\n--- a/sklearn/cluster/tests/test_k_means.py\n+++ b/sklearn/cluster/tests/test_k_means.py\n@@ -951,3 +951,13 @@ def test_minibatch_kmeans_partial_fit_int_data():\n     km = MiniBatchKMeans(n_clusters=2)\n     km.partial_fit(X)\n     assert km.cluster_centers_.dtype.kind == \"f\"\n+\n+\n+def test_result_of_kmeans_equal_in_diff_n_jobs():\n+    # PR 9288\n+    rnd = np.random.RandomState(0)\n+    X = rnd.normal(size=(50, 10))\n+\n+    result_1 = KMeans(n_clusters=3, random_state=0, n_jobs=1).fit(X).labels_\n+    result_2 = KMeans(n_clusters=3, random_state=0, n_jobs=2).fit(X).labels_\n+    assert_array_equal(result_1, result_2)\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/9288"}