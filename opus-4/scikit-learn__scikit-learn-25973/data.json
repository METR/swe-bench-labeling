{"instance_id": "scikit-learn__scikit-learn-25973", "score": "C", "diff": "diff --git a/sklearn/feature_selection/_sequential.py b/sklearn/feature_selection/_sequential.py\nindex e983c55de..dacf4ab20 100644\n--- a/sklearn/feature_selection/_sequential.py\n+++ b/sklearn/feature_selection/_sequential.py\n@@ -13,7 +13,7 @@ from ..utils._param_validation import HasMethods, Hidden, Interval, StrOptions\n from ..utils._param_validation import RealNotInt\n from ..utils._tags import _safe_tags\n from ..utils.validation import check_is_fitted\n-from ..model_selection import cross_val_score\n+from ..model_selection import cross_val_score, check_cv\n from ..metrics import get_scorer_names\n \n \n@@ -259,6 +259,9 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator\n         if self.tol is not None and self.tol < 0 and self.direction == \"forward\":\n             raise ValueError(\"tol must be positive when doing forward selection\")\n \n+        # Validate cv to ensure it's reusable across multiple calls\n+        cv = check_cv(self.cv, y, classifier=tags.get(\"requires_positive_y\", False))\n+\n         cloned_estimator = clone(self.estimator)\n \n         # the current mask corresponds to the set of features:\n@@ -275,7 +278,7 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator\n         is_auto_select = self.tol is not None and self.n_features_to_select == \"auto\"\n         for _ in range(n_iterations):\n             new_feature_idx, new_score = self._get_best_new_feature_score(\n-                cloned_estimator, X, y, current_mask\n+                cloned_estimator, X, y, current_mask, cv\n             )\n             if is_auto_select and ((new_score - old_score) < self.tol):\n                 break\n@@ -291,7 +294,7 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator\n \n         return self\n \n-    def _get_best_new_feature_score(self, estimator, X, y, current_mask):\n+    def _get_best_new_feature_score(self, estimator, X, y, current_mask, cv):\n         # Return the best new feature and its score to add to the current_mask,\n         # i.e. return the best new feature and its score to add (resp. remove)\n         # when doing forward selection (resp. backward selection).\n@@ -309,7 +312,7 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator\n                 estimator,\n                 X_new,\n                 y,\n-                cv=self.cv,\n+                cv=cv,\n                 scoring=self.scoring,\n                 n_jobs=self.n_jobs,\n             ).mean()\ndiff --git a/sklearn/feature_selection/tests/test_sequential.py b/sklearn/feature_selection/tests/test_sequential.py\nindex f6451a360..2df20800f 100644\n--- a/sklearn/feature_selection/tests/test_sequential.py\n+++ b/sklearn/feature_selection/tests/test_sequential.py\n@@ -9,8 +9,10 @@ from sklearn.feature_selection import SequentialFeatureSelector\n from sklearn.datasets import make_regression, make_blobs\n from sklearn.linear_model import LinearRegression\n from sklearn.ensemble import HistGradientBoostingRegressor\n-from sklearn.model_selection import cross_val_score\n+from sklearn.model_selection import cross_val_score, LeaveOneGroupOut, KFold\n from sklearn.cluster import KMeans\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.datasets import make_classification\n \n \n def test_bad_n_features_to_select():\ndiff --git a/test_cv_iterable_fix.py b/test_cv_iterable_fix.py\nnew file mode 100644\nindex 000000000..3b3acb68b\n--- /dev/null\n+++ b/test_cv_iterable_fix.py\n@@ -0,0 +1,75 @@\n+\"\"\"\n+Test that SequentialFeatureSelector works with iterable cv parameter.\n+This is a regression test for the issue where passing an iterable of splits\n+would fail because the iterator gets exhausted after the first feature evaluation.\n+\"\"\"\n+import pytest\n+from sklearn.datasets import make_classification\n+from sklearn.feature_selection import SequentialFeatureSelector\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.model_selection import LeaveOneGroupOut, KFold\n+import numpy as np\n+\n+\n+def test_sequential_feature_selector_with_iterable_cv():\n+    \"\"\"Test that SequentialFeatureSelector works with iterator/generator cv.\"\"\"\n+    X, y = make_classification(n_samples=50, n_features=10, random_state=42)\n+    \n+    # Test with LeaveOneGroupOut iterator\n+    groups = np.zeros_like(y, dtype=int)\n+    groups[y.size//2:] = 1\n+    \n+    cv = LeaveOneGroupOut()\n+    splits = cv.split(X, y, groups=groups)\n+    \n+    clf = KNeighborsClassifier(n_neighbors=5)\n+    seq = SequentialFeatureSelector(clf, n_features_to_select=3, scoring='accuracy', cv=splits)\n+    \n+    # This should not raise an IndexError anymore\n+    seq.fit(X, y)\n+    \n+    assert seq.n_features_to_select_ == 3\n+    assert seq.transform(X).shape == (50, 3)\n+\n+\n+def test_sequential_feature_selector_with_list_of_splits():\n+    \"\"\"Test that SequentialFeatureSelector works with a list of splits.\"\"\"\n+    X, y = make_classification(n_samples=50, n_features=10, random_state=42)\n+    \n+    # Test with a list of splits from KFold\n+    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n+    splits_list = list(cv.split(X))\n+    \n+    clf = KNeighborsClassifier(n_neighbors=5)\n+    seq = SequentialFeatureSelector(clf, n_features_to_select=4, scoring='accuracy', cv=splits_list)\n+    \n+    seq.fit(X, y)\n+    \n+    assert seq.n_features_to_select_ == 4\n+    assert seq.transform(X).shape == (50, 4)\n+\n+\n+def test_sequential_feature_selector_backwards_compatibility():\n+    \"\"\"Test that existing behavior with cv objects and integers still works.\"\"\"\n+    X, y = make_classification(n_samples=50, n_features=10, random_state=42)\n+    clf = KNeighborsClassifier(n_neighbors=5)\n+    \n+    # Test with CV splitter object\n+    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n+    seq1 = SequentialFeatureSelector(clf, n_features_to_select=3, scoring='accuracy', cv=cv)\n+    seq1.fit(X, y)\n+    \n+    assert seq1.n_features_to_select_ == 3\n+    \n+    # Test with integer cv\n+    seq2 = SequentialFeatureSelector(clf, n_features_to_select=3, scoring='accuracy', cv=3)\n+    seq2.fit(X, y)\n+    \n+    assert seq2.n_features_to_select_ == 3\n+\n+\n+if __name__ == \"__main__\":\n+    test_sequential_feature_selector_with_iterable_cv()\n+    test_sequential_feature_selector_with_list_of_splits()\n+    test_sequential_feature_selector_backwards_compatibility()\n+    print(\"All tests passed!\")\ndiff --git a/test_fix_comprehensive.py b/test_fix_comprehensive.py\nnew file mode 100644\nindex 000000000..60f695059\n--- /dev/null\n+++ b/test_fix_comprehensive.py\n@@ -0,0 +1,67 @@\n+from sklearn.datasets import make_classification\n+from sklearn.feature_selection import SequentialFeatureSelector\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.model_selection import LeaveOneGroupOut, KFold\n+\n+import numpy as np\n+\n+# Test 1: Using iterator splits from LeaveOneGroupOut\n+print(\"Test 1: Using iterator splits...\")\n+X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n+\n+groups = np.zeros_like(y, dtype=int)\n+groups[y.size//2:] = 1\n+\n+cv = LeaveOneGroupOut()\n+splits = cv.split(X, y, groups=groups)\n+\n+clf = KNeighborsClassifier(n_neighbors=5)\n+\n+seq = SequentialFeatureSelector(clf, n_features_to_select=5, scoring='accuracy', cv=splits)\n+seq.fit(X, y)\n+\n+print(\"Number of selected features:\", seq.n_features_to_select_)\n+print(\"Selected features shape:\", seq.transform(X).shape)\n+assert seq.n_features_to_select_ == 5\n+assert seq.transform(X).shape == (100, 5)\n+print(\"\u2713 Test 1 passed!\")\n+\n+# Test 2: Using list of splits from KFold\n+print(\"\\nTest 2: Using list of splits from KFold...\")\n+cv = KFold(n_splits=3, shuffle=True, random_state=42)\n+splits_list = list(cv.split(X))\n+\n+seq2 = SequentialFeatureSelector(clf, n_features_to_select=7, scoring='accuracy', cv=splits_list)\n+seq2.fit(X, y)\n+\n+print(\"Number of selected features:\", seq2.n_features_to_select_)\n+print(\"Selected features shape:\", seq2.transform(X).shape)\n+assert seq2.n_features_to_select_ == 7\n+assert seq2.transform(X).shape == (100, 7)\n+print(\"\u2713 Test 2 passed!\")\n+\n+# Test 3: Using CV splitter object directly \n+print(\"\\nTest 3: Using CV splitter object directly...\")\n+cv = KFold(n_splits=5, shuffle=True, random_state=42)\n+\n+seq3 = SequentialFeatureSelector(clf, n_features_to_select=3, scoring='accuracy', cv=cv)\n+seq3.fit(X, y)\n+\n+print(\"Number of selected features:\", seq3.n_features_to_select_)\n+print(\"Selected features shape:\", seq3.transform(X).shape)\n+assert seq3.n_features_to_select_ == 3\n+assert seq3.transform(X).shape == (100, 3)\n+print(\"\u2713 Test 3 passed!\")\n+\n+# Test 4: Using integer cv\n+print(\"\\nTest 4: Using integer cv...\")\n+seq4 = SequentialFeatureSelector(clf, n_features_to_select=4, scoring='accuracy', cv=3)\n+seq4.fit(X, y)\n+\n+print(\"Number of selected features:\", seq4.n_features_to_select_)\n+print(\"Selected features shape:\", seq4.transform(X).shape) \n+assert seq4.n_features_to_select_ == 4\n+assert seq4.transform(X).shape == (100, 4)\n+print(\"\u2713 Test 4 passed!\")\n+\n+print(\"\\n\u2705 All tests passed!\")\ndiff --git a/test_sequential_splits.py b/test_sequential_splits.py\nnew file mode 100644\nindex 000000000..cd49b4026\n--- /dev/null\n+++ b/test_sequential_splits.py\n@@ -0,0 +1,19 @@\n+from sklearn.datasets import make_classification\n+from sklearn.feature_selection import SequentialFeatureSelector\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.model_selection import LeaveOneGroupOut\n+\n+import numpy as np\n+\n+X, y = make_classification()\n+\n+groups = np.zeros_like(y, dtype=int)\n+groups[y.size//2:] = 1\n+\n+cv = LeaveOneGroupOut()\n+splits = cv.split(X, y, groups=groups)\n+\n+clf = KNeighborsClassifier(n_neighbors=5)\n+\n+seq = SequentialFeatureSelector(clf, n_features_to_select=5, scoring='accuracy', cv=splits)\n+seq.fit(X, y)\n", "gold_standard_diff": "diff --git a/doc/whats_new/v1.3.rst b/doc/whats_new/v1.3.rst\nindex 51b4214145216..add8f896dcb1a 100644\n--- a/doc/whats_new/v1.3.rst\n+++ b/doc/whats_new/v1.3.rst\n@@ -146,6 +146,9 @@ Changelog\n - |Enhancement| All selectors in :mod:`sklearn.feature_selection` will preserve\n   a DataFrame's dtype when transformed. :pr:`25102` by `Thomas Fan`_.\n \n+- |Fix| :class:`feature_selection.SequentialFeatureSelector`'s `cv` parameter\n+  now supports generators. :pr:`25973` by `Yao Xiao <Charlie-XIAO>`.\n+\n :mod:`sklearn.base`\n ...................\n \ndiff --git a/sklearn/feature_selection/_sequential.py b/sklearn/feature_selection/_sequential.py\nindex e983c55de7d25..2498cd53b39f6 100644\n--- a/sklearn/feature_selection/_sequential.py\n+++ b/sklearn/feature_selection/_sequential.py\n@@ -8,12 +8,12 @@\n import warnings\n \n from ._base import SelectorMixin\n-from ..base import BaseEstimator, MetaEstimatorMixin, clone\n+from ..base import BaseEstimator, MetaEstimatorMixin, clone, is_classifier\n from ..utils._param_validation import HasMethods, Hidden, Interval, StrOptions\n from ..utils._param_validation import RealNotInt\n from ..utils._tags import _safe_tags\n from ..utils.validation import check_is_fitted\n-from ..model_selection import cross_val_score\n+from ..model_selection import cross_val_score, check_cv\n from ..metrics import get_scorer_names\n \n \n@@ -259,6 +259,8 @@ def fit(self, X, y=None):\n         if self.tol is not None and self.tol < 0 and self.direction == \"forward\":\n             raise ValueError(\"tol must be positive when doing forward selection\")\n \n+        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n+\n         cloned_estimator = clone(self.estimator)\n \n         # the current mask corresponds to the set of features:\n@@ -275,7 +277,7 @@ def fit(self, X, y=None):\n         is_auto_select = self.tol is not None and self.n_features_to_select == \"auto\"\n         for _ in range(n_iterations):\n             new_feature_idx, new_score = self._get_best_new_feature_score(\n-                cloned_estimator, X, y, current_mask\n+                cloned_estimator, X, y, cv, current_mask\n             )\n             if is_auto_select and ((new_score - old_score) < self.tol):\n                 break\n@@ -291,7 +293,7 @@ def fit(self, X, y=None):\n \n         return self\n \n-    def _get_best_new_feature_score(self, estimator, X, y, current_mask):\n+    def _get_best_new_feature_score(self, estimator, X, y, cv, current_mask):\n         # Return the best new feature and its score to add to the current_mask,\n         # i.e. return the best new feature and its score to add (resp. remove)\n         # when doing forward selection (resp. backward selection).\n@@ -309,7 +311,7 @@ def _get_best_new_feature_score(self, estimator, X, y, current_mask):\n                 estimator,\n                 X_new,\n                 y,\n-                cv=self.cv,\n+                cv=cv,\n                 scoring=self.scoring,\n                 n_jobs=self.n_jobs,\n             ).mean()\ndiff --git a/sklearn/feature_selection/tests/test_sequential.py b/sklearn/feature_selection/tests/test_sequential.py\nindex f6451a36005ac..98134addc39e7 100644\n--- a/sklearn/feature_selection/tests/test_sequential.py\n+++ b/sklearn/feature_selection/tests/test_sequential.py\n@@ -6,11 +6,12 @@\n from sklearn.preprocessing import StandardScaler\n from sklearn.pipeline import make_pipeline\n from sklearn.feature_selection import SequentialFeatureSelector\n-from sklearn.datasets import make_regression, make_blobs\n+from sklearn.datasets import make_regression, make_blobs, make_classification\n from sklearn.linear_model import LinearRegression\n from sklearn.ensemble import HistGradientBoostingRegressor\n-from sklearn.model_selection import cross_val_score\n+from sklearn.model_selection import cross_val_score, LeaveOneGroupOut\n from sklearn.cluster import KMeans\n+from sklearn.neighbors import KNeighborsClassifier\n \n \n def test_bad_n_features_to_select():\n@@ -314,3 +315,22 @@ def test_backward_neg_tol():\n \n     assert 0 < sfs.get_support().sum() < X.shape[1]\n     assert new_score < initial_score\n+\n+\n+def test_cv_generator_support():\n+    \"\"\"Check that no exception raised when cv is generator\n+\n+    non-regression test for #25957\n+    \"\"\"\n+    X, y = make_classification(random_state=0)\n+\n+    groups = np.zeros_like(y, dtype=int)\n+    groups[y.size // 2 :] = 1\n+\n+    cv = LeaveOneGroupOut()\n+    splits = cv.split(X, y, groups=groups)\n+\n+    knc = KNeighborsClassifier(n_neighbors=5)\n+\n+    sfs = SequentialFeatureSelector(knc, n_features_to_select=5, cv=splits)\n+    sfs.fit(X, y)\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/25973"}