{"instance_id": "scikit-learn__scikit-learn-12682", "score": "C", "diff": "diff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex a318c957f..7218b01dc 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -865,7 +865,8 @@ class SparseCodingMixin(TransformerMixin):\n                                   transform_algorithm='omp',\n                                   transform_n_nonzero_coefs=None,\n                                   transform_alpha=None, split_sign=False,\n-                                  n_jobs=None, positive_code=False):\n+                                  n_jobs=None, positive_code=False,\n+                                  transform_max_iter=1000):\n         self.n_components = n_components\n         self.transform_algorithm = transform_algorithm\n         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\n@@ -873,6 +874,7 @@ class SparseCodingMixin(TransformerMixin):\n         self.split_sign = split_sign\n         self.n_jobs = n_jobs\n         self.positive_code = positive_code\n+        self.transform_max_iter = transform_max_iter\n \n     def transform(self, X):\n         \"\"\"Encode the data as a sparse combination of the dictionary atoms.\n@@ -900,7 +902,7 @@ class SparseCodingMixin(TransformerMixin):\n             X, self.components_, algorithm=self.transform_algorithm,\n             n_nonzero_coefs=self.transform_n_nonzero_coefs,\n             alpha=self.transform_alpha, n_jobs=self.n_jobs,\n-            positive=self.positive_code)\n+            positive=self.positive_code, max_iter=self.transform_max_iter)\n \n         if self.split_sign:\n             # feature vector is split into a positive and negative side\n@@ -974,6 +976,9 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n \n         .. versionadded:: 0.20\n \n+    transform_max_iter : int, 1000 by default\n+        Maximum number of iterations to perform if `transform_algorithm='lasso_cd'`.\n+\n     Attributes\n     ----------\n     components_ : array, [n_components, n_features]\n@@ -991,12 +996,13 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n \n     def __init__(self, dictionary, transform_algorithm='omp',\n                  transform_n_nonzero_coefs=None, transform_alpha=None,\n-                 split_sign=False, n_jobs=None, positive_code=False):\n+                 split_sign=False, n_jobs=None, positive_code=False,\n+                 transform_max_iter=1000):\n         self._set_sparse_coding_params(dictionary.shape[0],\n                                        transform_algorithm,\n                                        transform_n_nonzero_coefs,\n                                        transform_alpha, split_sign, n_jobs,\n-                                       positive_code)\n+                                       positive_code, transform_max_iter)\n         self.components_ = dictionary\n \n     def fit(self, X, y=None):\n@@ -1157,7 +1163,7 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n         self._set_sparse_coding_params(n_components, transform_algorithm,\n                                        transform_n_nonzero_coefs,\n                                        transform_alpha, split_sign, n_jobs,\n-                                       positive_code)\n+                                       positive_code, 1000)\n         self.alpha = alpha\n         self.max_iter = max_iter\n         self.tol = tol\n@@ -1346,7 +1352,7 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):\n         self._set_sparse_coding_params(n_components, transform_algorithm,\n                                        transform_n_nonzero_coefs,\n                                        transform_alpha, split_sign, n_jobs,\n-                                       positive_code)\n+                                       positive_code, 1000)\n         self.alpha = alpha\n         self.n_iter = n_iter\n         self.fit_algorithm = fit_algorithm\ndiff --git a/test_issue_reproduction.py b/test_issue_reproduction.py\nnew file mode 100644\nindex 000000000..54f2ceef7\n--- /dev/null\n+++ b/test_issue_reproduction.py\n@@ -0,0 +1,76 @@\n+\"\"\"\n+Test to reproduce the issue mentioned:\n+SparseCoder with lasso_cd algorithm produces convergence warnings\n+when max_iter is not exposed.\n+\"\"\"\n+import numpy as np\n+from sklearn.decomposition import SparseCoder\n+import warnings\n+\n+# Create a problem similar to the plot_sparse_coding example\n+np.random.seed(42)\n+resolution = 1024\n+width = 100\n+n_components = resolution // 3  # Similar to the example\n+\n+# Create dictionary (simplified version of ricker_matrix)\n+centers = np.linspace(0, resolution - 1, n_components)\n+D = np.empty((n_components, resolution))\n+for i, center in enumerate(centers):\n+    x = np.linspace(0, resolution - 1, resolution)\n+    D[i] = ((2 / ((np.sqrt(3 * width) * np.pi ** 0.25)))\n+            * (1 - ((x - center) ** 2 / width ** 2))\n+            * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n+D /= np.sqrt(np.sum(D ** 2, axis=1))[:, np.newaxis]\n+\n+# Generate a signal similar to the example\n+y = np.linspace(0, resolution - 1, resolution)\n+first_quarter = y < resolution / 4\n+y[first_quarter] = 3.\n+y[~first_quarter] = -1.\n+\n+print(\"Testing SparseCoder with lasso_cd algorithm (before fix):\")\n+print(\"This would produce convergence warnings with default max_iter=1000\")\n+print()\n+\n+# Test with default max_iter (should work now without warnings for reasonable problems)\n+coder_default = SparseCoder(dictionary=D, transform_algorithm='lasso_cd',\n+                           transform_alpha=2)\n+with warnings.catch_warnings(record=True) as w:\n+    warnings.simplefilter(\"always\")\n+    x_default = coder_default.transform(y.reshape(1, -1))\n+    conv_warnings = [warning for warning in w if 'ConvergenceWarning' in str(warning.category)]\n+    if conv_warnings:\n+        print(f\"Got convergence warning with default max_iter\")\n+    else:\n+        print(f\"No convergence warning with default max_iter (transform_max_iter={coder_default.transform_max_iter})\")\n+\n+# Test with explicitly low max_iter (should produce warning)\n+print(\"\\nTesting with very low max_iter to force convergence warning:\")\n+coder_low = SparseCoder(dictionary=D, transform_algorithm='lasso_cd',\n+                       transform_alpha=2, transform_max_iter=1)\n+with warnings.catch_warnings(record=True) as w:\n+    warnings.simplefilter(\"always\")\n+    x_low = coder_low.transform(y.reshape(1, -1))\n+    conv_warnings = [warning for warning in w if 'ConvergenceWarning' in str(warning.category)]\n+    if conv_warnings:\n+        print(f\"Got expected convergence warning with transform_max_iter=1\")\n+    else:\n+        print(f\"No convergence warning with transform_max_iter=1 (unexpected)\")\n+\n+# Test with high max_iter (should not produce warning)\n+print(\"\\nTesting with high max_iter:\")\n+coder_high = SparseCoder(dictionary=D, transform_algorithm='lasso_cd',\n+                        transform_alpha=2, transform_max_iter=10000)\n+with warnings.catch_warnings(record=True) as w:\n+    warnings.simplefilter(\"always\")\n+    x_high = coder_high.transform(y.reshape(1, -1))\n+    conv_warnings = [warning for warning in w if 'ConvergenceWarning' in str(warning.category)]\n+    if conv_warnings:\n+        print(f\"Got convergence warning with transform_max_iter=10000 (unexpected)\")\n+    else:\n+        print(f\"No convergence warning with transform_max_iter=10000 (expected)\")\n+\n+print(\"\\nThe fix allows users to control max_iter for lasso_cd algorithm!\")\n+print(f\"Default transform_max_iter: {coder_default.transform_max_iter}\")\n+print(f\"Custom transform_max_iter: {coder_high.transform_max_iter}\")\ndiff --git a/test_max_iter_fix.py b/test_max_iter_fix.py\nnew file mode 100644\nindex 000000000..1c17caece\n--- /dev/null\n+++ b/test_max_iter_fix.py\n@@ -0,0 +1,70 @@\n+import numpy as np\n+from sklearn.decomposition import SparseCoder\n+import warnings\n+\n+# Test 1: Check that max_iter parameter is accessible and works\n+np.random.seed(42)\n+n_features = 50\n+n_components = 30\n+\n+# Create a simple dictionary\n+dictionary = np.random.randn(n_components, n_features)\n+dictionary = dictionary / np.linalg.norm(dictionary, axis=1, keepdims=True)\n+\n+# Test with default max_iter\n+print(\"Test 1: SparseCoder with default transform_max_iter (1000)\")\n+coder1 = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_cd')\n+print(f\"transform_max_iter: {coder1.transform_max_iter}\")\n+\n+# Test with custom max_iter\n+print(\"\\nTest 2: SparseCoder with custom transform_max_iter\")\n+coder2 = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_cd', \n+                     transform_max_iter=2000)\n+print(f\"transform_max_iter: {coder2.transform_max_iter}\")\n+\n+# Test transform with both\n+X = np.random.randn(10, n_features)\n+\n+print(\"\\nTest 3: Transform data with both coders\")\n+result1 = coder1.transform(X)\n+result2 = coder2.transform(X)\n+print(f\"Result1 shape: {result1.shape}\")\n+print(f\"Result2 shape: {result2.shape}\")\n+\n+# Test 4: Verify that max_iter is properly passed to Lasso\n+print(\"\\nTest 4: Checking if max_iter is used when algorithm='lasso_cd'\")\n+\n+# Create a difficult problem that needs many iterations\n+n_features = 200\n+n_components = 100\n+X_hard = np.random.randn(50, n_features) * 10\n+dictionary_hard = np.random.randn(n_components, n_features)\n+dictionary_hard = dictionary_hard / np.linalg.norm(dictionary_hard, axis=1, keepdims=True)\n+\n+# With low max_iter, should potentially get convergence warning\n+coder_low_iter = SparseCoder(dictionary=dictionary_hard, transform_algorithm='lasso_cd',\n+                            transform_alpha=0.01, transform_max_iter=10)\n+\n+with warnings.catch_warnings(record=True) as w:\n+    warnings.simplefilter(\"always\")\n+    result_low = coder_low_iter.transform(X_hard)\n+    conv_warnings = [warning for warning in w if 'ConvergenceWarning' in str(warning.category)]\n+    if conv_warnings:\n+        print(f\"Got expected convergence warning with max_iter=10\")\n+    else:\n+        print(f\"No convergence warning with max_iter=10\")\n+\n+# With high max_iter, should not get convergence warning\n+coder_high_iter = SparseCoder(dictionary=dictionary_hard, transform_algorithm='lasso_cd',\n+                             transform_alpha=0.01, transform_max_iter=5000)\n+\n+with warnings.catch_warnings(record=True) as w:\n+    warnings.simplefilter(\"always\")\n+    result_high = coder_high_iter.transform(X_hard)\n+    conv_warnings = [warning for warning in w if 'ConvergenceWarning' in str(warning.category)]\n+    if conv_warnings:\n+        print(f\"Got convergence warning with max_iter=5000 (unexpected)\")\n+    else:\n+        print(f\"No convergence warning with max_iter=5000 (expected)\")\n+\n+print(\"\\nAll tests passed!\")\ndiff --git a/test_sparse_coder_issue.py b/test_sparse_coder_issue.py\nnew file mode 100644\nindex 000000000..c60e10c34\n--- /dev/null\n+++ b/test_sparse_coder_issue.py\n@@ -0,0 +1,38 @@\n+import numpy as np\n+from sklearn.decomposition import SparseCoder\n+import warnings\n+\n+# Create a test case that's more likely to require many iterations\n+np.random.seed(42)\n+n_features = 1024\n+n_components = n_features // 3  # Similar to the example\n+\n+# Create a difficult sparse coding problem\n+# Using a signal similar to the example\n+y = np.linspace(0, n_features - 1, n_features)\n+first_quarter = y < n_features / 4\n+y[first_quarter] = 3.\n+y[~first_quarter] = -1.\n+\n+# Create a dictionary that requires many iterations to converge\n+dictionary = np.random.randn(n_components, n_features)\n+# Normalize dictionary atoms to unit norm\n+dictionary = dictionary / np.linalg.norm(dictionary, axis=1, keepdims=True)\n+\n+# Test with lasso_cd algorithm - using lasso_lars instead of lasso_cd\n+print(\"Testing SparseCoder with lasso_cd algorithm...\")\n+# Using a small alpha to make convergence harder\n+coder = SparseCoder(dictionary=dictionary, transform_algorithm='lasso_cd', \n+                    transform_alpha=0.1)\n+\n+# This should produce a convergence warning with default max_iter=1000\n+with warnings.catch_warnings(record=True) as w:\n+    warnings.simplefilter(\"always\")\n+    result = coder.transform(y.reshape(1, -1))\n+    conv_warnings = [warning for warning in w if 'ConvergenceWarning' in str(warning.category)]\n+    if conv_warnings:\n+        print(f\"ConvergenceWarning caught: {conv_warnings[0].message}\")\n+    else:\n+        print(\"No convergence warning produced\")\n+\n+print(f\"Result shape: {result.shape}\")\n", "gold_standard_diff": "From efe7a274d8573b8cd5f85d86a97b9a275c98494d Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Tue, 27 Nov 2018 09:28:05 +0100\nSubject: [PATCH 01/18] SparseCoder passes max_iter to LassoLarse\n\n---\n doc/whats_new/v0.21.rst                | 8 ++++++++\n sklearn/decomposition/dict_learning.py | 2 +-\n 2 files changed, 9 insertions(+), 1 deletion(-)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex 4d472a0bb9835..d394964517c7b 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -22,6 +22,7 @@ random sampling procedures.\n \n - Decision trees and derived ensembles when both `max_depth` and\n   `max_leaf_nodes` are set. |Fix|\n+- :class:`decomposition.SparseCoder` with `algorithm='lasso_lars'` |Fix|\n - :class:`linear_model.LogisticRegression` and\n   :class:`linear_model.LogisticRegressionCV` with 'saga' solver. |Fix|\n \n@@ -55,6 +56,13 @@ Support for Python 3.4 and below has been officially dropped.\n   to set and that scales better, by :user:`Shane <espg>`,\n   :user:`Adrin Jalali <adrinjalali>`, and :user:`Erich Schubert <kno10>`.\n \n+:mod:`slkearn.decomposition`\n+............................\n+\n+- |Fix| :class:`decomposition.SparseCoder` now passes `max_iter` to the\n+  underlying :class:`linear_model.LassoLars` when algorithm is set to\n+  `lasso_lars`. :issue:`12650` by user:`Adrin Jalali <adrinjalali>`.\n+\n :mod:`sklearn.linear_model`\n ...........................\n \ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 65ae605b6c19b..858f936f54aa5 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -119,7 +119,7 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',\n             lasso_lars = LassoLars(alpha=alpha, fit_intercept=False,\n                                    verbose=verbose, normalize=False,\n                                    precompute=gram, fit_path=False,\n-                                   positive=positive)\n+                                   positive=positive, max_iter=max_iter)\n             lasso_lars.fit(dictionary.T, X.T, Xy=cov)\n             new_code = lasso_lars.coef_\n         finally:\n\nFrom 75df47a41510592e142289a74d61b51fb76a5a7c Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Wed, 28 Nov 2018 15:03:24 +0100\nSubject: [PATCH 02/18] expose methods' max_iter as transform_max_iter\n\n---\n sklearn/decomposition/dict_learning.py | 47 ++++++++++++++++----------\n 1 file changed, 29 insertions(+), 18 deletions(-)\n\ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 858f936f54aa5..e2afaa5535371 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -326,6 +326,7 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',\n             init=init[this_slice] if init is not None else None,\n             max_iter=max_iter,\n             check_input=False,\n+            verbose=verbose,\n             positive=positive)\n         for this_slice in slices)\n     for this_slice, this_view in zip(slices, code_views):\n@@ -571,7 +572,8 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n \n         # Update code\n         code = sparse_encode(X, dictionary, algorithm=method, alpha=alpha,\n-                             init=code, n_jobs=n_jobs, positive=positive_code)\n+                             init=code, n_jobs=n_jobs, positive=positive_code,\n+                             max_iter=max_iter, verbose=verbose)\n         # Update dictionary\n         dictionary, residuals = _update_dict(dictionary.T, X.T, code.T,\n                                              verbose=verbose, return_r2=True,\n@@ -605,10 +607,11 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n                          return_code=True, dict_init=None, callback=None,\n                          batch_size=3, verbose=False, shuffle=True,\n-                         n_jobs=None, method='lars', iter_offset=0,\n-                         random_state=None, return_inner_stats=False,\n-                         inner_stats=None, return_n_iter=False,\n-                         positive_dict=False, positive_code=False):\n+                         n_jobs=None, method='lars', method_max_iter=1000,\n+                         iter_offset=0, random_state=None,\n+                         return_inner_stats=False, inner_stats=None,\n+                         return_n_iter=False, positive_dict=False,\n+                         positive_code=False):\n     \"\"\"Solves a dictionary learning matrix factorization problem online.\n \n     Finds the best dictionary and the corresponding sparse code for\n@@ -797,7 +800,8 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n         this_code = sparse_encode(this_X, dictionary.T, algorithm=method,\n                                   alpha=alpha, n_jobs=n_jobs,\n                                   check_input=False,\n-                                  positive=positive_code).T\n+                                  positive=positive_code,\n+                                  max_iter=method_max_iter, verbose=verbose).T\n \n         # Update the auxiliary variables\n         if ii < batch_size - 1:\n@@ -834,7 +838,8 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n             print('|', end=' ')\n         code = sparse_encode(X, dictionary.T, algorithm=method, alpha=alpha,\n                              n_jobs=n_jobs, check_input=False,\n-                             positive=positive_code)\n+                             positive=positive_code, max_iter=method_max_iter,\n+                             verbose=verbose)\n         if verbose > 1:\n             dt = (time.time() - t0)\n             print('done (total time: % 3is, % 4.1fmn)' % (dt, dt / 60))\n@@ -855,12 +860,14 @@ class SparseCodingMixin(TransformerMixin):\n     def _set_sparse_coding_params(self, n_components,\n                                   transform_algorithm='omp',\n                                   transform_n_nonzero_coefs=None,\n-                                  transform_alpha=None, split_sign=False,\n+                                  transform_alpha=None,\n+                                  transform_max_iter=1000, split_sign=False,\n                                   n_jobs=None, positive_code=False):\n         self.n_components = n_components\n         self.transform_algorithm = transform_algorithm\n         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\n         self.transform_alpha = transform_alpha\n+        self.transform_max_iter = transform_max_iter\n         self.split_sign = split_sign\n         self.n_jobs = n_jobs\n         self.positive_code = positive_code\n@@ -890,8 +897,8 @@ def transform(self, X):\n         code = sparse_encode(\n             X, self.components_, algorithm=self.transform_algorithm,\n             n_nonzero_coefs=self.transform_n_nonzero_coefs,\n-            alpha=self.transform_alpha, n_jobs=self.n_jobs,\n-            positive=self.positive_code)\n+            alpha=self.transform_alpha, max_iter=self.transform_max_iter,\n+            n_jobs=self.n_jobs, positive=self.positive_code)\n \n         if self.split_sign:\n             # feature vector is split into a positive and negative side\n@@ -982,11 +989,13 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n \n     def __init__(self, dictionary, transform_algorithm='omp',\n                  transform_n_nonzero_coefs=None, transform_alpha=None,\n-                 split_sign=False, n_jobs=None, positive_code=False):\n+                 transform_max_iter=1000, split_sign=False, n_jobs=None,\n+                 positive_code=False):\n         self._set_sparse_coding_params(dictionary.shape[0],\n                                        transform_algorithm,\n                                        transform_n_nonzero_coefs,\n-                                       transform_alpha, split_sign, n_jobs,\n+                                       transform_alpha, transform_max_iter,\n+                                       split_sign, n_jobs,\n                                        positive_code)\n         self.components_ = dictionary\n \n@@ -1147,8 +1156,8 @@ def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-8,\n \n         self._set_sparse_coding_params(n_components, transform_algorithm,\n                                        transform_n_nonzero_coefs,\n-                                       transform_alpha, split_sign, n_jobs,\n-                                       positive_code)\n+                                       transform_alpha, max_iter, split_sign,\n+                                       n_jobs, positive_code)\n         self.alpha = alpha\n         self.max_iter = max_iter\n         self.tol = tol\n@@ -1331,13 +1340,13 @@ def __init__(self, n_components=None, alpha=1, n_iter=1000,\n                  fit_algorithm='lars', n_jobs=None, batch_size=3,\n                  shuffle=True, dict_init=None, transform_algorithm='omp',\n                  transform_n_nonzero_coefs=None, transform_alpha=None,\n-                 verbose=False, split_sign=False, random_state=None,\n-                 positive_code=False, positive_dict=False):\n+                 transform_max_iter=1000, verbose=False, split_sign=False,\n+                 random_state=None, positive_code=False, positive_dict=False):\n \n         self._set_sparse_coding_params(n_components, transform_algorithm,\n                                        transform_n_nonzero_coefs,\n-                                       transform_alpha, split_sign, n_jobs,\n-                                       positive_code)\n+                                       transform_alpha, transform_max_iter,\n+                                       split_sign, n_jobs, positive_code)\n         self.alpha = alpha\n         self.n_iter = n_iter\n         self.fit_algorithm = fit_algorithm\n@@ -1372,6 +1381,7 @@ def fit(self, X, y=None):\n             X, self.n_components, self.alpha,\n             n_iter=self.n_iter, return_code=False,\n             method=self.fit_algorithm,\n+            method_max_iter=self.transform_max_iter,\n             n_jobs=self.n_jobs, dict_init=self.dict_init,\n             batch_size=self.batch_size, shuffle=self.shuffle,\n             verbose=self.verbose, random_state=random_state,\n@@ -1421,6 +1431,7 @@ def partial_fit(self, X, y=None, iter_offset=None):\n         U, (A, B) = dict_learning_online(\n             X, self.n_components, self.alpha,\n             n_iter=self.n_iter, method=self.fit_algorithm,\n+            method_max_iter=self.transform_max_iter,\n             n_jobs=self.n_jobs, dict_init=dict_init,\n             batch_size=len(X), shuffle=False,\n             verbose=self.verbose, return_code=False,\n\nFrom 43bfe37f6cc48e3d7eea710749f814589a5c99b8 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Wed, 28 Nov 2018 16:37:32 +0100\nSubject: [PATCH 03/18] add a test\n\n---\n .../decomposition/tests/test_dict_learning.py | 49 +++++++++++++++++++\n 1 file changed, 49 insertions(+)\n\ndiff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py\nindex fd2937ed8f25d..8d516c67f7c1d 100644\n--- a/sklearn/decomposition/tests/test_dict_learning.py\n+++ b/sklearn/decomposition/tests/test_dict_learning.py\n@@ -57,6 +57,55 @@ def test_dict_learning_overcomplete():\n     assert dico.components_.shape == (n_components, n_features)\n \n \n+def test_max_iter():\n+    def ricker_function(resolution, center, width):\n+        \"\"\"Discrete sub-sampled Ricker (Mexican hat) wavelet\"\"\"\n+        x = np.linspace(0, resolution - 1, resolution)\n+        x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4)))\n+             * (1 - ((x - center) ** 2 / width ** 2))\n+             * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n+        return x\n+\n+    def ricker_matrix(width, resolution, n_components):\n+        \"\"\"Dictionary of Ricker (Mexican hat) wavelets\"\"\"\n+        centers = np.linspace(0, resolution - 1, n_components)\n+        D = np.empty((n_components, resolution))\n+        for i, center in enumerate(centers):\n+            D[i] = ricker_function(resolution, center, width)\n+        D /= np.sqrt(np.sum(D ** 2, axis=1))[:, np.newaxis]\n+        return D\n+\n+    transform_algorithm = 'lasso_cd'\n+    resolution = 1024\n+    subsampling = 3  # subsampling factor\n+    width = 100\n+    n_components = resolution // subsampling\n+\n+    # Compute a wavelet dictionary\n+    D_multi = np.r_[tuple(ricker_matrix(width=w, resolution=resolution,\n+                          n_components=n_components // 5)\n+                          for w in (10, 50, 100, 500, 1000))]\n+\n+    X = np.linspace(0, resolution - 1, resolution)\n+    first_quarter = X < resolution / 4\n+    X[first_quarter] = 3.\n+    X[np.logical_not(first_quarter)] = -1.\n+    X = X.reshape(1, -1)\n+\n+    # check that the underlying model fails to converge\n+    with pytest.warns(ConvergenceWarning):\n+        model = SparseCoder(D_multi, transform_algorithm=transform_algorithm,\n+                            transform_max_iter=1)\n+        model.fit_transform(X)\n+\n+    # check that the underlying model converges w/o warnings\n+    with pytest.warns(None) as record:\n+        model = SparseCoder(D_multi, transform_algorithm=transform_algorithm,\n+                            transform_max_iter=2000)\n+        model.fit_transform(X)\n+    assert not record.list\n+\n+\n # positive lars deprecated 0.22\n @pytest.mark.filterwarnings('ignore::DeprecationWarning')\n @pytest.mark.parametrize(\"transform_algorithm\", [\n\nFrom 7dce627c3f9d5e366e3fa5bf16467f2f60454313 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Wed, 28 Nov 2018 16:43:32 +0100\nSubject: [PATCH 04/18] remove extra var\n\n---\n sklearn/decomposition/tests/test_dict_learning.py | 1 -\n 1 file changed, 1 deletion(-)\n\ndiff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py\nindex 8d516c67f7c1d..06151b9067950 100644\n--- a/sklearn/decomposition/tests/test_dict_learning.py\n+++ b/sklearn/decomposition/tests/test_dict_learning.py\n@@ -78,7 +78,6 @@ def ricker_matrix(width, resolution, n_components):\n     transform_algorithm = 'lasso_cd'\n     resolution = 1024\n     subsampling = 3  # subsampling factor\n-    width = 100\n     n_components = resolution // subsampling\n \n     # Compute a wavelet dictionary\n\nFrom 5f6882a336c2d97c38a473fc6149896dba383211 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Thu, 29 Nov 2018 12:08:39 +0100\nSubject: [PATCH 05/18] pass tests\n\n---\n sklearn/decomposition/dict_learning.py | 26 +++++++++++++++++++++-----\n 1 file changed, 21 insertions(+), 5 deletions(-)\n\ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex e2afaa5535371..f8fb63f0e4843 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -672,6 +672,10 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n         Lasso solution (linear_model.Lasso). Lars will be faster if\n         the estimated components are sparse.\n \n+    method_max_iter : int, optional (default=1000)\n+        It is passed to the underlying `method` as their `max_iter`\n+        parameter.\n+\n     iter_offset : int, default 0\n         Number of previous iterations completed on the dictionary used for\n         initialization.\n@@ -956,6 +960,10 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n         the reconstruction error targeted. In this case, it overrides\n         `n_nonzero_coefs`.\n \n+    transform_max_iter : int, optional (default=1000)\n+        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n+        `transform_max_iter` is passed to the underlying transformer.\n+\n     split_sign : bool, False by default\n         Whether to split the sparse feature vector into the concatenation of\n         its negative part and its positive part. This can improve the\n@@ -1086,6 +1094,10 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n         the reconstruction error targeted. In this case, it overrides\n         `n_nonzero_coefs`.\n \n+    transform_max_iter : int, optional (default=1000)\n+        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n+        `transform_max_iter` is passed to the underlying transformer.\n+\n     n_jobs : int or None, optional (default=None)\n         Number of parallel jobs to run.\n         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n@@ -1150,14 +1162,14 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n     def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-8,\n                  fit_algorithm='lars', transform_algorithm='omp',\n                  transform_n_nonzero_coefs=None, transform_alpha=None,\n-                 n_jobs=None, code_init=None, dict_init=None, verbose=False,\n-                 split_sign=False, random_state=None,\n-                 positive_code=False, positive_dict=False):\n+                 transform_max_iter=1000, n_jobs=None, code_init=None,\n+                 dict_init=None, verbose=False, split_sign=False,\n+                 random_state=None, positive_code=False, positive_dict=False):\n \n         self._set_sparse_coding_params(n_components, transform_algorithm,\n                                        transform_n_nonzero_coefs,\n-                                       transform_alpha, max_iter, split_sign,\n-                                       n_jobs, positive_code)\n+                                       transform_alpha, transform_max_iter,\n+                                       split_sign, n_jobs, positive_code)\n         self.alpha = alpha\n         self.max_iter = max_iter\n         self.tol = tol\n@@ -1281,6 +1293,10 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):\n         the reconstruction error targeted. In this case, it overrides\n         `n_nonzero_coefs`.\n \n+    transform_max_iter : int, optional (default=1000)\n+        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n+        `transform_max_iter` is passed to the underlying transformer.\n+\n     verbose : bool, optional (default: False)\n         To control the verbosity of the procedure.\n \n\nFrom d9ba9efe70c09f556a67b358e9cffb143e28c33d Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Mon, 3 Dec 2018 17:41:28 +0100\nSubject: [PATCH 06/18] add method_max_iter to dic_learning as well\n\n---\n sklearn/decomposition/dict_learning.py | 13 +++++++++----\n 1 file changed, 9 insertions(+), 4 deletions(-)\n\ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex f8fb63f0e4843..1e5ac80830084 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -418,9 +418,9 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,\n \n \n def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n-                  method='lars', n_jobs=None, dict_init=None, code_init=None,\n-                  callback=None, verbose=False, random_state=None,\n-                  return_n_iter=False, positive_dict=False,\n+                  method='lars', method_max_iter=1000, n_jobs=None,\n+                  dict_init=None, code_init=None, callback=None, verbose=False,\n+                  random_state=None, return_n_iter=False, positive_dict=False,\n                   positive_code=False):\n     \"\"\"Solves a dictionary learning matrix factorization problem.\n \n@@ -459,6 +459,10 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n         Lasso solution (linear_model.Lasso). Lars will be faster if\n         the estimated components are sparse.\n \n+    method_max_iter : int, optional (default=1000)\n+        It is passed to the underlying `method` as their `max_iter`\n+        parameter.\n+\n     n_jobs : int or None, optional (default=None)\n         Number of parallel jobs to run.\n         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n@@ -573,7 +577,7 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n         # Update code\n         code = sparse_encode(X, dictionary, algorithm=method, alpha=alpha,\n                              init=code, n_jobs=n_jobs, positive=positive_code,\n-                             max_iter=max_iter, verbose=verbose)\n+                             max_iter=method_max_iter, verbose=verbose)\n         # Update dictionary\n         dictionary, residuals = _update_dict(dictionary.T, X.T, code.T,\n                                              verbose=verbose, return_r2=True,\n@@ -1207,6 +1211,7 @@ def fit(self, X, y=None):\n             X, n_components, self.alpha,\n             tol=self.tol, max_iter=self.max_iter,\n             method=self.fit_algorithm,\n+            method_max_iter=self.transform_max_iter,\n             n_jobs=self.n_jobs,\n             code_init=self.code_init,\n             dict_init=self.dict_init,\n\nFrom 1d2bd30a0567aedfd9532a74f101e78f96cabedc Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Mon, 3 Dec 2018 17:52:00 +0100\nSubject: [PATCH 07/18] include all changes in the whats_new\n\n---\n doc/whats_new/v0.21.rst | 13 ++++++++++---\n 1 file changed, 10 insertions(+), 3 deletions(-)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex d394964517c7b..b8bf8aa7b23e9 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -59,9 +59,16 @@ Support for Python 3.4 and below has been officially dropped.\n :mod:`slkearn.decomposition`\n ............................\n \n-- |Fix| :class:`decomposition.SparseCoder` now passes `max_iter` to the\n-  underlying :class:`linear_model.LassoLars` when algorithm is set to\n-  `lasso_lars`. :issue:`12650` by user:`Adrin Jalali <adrinjalali>`.\n+- |Fix| `sparse_encode()` now passes the `max_iter` to the underlying\n+  `LassoLarse` when `algorithm='lasso_lars'`. :issue:`12650` by user:`Adrin\n+  Jalali <adrinjalali>`.\n+- |Enhancement| `dict_learning()` and `dict_learning_online()` now accept\n+  `method_max_iter` and pass it to `sparse_encode`. :issue:`12650` by\n+  user:`Adrin Jalali <adrinjalali>`.\n+- |Enhancement| `SparseCoder`, `DictionaryLearning`, and\n+  `MiniBatchDictionaryLearning` now take a `transform_max_iter` parameter and\n+  pass it to either `dict_learning()` or `sparse_encode()`. :issue:`12650` by\n+  user:`Adrin Jalali <adrinjalali>`.\n \n :mod:`sklearn.linear_model`\n ...........................\n\nFrom b7cebac2f38b6371c0e297e008b3600e52250939 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Fri, 7 Dec 2018 11:38:49 +0100\nSubject: [PATCH 08/18] fix/improve the whats_new entries\n\n---\n doc/whats_new/v0.21.rst | 28 +++++++++++++++++++---------\n 1 file changed, 19 insertions(+), 9 deletions(-)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex b8bf8aa7b23e9..6c7fcd60f02a0 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -22,6 +22,12 @@ random sampling procedures.\n \n - Decision trees and derived ensembles when both `max_depth` and\n   `max_leaf_nodes` are set. |Fix|\n+- :func:`decomposition.sparse_encode()` with `algorithm='lasso_lars'` |Fix|\n+- :func:`decomposition.dict_learning()` and\n+  :func:`decomposition.dict_learning_online()` |Fix|\n+- :class:`decomposition.SparseCoder`,\n+  :class:`decomposition.DictionaryLearning`, and\n+  :class:`decomposition.MiniBatchDictionaryLearning` |Fix|\n - :class:`decomposition.SparseCoder` with `algorithm='lasso_lars'` |Fix|\n - :class:`linear_model.LogisticRegression` and\n   :class:`linear_model.LogisticRegressionCV` with 'saga' solver. |Fix|\n@@ -59,16 +65,20 @@ Support for Python 3.4 and below has been officially dropped.\n :mod:`slkearn.decomposition`\n ............................\n \n-- |Fix| `sparse_encode()` now passes the `max_iter` to the underlying\n-  `LassoLarse` when `algorithm='lasso_lars'`. :issue:`12650` by user:`Adrin\n-  Jalali <adrinjalali>`.\n-- |Enhancement| `dict_learning()` and `dict_learning_online()` now accept\n-  `method_max_iter` and pass it to `sparse_encode`. :issue:`12650` by\n-  user:`Adrin Jalali <adrinjalali>`.\n-- |Enhancement| `SparseCoder`, `DictionaryLearning`, and\n-  `MiniBatchDictionaryLearning` now take a `transform_max_iter` parameter and\n-  pass it to either `dict_learning()` or `sparse_encode()`. :issue:`12650` by\n+- |Fix| :func:`decomposition.sparse_encode()` now passes the `max_iter` to the\n+  underlying `LassoLarse` when `algorithm='lasso_lars'`. :issue:`12650` by\n   user:`Adrin Jalali <adrinjalali>`.\n+- |Enhancement| :func:`decomposition.dict_learning()` and\n+  :func:`decomposition.dict_learning_online()` now accept `method_max_iter` and\n+  pass it to `sparse_encode`. :issue:`12650` by user:`Adrin Jalali\n+  <adrinjalali>`.\n+- |Enhancement| :class:`decomposition.SparseCoder`,\n+  :class:`decomposition.DictionaryLearning`, and\n+  :class:`decomposition.MiniBatchDictionaryLearning` now take a\n+  `transform_max_iter` parameter and pass it to either\n+  :func:`decomposition.dict_learning()` or\n+  :func:`decomposition.sparse_encode()`. :issue:`12650` by user:`Adrin Jalali\n+  <adrinjalali>`.\n \n :mod:`sklearn.linear_model`\n ...........................\n\nFrom fcff14a010e8f5191d166b31599c4c4f4ad17d25 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Wed, 16 Jan 2019 16:27:05 +0100\nSubject: [PATCH 09/18] fix paranthesis issue\n\n---\n doc/whats_new/v0.21.rst                           | 5 +----\n examples/decomposition/plot_sparse_coding.py      | 2 +-\n sklearn/decomposition/tests/test_dict_learning.py | 2 +-\n 3 files changed, 3 insertions(+), 6 deletions(-)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex 55ea0ddb41634..1dc0792e60a0f 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -20,9 +20,6 @@ random sampling procedures.\n - :class:`linear_model.BayesianRidge` |Fix|\n - Decision trees and derived ensembles when both `max_depth` and\n   `max_leaf_nodes` are set. |Fix|\n-- :func:`decomposition.sparse_encode()` with `algorithm='lasso_lars'` |Fix|\n-- :func:`decomposition.dict_learning()` and\n-  :func:`decomposition.dict_learning_online()` |Fix|\n - :class:`decomposition.SparseCoder`,\n   :class:`decomposition.DictionaryLearning`, and\n   :class:`decomposition.MiniBatchDictionaryLearning` |Fix|\n@@ -69,7 +66,7 @@ Support for Python 3.4 and below has been officially dropped.\n ............................\n \n - |Fix| :func:`decomposition.sparse_encode()` now passes the `max_iter` to the\n-  underlying `LassoLarse` when `algorithm='lasso_lars'`. :issue:`12650` by\n+  underlying `LassoLars` when `algorithm='lasso_lars'`. :issue:`12650` by\n   user:`Adrin Jalali <adrinjalali>`.\n - |Enhancement| :func:`decomposition.dict_learning()` and\n   :func:`decomposition.dict_learning_online()` now accept `method_max_iter` and\ndiff --git a/examples/decomposition/plot_sparse_coding.py b/examples/decomposition/plot_sparse_coding.py\nindex 14cab33743730..fadec3dd8f8d4 100644\n--- a/examples/decomposition/plot_sparse_coding.py\n+++ b/examples/decomposition/plot_sparse_coding.py\n@@ -27,7 +27,7 @@\n def ricker_function(resolution, center, width):\n     \"\"\"Discrete sub-sampled Ricker (Mexican hat) wavelet\"\"\"\n     x = np.linspace(0, resolution - 1, resolution)\n-    x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4)))\n+    x = ((2 / (np.sqrt(3 * width) * np.pi ** .25))\n          * (1 - ((x - center) ** 2 / width ** 2))\n          * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n     return x\ndiff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py\nindex 3ff0ac96f8022..730a637c7028d 100644\n--- a/sklearn/decomposition/tests/test_dict_learning.py\n+++ b/sklearn/decomposition/tests/test_dict_learning.py\n@@ -61,7 +61,7 @@ def test_max_iter():\n     def ricker_function(resolution, center, width):\n         \"\"\"Discrete sub-sampled Ricker (Mexican hat) wavelet\"\"\"\n         x = np.linspace(0, resolution - 1, resolution)\n-        x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4)))\n+        x = ((2 / (np.sqrt(3 * width) * np.pi ** .25))\n              * (1 - ((x - center) ** 2 / width ** 2))\n              * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n         return x\n\nFrom e6db55b338955be46cf37f21e52efccd787dc2f2 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Wed, 16 Jan 2019 16:33:11 +0100\nSubject: [PATCH 10/18] less ()\n\n---\n examples/decomposition/plot_sparse_coding.py      | 4 ++--\n sklearn/decomposition/tests/test_dict_learning.py | 4 ++--\n 2 files changed, 4 insertions(+), 4 deletions(-)\n\ndiff --git a/examples/decomposition/plot_sparse_coding.py b/examples/decomposition/plot_sparse_coding.py\nindex fadec3dd8f8d4..2dcc3c02e1abb 100644\n--- a/examples/decomposition/plot_sparse_coding.py\n+++ b/examples/decomposition/plot_sparse_coding.py\n@@ -28,8 +28,8 @@ def ricker_function(resolution, center, width):\n     \"\"\"Discrete sub-sampled Ricker (Mexican hat) wavelet\"\"\"\n     x = np.linspace(0, resolution - 1, resolution)\n     x = ((2 / (np.sqrt(3 * width) * np.pi ** .25))\n-         * (1 - ((x - center) ** 2 / width ** 2))\n-         * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n+         * (1 - (x - center) ** 2 / width ** 2)\n+         * np.exp(-(x - center) ** 2 / (2 * width ** 2)))\n     return x\n \n \ndiff --git a/sklearn/decomposition/tests/test_dict_learning.py b/sklearn/decomposition/tests/test_dict_learning.py\nindex 730a637c7028d..e97132511e9ef 100644\n--- a/sklearn/decomposition/tests/test_dict_learning.py\n+++ b/sklearn/decomposition/tests/test_dict_learning.py\n@@ -62,8 +62,8 @@ def ricker_function(resolution, center, width):\n         \"\"\"Discrete sub-sampled Ricker (Mexican hat) wavelet\"\"\"\n         x = np.linspace(0, resolution - 1, resolution)\n         x = ((2 / (np.sqrt(3 * width) * np.pi ** .25))\n-             * (1 - ((x - center) ** 2 / width ** 2))\n-             * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n+             * (1 - (x - center) ** 2 / width ** 2)\n+             * np.exp(-(x - center) ** 2 / (2 * width ** 2)))\n         return x\n \n     def ricker_matrix(width, resolution, n_components):\n\nFrom de72daaacb7a245971382d75aac8c60139091d12 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Fri, 1 Mar 2019 14:44:25 +0100\nSubject: [PATCH 11/18] add me in _contributors.rst\n\n---\n doc/whats_new/_contributors.rst | 2 ++\n doc/whats_new/v0.21.rst         | 8 +++-----\n 2 files changed, 5 insertions(+), 5 deletions(-)\n\ndiff --git a/doc/whats_new/_contributors.rst b/doc/whats_new/_contributors.rst\nindex 270df1fb837fc..1a7f04b691565 100644\n--- a/doc/whats_new/_contributors.rst\n+++ b/doc/whats_new/_contributors.rst\n@@ -169,3 +169,5 @@\n .. _Roman Yurchak: https://github.com/rth\n \n .. _Hanmin Qin: https://github.com/qinhanmin2014\n+\n+.. _Adrin Jalali: https://github.com/adrinjalali\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex b22614b4a4043..5ee61df4dcd1e 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -99,18 +99,16 @@ Support for Python 3.4 and below has been officially dropped.\n \n - |Fix| :func:`decomposition.sparse_encode()` now passes the `max_iter` to the\n   underlying `LassoLars` when `algorithm='lasso_lars'`. :issue:`12650` by\n-  user:`Adrin Jalali <adrinjalali>`.\n+  `Adrin Jalali`_.\n - |Enhancement| :func:`decomposition.dict_learning()` and\n   :func:`decomposition.dict_learning_online()` now accept `method_max_iter` and\n-  pass it to `sparse_encode`. :issue:`12650` by user:`Adrin Jalali\n-  <adrinjalali>`.\n+  pass it to `sparse_encode`. :issue:`12650` by `Adrin Jalali`_.\n - |Enhancement| :class:`decomposition.SparseCoder`,\n   :class:`decomposition.DictionaryLearning`, and\n   :class:`decomposition.MiniBatchDictionaryLearning` now take a\n   `transform_max_iter` parameter and pass it to either\n   :func:`decomposition.dict_learning()` or\n-  :func:`decomposition.sparse_encode()`. :issue:`12650` by user:`Adrin Jalali\n-  <adrinjalali>`.\n+  :func:`decomposition.sparse_encode()`. :issue:`12650` by `Adrin Jalali`_.\n   \n :mod:`sklearn.discriminant_analysis`\n ....................................\n\nFrom 0dd6a2c007fe462913fcd22ff14d675fca87f5e2 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Wed, 6 Mar 2019 13:38:39 +0100\nSubject: [PATCH 12/18] add parameter at the end\n\n---\n sklearn/decomposition/dict_learning.py | 106 ++++++++++++++-----------\n 1 file changed, 58 insertions(+), 48 deletions(-)\n\ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 6ea4325ba8d66..77756b4ebff2b 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -416,10 +416,10 @@ def _update_dict(dictionary, Y, code, verbose=False, return_r2=False,\n \n \n def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n-                  method='lars', method_max_iter=1000, n_jobs=None,\n-                  dict_init=None, code_init=None, callback=None, verbose=False,\n-                  random_state=None, return_n_iter=False, positive_dict=False,\n-                  positive_code=False):\n+                  method='lars', n_jobs=None, dict_init=None, code_init=None,\n+                  callback=None, verbose=False, random_state=None,\n+                  return_n_iter=False, positive_dict=False,\n+                  positive_code=False, method_max_iter=1000):\n     \"\"\"Solves a dictionary learning matrix factorization problem.\n \n     Finds the best dictionary and the corresponding sparse code for\n@@ -457,10 +457,6 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n         Lasso solution (linear_model.Lasso). Lars will be faster if\n         the estimated components are sparse.\n \n-    method_max_iter : int, optional (default=1000)\n-        It is passed to the underlying `method` as their `max_iter`\n-        parameter.\n-\n     n_jobs : int or None, optional (default=None)\n         Number of parallel jobs to run.\n         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n@@ -498,6 +494,12 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n \n         .. versionadded:: 0.20\n \n+    method_max_iter : int, optional (default=1000)\n+        It is passed to the underlying ``method`` as their ``max_iter``\n+        parameter.\n+\n+        .. versionadded:: 0.21\n+\n     Returns\n     -------\n     code : array of shape (n_samples, n_components)\n@@ -609,11 +611,11 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n                          return_code=True, dict_init=None, callback=None,\n                          batch_size=3, verbose=False, shuffle=True,\n-                         n_jobs=None, method='lars', method_max_iter=1000,\n-                         iter_offset=0, random_state=None,\n-                         return_inner_stats=False, inner_stats=None,\n-                         return_n_iter=False, positive_dict=False,\n-                         positive_code=False):\n+                         n_jobs=None, method='lars', iter_offset=0,\n+                         random_state=None, return_inner_stats=False,\n+                         inner_stats=None, return_n_iter=False,\n+                         positive_dict=False, positive_code=False,\n+                         method_max_iter=1000):\n     \"\"\"Solves a dictionary learning matrix factorization problem online.\n \n     Finds the best dictionary and the corresponding sparse code for\n@@ -674,10 +676,6 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n         Lasso solution (linear_model.Lasso). Lars will be faster if\n         the estimated components are sparse.\n \n-    method_max_iter : int, optional (default=1000)\n-        It is passed to the underlying `method` as their `max_iter`\n-        parameter.\n-\n     iter_offset : int, default 0\n         Number of previous iterations completed on the dictionary used for\n         initialization.\n@@ -714,6 +712,12 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n \n         .. versionadded:: 0.20\n \n+    method_max_iter : int, optional (default=1000)\n+        It is passed to the underlying ``method`` as their ``max_iter``\n+        parameter.\n+\n+        .. versionadded:: 0.21\n+\n     Returns\n     -------\n     code : array of shape (n_samples, n_components),\n@@ -866,9 +870,9 @@ class SparseCodingMixin(TransformerMixin):\n     def _set_sparse_coding_params(self, n_components,\n                                   transform_algorithm='omp',\n                                   transform_n_nonzero_coefs=None,\n-                                  transform_alpha=None,\n-                                  transform_max_iter=1000, split_sign=False,\n-                                  n_jobs=None, positive_code=False):\n+                                  transform_alpha=None, split_sign=False,\n+                                  n_jobs=None, positive_code=False,\n+                                  transform_max_iter=1000):\n         self.n_components = n_components\n         self.transform_algorithm = transform_algorithm\n         self.transform_n_nonzero_coefs = transform_n_nonzero_coefs\n@@ -962,10 +966,6 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n         the reconstruction error targeted. In this case, it overrides\n         `n_nonzero_coefs`.\n \n-    transform_max_iter : int, optional (default=1000)\n-        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n-        `transform_max_iter` is passed to the underlying transformer.\n-\n     split_sign : bool, False by default\n         Whether to split the sparse feature vector into the concatenation of\n         its negative part and its positive part. This can improve the\n@@ -982,6 +982,12 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n \n         .. versionadded:: 0.20\n \n+    transform_max_iter : int, optional (default=1000)\n+        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n+        `transform_max_iter` is passed to the underlying transformer.\n+\n+        .. versionadded:: 0.21\n+\n     Attributes\n     ----------\n     components_ : array, [n_components, n_features]\n@@ -999,14 +1005,13 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n \n     def __init__(self, dictionary, transform_algorithm='omp',\n                  transform_n_nonzero_coefs=None, transform_alpha=None,\n-                 transform_max_iter=1000, split_sign=False, n_jobs=None,\n-                 positive_code=False):\n+                 split_sign=False, n_jobs=None, positive_code=False,\n+                 transform_max_iter=1000):\n         self._set_sparse_coding_params(dictionary.shape[0],\n                                        transform_algorithm,\n                                        transform_n_nonzero_coefs,\n-                                       transform_alpha, transform_max_iter,\n-                                       split_sign, n_jobs,\n-                                       positive_code)\n+                                       transform_alpha, split_sign, n_jobs,\n+                                       positive_code, transform_max_iter)\n         self.components_ = dictionary\n \n     def fit(self, X, y=None):\n@@ -1096,10 +1101,6 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n         the reconstruction error targeted. In this case, it overrides\n         `n_nonzero_coefs`.\n \n-    transform_max_iter : int, optional (default=1000)\n-        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n-        `transform_max_iter` is passed to the underlying transformer.\n-\n     n_jobs : int or None, optional (default=None)\n         Number of parallel jobs to run.\n         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n@@ -1136,6 +1137,12 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n \n         .. versionadded:: 0.20\n \n+    transform_max_iter : int, optional (default=1000)\n+        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n+        `transform_max_iter` is passed to the underlying transformer.\n+\n+        .. versionadded:: 0.21\n+\n     Attributes\n     ----------\n     components_ : array, [n_components, n_features]\n@@ -1164,14 +1171,14 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n     def __init__(self, n_components=None, alpha=1, max_iter=1000, tol=1e-8,\n                  fit_algorithm='lars', transform_algorithm='omp',\n                  transform_n_nonzero_coefs=None, transform_alpha=None,\n-                 transform_max_iter=1000, n_jobs=None, code_init=None,\n-                 dict_init=None, verbose=False, split_sign=False,\n-                 random_state=None, positive_code=False, positive_dict=False):\n+                 n_jobs=None, code_init=None, dict_init=None, verbose=False,\n+                 split_sign=False, random_state=None, positive_code=False,\n+                 positive_dict=False, transform_max_iter=1000):\n \n         self._set_sparse_coding_params(n_components, transform_algorithm,\n                                        transform_n_nonzero_coefs,\n-                                       transform_alpha, transform_max_iter,\n-                                       split_sign, n_jobs, positive_code)\n+                                       transform_alpha, split_sign, n_jobs,\n+                                       positive_code, transform_max_iter)\n         self.alpha = alpha\n         self.max_iter = max_iter\n         self.tol = tol\n@@ -1296,10 +1303,6 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):\n         the reconstruction error targeted. In this case, it overrides\n         `n_nonzero_coefs`.\n \n-    transform_max_iter : int, optional (default=1000)\n-        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n-        `transform_max_iter` is passed to the underlying transformer.\n-\n     verbose : bool, optional (default: False)\n         To control the verbosity of the procedure.\n \n@@ -1324,6 +1327,12 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):\n \n         .. versionadded:: 0.20\n \n+    transform_max_iter : int, optional (default=1000)\n+        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n+        `transform_max_iter` is passed to the underlying transformer.\n+\n+        .. versionadded:: 0.21\n+\n     Attributes\n     ----------\n     components_ : array, [n_components, n_features]\n@@ -1356,16 +1365,17 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):\n \n     \"\"\"\n     def __init__(self, n_components=None, alpha=1, n_iter=1000,\n-                 fit_algorithm='lars', n_jobs=None, batch_size=3,\n-                 shuffle=True, dict_init=None, transform_algorithm='omp',\n+                 fit_algorithm='lars', n_jobs=None, batch_size=3, shuffle=True,\n+                 dict_init=None, transform_algorithm='omp',\n                  transform_n_nonzero_coefs=None, transform_alpha=None,\n-                 transform_max_iter=1000, verbose=False, split_sign=False,\n-                 random_state=None, positive_code=False, positive_dict=False):\n+                 verbose=False, split_sign=False, random_state=None,\n+                 positive_code=False, positive_dict=False,\n+                 transform_max_iter=1000):\n \n         self._set_sparse_coding_params(n_components, transform_algorithm,\n                                        transform_n_nonzero_coefs,\n-                                       transform_alpha, transform_max_iter,\n-                                       split_sign, n_jobs, positive_code)\n+                                       transform_alpha, split_sign, n_jobs,\n+                                       positive_code, transform_max_iter)\n         self.alpha = alpha\n         self.n_iter = n_iter\n         self.fit_algorithm = fit_algorithm\n\nFrom 2e722b78add12a6af24bf9fe75707284def8e8e5 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Thu, 25 Apr 2019 10:49:54 +0200\nSubject: [PATCH 13/18] update docstring\n\n---\n sklearn/decomposition/dict_learning.py | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 77756b4ebff2b..8de4676d1b792 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -983,8 +983,8 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n         .. versionadded:: 0.20\n \n     transform_max_iter : int, optional (default=1000)\n-        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n-        `transform_max_iter` is passed to the underlying transformer.\n+        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n+        `lasso_cd`.\n \n         .. versionadded:: 0.21\n \n\nFrom 4f28dbd0c426f751aebcf116b6fe60d28721f68b Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Thu, 25 Apr 2019 12:57:54 +0200\nSubject: [PATCH 14/18] empty commit\n\n\nFrom 3829ef44bdc3192f529e9e74cb94458e47e21607 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Fri, 26 Apr 2019 11:57:37 +0200\nSubject: [PATCH 15/18] unifying the docstrings for max_iter\n\n---\n sklearn/decomposition/dict_learning.py | 22 +++++++++++-----------\n 1 file changed, 11 insertions(+), 11 deletions(-)\n\ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 8de4676d1b792..600be0028834d 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -65,7 +65,8 @@ def _sparse_encode(X, dictionary, gram, cov=None, algorithm='lasso_lars',\n         `algorithm='lasso_cd'`.\n \n     max_iter : int, 1000 by default\n-        Maximum number of iterations to perform if `algorithm='lasso_cd'`.\n+        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n+        `lasso_lars`.\n \n     copy_cov : boolean, optional\n         Whether to copy the precomputed covariance matrix; if False, it may be\n@@ -241,7 +242,8 @@ def sparse_encode(X, dictionary, gram=None, cov=None, algorithm='lasso_lars',\n         `algorithm='lasso_cd'`.\n \n     max_iter : int, 1000 by default\n-        Maximum number of iterations to perform if `algorithm='lasso_cd'`.\n+        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n+        `lasso_lars`.\n \n     n_jobs : int or None, optional (default=None)\n         Number of parallel jobs to run.\n@@ -495,8 +497,7 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n         .. versionadded:: 0.20\n \n     method_max_iter : int, optional (default=1000)\n-        It is passed to the underlying ``method`` as their ``max_iter``\n-        parameter.\n+        Maximum number of iterations to perform.\n \n         .. versionadded:: 0.21\n \n@@ -713,8 +714,7 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n         .. versionadded:: 0.20\n \n     method_max_iter : int, optional (default=1000)\n-        It is passed to the underlying ``method`` as their ``max_iter``\n-        parameter.\n+        Maximum number of iterations to perform in each ``sparse_encode`` step.\n \n         .. versionadded:: 0.21\n \n@@ -984,7 +984,7 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n \n     transform_max_iter : int, optional (default=1000)\n         Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n-        `lasso_cd`.\n+        `lasso_lars`.\n \n         .. versionadded:: 0.21\n \n@@ -1138,8 +1138,8 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n         .. versionadded:: 0.20\n \n     transform_max_iter : int, optional (default=1000)\n-        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n-        `transform_max_iter` is passed to the underlying transformer.\n+        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n+        `lasso_lars`.\n \n         .. versionadded:: 0.21\n \n@@ -1328,8 +1328,8 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):\n         .. versionadded:: 0.20\n \n     transform_max_iter : int, optional (default=1000)\n-        If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`,\n-        `transform_max_iter` is passed to the underlying transformer.\n+        Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n+        `lasso_lars`.\n \n         .. versionadded:: 0.21\n \n\nFrom e4a4f2d17d2576fd68d004090a0770c4698d4f03 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Thu, 2 May 2019 18:01:21 +0200\nSubject: [PATCH 16/18] apply Thomas's comment\n\n---\n sklearn/decomposition/dict_learning.py | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex 600be0028834d..830e86d188b9c 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -644,7 +644,7 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n         Sparsity controlling parameter.\n \n     n_iter : int,\n-        Number of iterations to perform.\n+        Number of mini-batch iterations to perform.\n \n     return_code : boolean,\n         Whether to also return the code U or just the dictionary V.\n@@ -714,7 +714,7 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n         .. versionadded:: 0.20\n \n     method_max_iter : int, optional (default=1000)\n-        Maximum number of iterations to perform in each ``sparse_encode`` step.\n+        Maximum number of iterations to perform when solving the lasso problem.\n \n         .. versionadded:: 0.21\n \n\nFrom 0c49d1845e450400442d81f702be6cf9a70693ba Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Thu, 2 May 2019 18:30:38 +0200\nSubject: [PATCH 17/18] move to 0.22\n\n---\n doc/whats_new/v0.21.rst                | 17 -----------------\n doc/whats_new/v0.22.rst                | 25 +++++++++++++++++++++++--\n sklearn/decomposition/dict_learning.py | 10 +++++-----\n 3 files changed, 28 insertions(+), 24 deletions(-)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex 89e9ac08d6780..bf18d8350646e 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -24,10 +24,6 @@ random sampling procedures.\n - :class:`linear_model.BayesianRidge` |Fix|\n - Decision trees and derived ensembles when both `max_depth` and\n   `max_leaf_nodes` are set. |Fix|\n-- :class:`decomposition.SparseCoder`,\n-  :class:`decomposition.DictionaryLearning`, and\n-  :class:`decomposition.MiniBatchDictionaryLearning` |Fix|\n-- :class:`decomposition.SparseCoder` with `algorithm='lasso_lars'` |Fix|\n - :class:`linear_model.LogisticRegression` and\n   :class:`linear_model.LogisticRegressionCV` with 'saga' solver. |Fix|\n - :class:`ensemble.GradientBoostingClassifier` |Fix|\n@@ -160,19 +156,6 @@ Support for Python 3.4 and below has been officially dropped.\n   the default value is used.\n   :pr:`12988` by :user:`Zijie (ZJ) Poh <zjpoh>`.\n \n-- |Fix| :func:`decomposition.sparse_encode()` now passes the `max_iter` to the\n-  underlying `LassoLars` when `algorithm='lasso_lars'`. :issue:`12650` by\n-  `Adrin Jalali`_.\n-- |Enhancement| :func:`decomposition.dict_learning()` and\n-  :func:`decomposition.dict_learning_online()` now accept `method_max_iter` and\n-  pass it to `sparse_encode`. :issue:`12650` by `Adrin Jalali`_.\n-- |Enhancement| :class:`decomposition.SparseCoder`,\n-  :class:`decomposition.DictionaryLearning`, and\n-  :class:`decomposition.MiniBatchDictionaryLearning` now take a\n-  `transform_max_iter` parameter and pass it to either\n-  :func:`decomposition.dict_learning()` or\n-  :func:`decomposition.sparse_encode()`. :issue:`12650` by `Adrin Jalali`_.\n-  \n :mod:`sklearn.discriminant_analysis`\n ....................................\n \ndiff --git a/doc/whats_new/v0.22.rst b/doc/whats_new/v0.22.rst\nindex ba94680c4efc3..c04d59c3b63b5 100644\n--- a/doc/whats_new/v0.22.rst\n+++ b/doc/whats_new/v0.22.rst\n@@ -17,8 +17,11 @@ parameters, may produce different models from the previous version. This often\n occurs due to changes in the modelling logic (bug fixes or enhancements), or in\n random sampling procedures.\n \n-..\n-    TO FILL IN AS WE GO\n+- :class:`decomposition.SparseCoder`,\n+  :class:`decomposition.DictionaryLearning`, and\n+  :class:`decomposition.MiniBatchDictionaryLearning` |Fix|\n+- :class:`decomposition.SparseCoder` with `algorithm='lasso_lars'` |Fix|\n+\n \n Details are listed in the changelog below.\n \n@@ -39,6 +42,24 @@ Changelog\n     :pr:`123456` by :user:`Joe Bloggs <joeongithub>`.\n     where 123456 is the *pull request* number, not the issue number.\n \n+:mod:`sklearn.decomposition`\n+............................\n+\n+- |Fix| :func:`decomposition.sparse_encode()` now passes the `max_iter` to the\n+  underlying `LassoLars` when `algorithm='lasso_lars'`. :issue:`12650` by\n+  `Adrin Jalali`_.\n+\n+- |Enhancement| :func:`decomposition.dict_learning()` and\n+  :func:`decomposition.dict_learning_online()` now accept `method_max_iter` and\n+  pass it to `sparse_encode`. :issue:`12650` by `Adrin Jalali`_.\n+\n+- |Enhancement| :class:`decomposition.SparseCoder`,\n+  :class:`decomposition.DictionaryLearning`, and\n+  :class:`decomposition.MiniBatchDictionaryLearning` now take a\n+  `transform_max_iter` parameter and pass it to either\n+  :func:`decomposition.dict_learning()` or\n+  :func:`decomposition.sparse_encode()`. :issue:`12650` by `Adrin Jalali`_.\n+\n Changes to estimator checks\n ---------------------------\n \ndiff --git a/sklearn/decomposition/dict_learning.py b/sklearn/decomposition/dict_learning.py\nindex a5ea12838838a..ab9722f43821d 100644\n--- a/sklearn/decomposition/dict_learning.py\n+++ b/sklearn/decomposition/dict_learning.py\n@@ -499,7 +499,7 @@ def dict_learning(X, n_components, alpha, max_iter=100, tol=1e-8,\n     method_max_iter : int, optional (default=1000)\n         Maximum number of iterations to perform.\n \n-        .. versionadded:: 0.21\n+        .. versionadded:: 0.22\n \n     Returns\n     -------\n@@ -716,7 +716,7 @@ def dict_learning_online(X, n_components=2, alpha=1, n_iter=100,\n     method_max_iter : int, optional (default=1000)\n         Maximum number of iterations to perform when solving the lasso problem.\n \n-        .. versionadded:: 0.21\n+        .. versionadded:: 0.22\n \n     Returns\n     -------\n@@ -986,7 +986,7 @@ class SparseCoder(BaseEstimator, SparseCodingMixin):\n         Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n         `lasso_lars`.\n \n-        .. versionadded:: 0.21\n+        .. versionadded:: 0.22\n \n     Attributes\n     ----------\n@@ -1141,7 +1141,7 @@ class DictionaryLearning(BaseEstimator, SparseCodingMixin):\n         Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n         `lasso_lars`.\n \n-        .. versionadded:: 0.21\n+        .. versionadded:: 0.22\n \n     Attributes\n     ----------\n@@ -1331,7 +1331,7 @@ class MiniBatchDictionaryLearning(BaseEstimator, SparseCodingMixin):\n         Maximum number of iterations to perform if `algorithm='lasso_cd'` or\n         `lasso_lars`.\n \n-        .. versionadded:: 0.21\n+        .. versionadded:: 0.22\n \n     Attributes\n     ----------\n\nFrom 2bb0ea8a60848afbbaaf8207ff3464f8e294f114 Mon Sep 17 00:00:00 2001\nFrom: adrinjalali <adrin.jalali@gmail.com>\nDate: Thu, 27 Jun 2019 11:18:25 +0200\nSubject: [PATCH 18/18] remove extra line\n\n---\n doc/whats_new/v0.22.rst | 1 -\n 1 file changed, 1 deletion(-)\n\ndiff --git a/doc/whats_new/v0.22.rst b/doc/whats_new/v0.22.rst\nindex 9841500d526a1..390ad6b40cfc6 100644\n--- a/doc/whats_new/v0.22.rst\n+++ b/doc/whats_new/v0.22.rst\n@@ -151,7 +151,6 @@ Miscellaneous\n - |Fix| Port `lobpcg` from SciPy which implement some bug fixes but only\n   available in 1.3+.\n   :pr:`14195` by :user:`Guillaume Lemaitre <glemaitre>`.\n->>>>>>> upstream/master\n \n Changes to estimator checks\n ---------------------------\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/12682"}