{"instance_id": "scikit-learn__scikit-learn-13496", "score": "C", "diff": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..c665370ab 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -120,6 +120,10 @@ class IsolationForest(BaseBagging, OutlierMixin):\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n \n     Attributes\n     ----------\n@@ -173,7 +177,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +192,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\ndiff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py\nindex 67ba2d7f9..6c1d27b2d 100644\n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -345,6 +345,52 @@ def test_iforest_chunks_works1(\n     assert mocked_get_chunk.call_count == n_predict_calls\n \n \n+@pytest.mark.filterwarnings('ignore:default contamination')\n+@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')\n+def test_iforest_warm_start():\n+    \"\"\"Test IsolationForest warm_start implementation.\"\"\"\n+    X = iris.data\n+    # Use same random state to ensure reproducibility\n+    rng = check_random_state(42)\n+    \n+    # Fit a forest with warm_start=True, then increase n_estimators\n+    clf_ws = IsolationForest(n_estimators=50, random_state=rng,\n+                            warm_start=True)  \n+    clf_ws.fit(X)\n+    assert len(clf_ws.estimators_) == 50\n+    \n+    # Store decision function for first 50 trees\n+    decision_50 = clf_ws.decision_function(X)\n+    \n+    # Add 50 more estimators  \n+    clf_ws.set_params(n_estimators=100)\n+    clf_ws.fit(X)\n+    assert len(clf_ws.estimators_) == 100\n+    \n+    # Store decision function for 100 trees\n+    decision_100 = clf_ws.decision_function(X)\n+    \n+    # Decision function should have changed after adding trees\n+    assert not np.allclose(decision_50, decision_100)\n+    \n+    # Fit a new forest directly with 100 estimators\n+    clf_no_ws = IsolationForest(n_estimators=100, random_state=rng,\n+                                warm_start=False)\n+    clf_no_ws.fit(X)\n+    \n+    # Check that both have the same number of estimators\n+    assert len(clf_no_ws.estimators_) == len(clf_ws.estimators_)\n+    \n+    # Test that reducing n_estimators raises an error  \n+    clf_ws.set_params(n_estimators=10)\n+    assert_raises(ValueError, clf_ws.fit, X)\n+    \n+    # Test that warm start with the same n_estimators warns\n+    clf_ws.set_params(n_estimators=100)\n+    with pytest.warns(UserWarning, match=\"Warm-start fitting without\"):\n+        clf_ws.fit(X)\n+\n+\n # idem with chunk_size = 5 rows\n @patch(\n     \"sklearn.ensemble.iforest.get_chunk_n_rows\",\n", "gold_standard_diff": "From 528cba3e6a337bd83145d8b9d90457d02fabb00e Mon Sep 17 00:00:00 2001\nFrom: petibear <40757147+petibear@users.noreply.github.com>\nDate: Sat, 23 Mar 2019 10:20:06 +0100\nSubject: [PATCH 1/8] ENH iForest - expose warm_start (#13451)\n\n---\n sklearn/ensemble/iforest.py            |  8 +++++++-\n sklearn/ensemble/tests/test_iforest.py | 28 ++++++++++++++++++++++++++\n 2 files changed, 35 insertions(+), 1 deletion(-)\n\ndiff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd36259e48..589171f73eb4d 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -120,6 +120,10 @@ class IsolationForest(BaseBagging, OutlierMixin):\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n \n     Attributes\n     ----------\n@@ -173,7 +177,8 @@ def __init__(self,\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -185,6 +190,7 @@ def __init__(self,\n             n_estimators=n_estimators,\n             max_samples=max_samples,\n             max_features=max_features,\n+            warm_start=warm_start,\n             n_jobs=n_jobs,\n             random_state=random_state,\n             verbose=verbose)\ndiff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py\nindex 67ba2d7f933e3..10a26fa086e74 100644\n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -295,6 +295,34 @@ def test_score_samples():\n                        clf2.score_samples([[2., 2.]]))\n \n \n+@pytest.mark.filterwarnings('ignore:default contamination')\n+@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')\n+def test_iforest_warmstart():\n+    \"\"\"Test iterative addition of iTrees to an iForest \"\"\"\n+\n+    rng = check_random_state(0)\n+\n+    # Generate regular observations\n+    X = 0.3 * rng.randn(90, 2)\n+    X_reg = np.r_[X + 4, X - 4]\n+    # Generate some abnormal observations\n+    X_outliers = rng.uniform(low=-2, high=2, size=(20, 2))\n+    X_train = np.r_[X_reg, X_outliers]\n+\n+    # fit first 10 trees\n+    clf = IsolationForest(n_estimators=10, max_samples=20,\n+                          random_state=rng, warm_start=True)\n+    clf.fit(X_train)\n+    # keep the 1st tree\n+    tree_1 = clf.estimators_[0]\n+    # fit another 10 trees\n+    clf.n_estimators += 10\n+    clf.fit(X_train)\n+    # expecting 20 fitted trees and no overwritten trees\n+    assert len(clf.estimators_) == 20\n+    assert clf.estimators_[0] is tree_1\n+\n+\n @pytest.mark.filterwarnings('ignore:default contamination')\n @pytest.mark.filterwarnings('ignore:behaviour=\"old\"')\n def test_deprecation():\n\nFrom c32ca5634fbd1e933d31aa26edc9d8082da68d78 Mon Sep 17 00:00:00 2001\nFrom: petibear <40757147+petibear@users.noreply.github.com>\nDate: Sat, 23 Mar 2019 19:45:56 +0100\nSubject: [PATCH 2/8] Incorporates comments from PR #13496\n\n* versionadded=0.21\n\n* adition in whatsnew\n\n* test using iris dataset\n---\n doc/whats_new/v0.21.rst                |  4 ++++\n sklearn/ensemble/iforest.py            |  2 ++\n sklearn/ensemble/tests/test_iforest.py | 16 +++++-----------\n 3 files changed, 11 insertions(+), 11 deletions(-)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex 1c68efd38570c..7902a99ffc112 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -179,6 +179,10 @@ Support for Python 3.4 and below has been officially dropped.\n   prediction step, thus capping the memory usage. :issue:`13283` by\n   `Nicolas Goix`_.\n \n+- |Enhancement| :class:`ensemble.IsolationForest` now exposes ``warm_start``\n+  parameter, allowing iterative addition of trees to an isolation \n+  forest. :issue:`13451` by :user:`Peter Marko <petibear>`.\n+\n - |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` where\n   the gradients would be incorrectly computed in multiclass classification\n   problems. :issue:`12715` by :user:`Nicolas Hug<NicolasHug>`.\ndiff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 589171f73eb4d..eac3519c7d9e2 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -125,6 +125,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n         and add more estimators to the ensemble, otherwise, just fit a whole\n         new forest. See :term:`the Glossary <warm_start>`.\n \n+        .. versionadded:: 0.21\n+\n     Attributes\n     ----------\n     estimators_ : list of DecisionTreeClassifier\ndiff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py\nindex 10a26fa086e74..4d16d19427e30 100644\n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -301,23 +301,17 @@ def test_iforest_warmstart():\n     \"\"\"Test iterative addition of iTrees to an iForest \"\"\"\n \n     rng = check_random_state(0)\n-\n-    # Generate regular observations\n-    X = 0.3 * rng.randn(90, 2)\n-    X_reg = np.r_[X + 4, X - 4]\n-    # Generate some abnormal observations\n-    X_outliers = rng.uniform(low=-2, high=2, size=(20, 2))\n-    X_train = np.r_[X_reg, X_outliers]\n+    X = iris.data\n \n     # fit first 10 trees\n     clf = IsolationForest(n_estimators=10, max_samples=20,\n                           random_state=rng, warm_start=True)\n-    clf.fit(X_train)\n-    # keep the 1st tree\n+    clf.fit(X)\n+    # remember the 1st tree\n     tree_1 = clf.estimators_[0]\n     # fit another 10 trees\n-    clf.n_estimators += 10\n-    clf.fit(X_train)\n+    clf.set_params(n_estimators=20)\n+    clf.fit(X)\n     # expecting 20 fitted trees and no overwritten trees\n     assert len(clf.estimators_) == 20\n     assert clf.estimators_[0] is tree_1\n\nFrom 772ba9c0c14f3011b6b4586947a0cbe0079d9d1c Mon Sep 17 00:00:00 2001\nFrom: Albert Thomas <albertthomas88@gmail.com>\nDate: Sat, 23 Mar 2019 20:35:14 +0100\nSubject: [PATCH 3/8] Update sklearn/ensemble/tests/test_iforest.py\n\nsmaller dataset for testing\n\nCo-Authored-By: petibear <40757147+petibear@users.noreply.github.com>\n---\n sklearn/ensemble/tests/test_iforest.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py\nindex 4d16d19427e30..90c9087637dd0 100644\n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -301,7 +301,7 @@ def test_iforest_warmstart():\n     \"\"\"Test iterative addition of iTrees to an iForest \"\"\"\n \n     rng = check_random_state(0)\n-    X = iris.data\n+    X = rng.randn(20, 2)\n \n     # fit first 10 trees\n     clf = IsolationForest(n_estimators=10, max_samples=20,\n\nFrom 032ba52f46bcbdf1cd59f48bef4915e18eda113f Mon Sep 17 00:00:00 2001\nFrom: petibear <40757147+petibear@users.noreply.github.com>\nDate: Sun, 24 Mar 2019 16:53:43 +0100\nSubject: [PATCH 4/8] Trigger CI\n\n\nFrom 30fe6a74b31c1c96e169eecc10b241fe0c4ab2fe Mon Sep 17 00:00:00 2001\nFrom: petibear <40757147+petibear@users.noreply.github.com>\nDate: Sun, 24 Mar 2019 20:07:18 +0100\nSubject: [PATCH 5/8] Corrected the PR reference\n\n---\n doc/whats_new/v0.21.rst | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex 7902a99ffc112..d7e0aaffee226 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -181,7 +181,7 @@ Support for Python 3.4 and below has been officially dropped.\n \n - |Enhancement| :class:`ensemble.IsolationForest` now exposes ``warm_start``\n   parameter, allowing iterative addition of trees to an isolation \n-  forest. :issue:`13451` by :user:`Peter Marko <petibear>`.\n+  forest. :issue:`13496` by :user:`Peter Marko <petibear>`.\n \n - |Fix| Fixed a bug in :class:`ensemble.GradientBoostingClassifier` where\n   the gradients would be incorrectly computed in multiclass classification\n\nFrom 10c042a68c7c2e313185033c8df8f4c2f9a7a51a Mon Sep 17 00:00:00 2001\nFrom: petibear <40757147+petibear@users.noreply.github.com>\nDate: Mon, 25 Mar 2019 21:47:02 +0100\nSubject: [PATCH 6/8] doc entry on warm_start + renamed the test\n\n---\n doc/modules/outlier_detection.rst      | 14 ++++++++++++++\n sklearn/ensemble/tests/test_iforest.py |  2 +-\n 2 files changed, 15 insertions(+), 1 deletion(-)\n\ndiff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst\nindex b27b0c8a59643..037e89a12f57f 100644\n--- a/doc/modules/outlier_detection.rst\n+++ b/doc/modules/outlier_detection.rst\n@@ -269,6 +269,20 @@ This algorithm is illustrated below.\n     * Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n       Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n \n+.. _iforest_warm_start:\n+\n+Fitting additional trees\n+~~~~~~~~~~~~~~~~~~~~~~~~\n+The :class:`ensemble.IsolationForest` supports ``warm_start=True`` which\n+allows you to add more trees to an already fitted model.\n+\n+::\n+\n+  >>> clf = IsolationForest(n_estimators=100, warm_start=True)\n+  >>> clf.fit(X)\n+  >>> clf.set_params(n_estimators=200)  # set warm_start and new nr of trees\n+  >>> clf.fit(X) # fit additional 100 trees\n+\n \n Local Outlier Factor\n --------------------\ndiff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py\nindex 90c9087637dd0..9ab679ef66e05 100644\n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -297,7 +297,7 @@ def test_score_samples():\n \n @pytest.mark.filterwarnings('ignore:default contamination')\n @pytest.mark.filterwarnings('ignore:behaviour=\"old\"')\n-def test_iforest_warmstart():\n+def test_iforest_warm_start():\n     \"\"\"Test iterative addition of iTrees to an iForest \"\"\"\n \n     rng = check_random_state(0)\n\nFrom 5ac735b09023ab1ae188d95135c36256bc8ba91d Mon Sep 17 00:00:00 2001\nFrom: petibear <40757147+petibear@users.noreply.github.com>\nDate: Mon, 25 Mar 2019 23:56:58 +0100\nSubject: [PATCH 7/8] Corrections in the doc example\n\n---\n doc/modules/outlier_detection.rst | 29 +++++++++++++++--------------\n 1 file changed, 15 insertions(+), 14 deletions(-)\n\ndiff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst\nindex 037e89a12f57f..a4ebfd1d7e6df 100644\n--- a/doc/modules/outlier_detection.rst\n+++ b/doc/modules/outlier_detection.rst\n@@ -252,6 +252,21 @@ This algorithm is illustrated below.\n    :align: center\n    :scale: 75%\n \n+.. _iforest_warm_start:\n+\n+The :class:`ensemble.IsolationForest` supports ``warm_start=True`` which\n+allows you to add more trees to an already fitted model::\n+\n+  >>> from sklearn.ensemble import IsolationForest\n+  >>> import numpy as np\n+  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\n+  >>> # fit 10 trees\n+  >>> clf = IsolationForest(n_estimators=10, warm_start=True)\n+  >>> clf.fit(X)  # doctest: +SKIP\n+  >>> # add & fit 10 more trees\n+  >>> clf.set_params(n_estimators=20)  # doctest: +SKIP\n+  >>> clf.fit(X)  # doctest: +SKIP\n+\n .. topic:: Examples:\n \n    * See :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py` for\n@@ -269,20 +284,6 @@ This algorithm is illustrated below.\n     * Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n       Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n \n-.. _iforest_warm_start:\n-\n-Fitting additional trees\n-~~~~~~~~~~~~~~~~~~~~~~~~\n-The :class:`ensemble.IsolationForest` supports ``warm_start=True`` which\n-allows you to add more trees to an already fitted model.\n-\n-::\n-\n-  >>> clf = IsolationForest(n_estimators=100, warm_start=True)\n-  >>> clf.fit(X)\n-  >>> clf.set_params(n_estimators=200)  # set warm_start and new nr of trees\n-  >>> clf.fit(X) # fit additional 100 trees\n-\n \n Local Outlier Factor\n --------------------\n\nFrom 4529c83da2a99d275b2ceeba52fc33a38711b4d3 Mon Sep 17 00:00:00 2001\nFrom: petibear <40757147+petibear@users.noreply.github.com>\nDate: Tue, 26 Mar 2019 23:22:26 +0100\nSubject: [PATCH 8/8] comments made inline in the doc example\n\n---\n doc/modules/outlier_detection.rst | 8 +++-----\n 1 file changed, 3 insertions(+), 5 deletions(-)\n\ndiff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst\nindex a4ebfd1d7e6df..c061feb0b1d7c 100644\n--- a/doc/modules/outlier_detection.rst\n+++ b/doc/modules/outlier_detection.rst\n@@ -260,12 +260,10 @@ allows you to add more trees to an already fitted model::\n   >>> from sklearn.ensemble import IsolationForest\n   >>> import numpy as np\n   >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\n-  >>> # fit 10 trees\n   >>> clf = IsolationForest(n_estimators=10, warm_start=True)\n-  >>> clf.fit(X)  # doctest: +SKIP\n-  >>> # add & fit 10 more trees\n-  >>> clf.set_params(n_estimators=20)  # doctest: +SKIP\n-  >>> clf.fit(X)  # doctest: +SKIP\n+  >>> clf.fit(X)  # fit 10 trees  # doctest: +SKIP\n+  >>> clf.set_params(n_estimators=20)  # add 10 more trees  # doctest: +SKIP\n+  >>> clf.fit(X)  # fit the added trees  # doctest: +SKIP\n \n .. topic:: Examples:\n \n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/13496"}