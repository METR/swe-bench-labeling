{"instance_id": "scikit-learn__scikit-learn-13496", "score": "C", "diff": "diff --git a/examples/ensemble/plot_isolation_forest.py b/examples/ensemble/plot_isolation_forest.py\nindex 1b79072df..e1be9eb61 100644\n--- a/examples/ensemble/plot_isolation_forest.py\n+++ b/examples/ensemble/plot_isolation_forest.py\n@@ -21,6 +21,11 @@ Random partitioning produces noticeable shorter paths for anomalies.\n Hence, when a forest of random trees collectively produce shorter path lengths\n for particular samples, they are highly likely to be anomalies.\n \n+The IsolationForest also supports warm_start, which allows you to add more\n+trees to an existing fitted model. This can be useful when you want to\n+incrementally add more estimators to the ensemble without retraining the\n+entire model.\n+\n \"\"\"\n print(__doc__)\n \ndiff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..3281069b8 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -98,6 +98,11 @@ class IsolationForest(BaseBagging, OutlierMixin):\n         the default behaviour in the future. As explained in details in the\n         ``offset_`` attribute documentation, the ``decision_function`` becomes\n         dependent on the contamination parameter, in such a way that 0 becomes\n+\n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n         its natural threshold to detect outliers.\n \n         .. versionadded:: 0.20\n@@ -173,7 +178,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,10 +193,12 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\n+        self.warm_start = warm_start\n \n     def _set_oob_score(self, X, y):\n         raise NotImplementedError(\"OOB score not supported by iforest\")\ndiff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py\nindex 67ba2d7f9..98f066687 100644\n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -359,3 +359,39 @@ def test_iforest_chunks_works2(\n ):\n     test_iforest_works(contamination)\n     assert mocked_get_chunk.call_count == n_predict_calls\n+\n+def test_warm_start():\n+    \"\"\"Test warm_start functionality in IsolationForest\"\"\"\n+    X = iris.data\n+    \n+    # Train an initial model\n+    clf = IsolationForest(n_estimators=10, random_state=0, warm_start=True)\n+    clf.fit(X)\n+    \n+    # Get the initial number of trees\n+    initial_n_estimators = len(clf.estimators_)\n+    \n+    # Increase n_estimators and refit\n+    clf.set_params(n_estimators=20)\n+    clf.fit(X)\n+    \n+    # Check that the number of trees has increased\n+    assert len(clf.estimators_) == 20\n+    assert len(clf.estimators_) > initial_n_estimators\n+    \n+    # Check that the first 10 trees are the same as before\n+    clf_2 = IsolationForest(n_estimators=10, random_state=0, warm_start=False)\n+    clf_2.fit(X)\n+    \n+    for tree_1, tree_2 in zip(clf.estimators_[:10], clf_2.estimators_):\n+        assert_array_almost_equal(\n+            tree_1.tree_.feature, tree_2.tree_.feature\n+        )\n+        assert_array_almost_equal(\n+            tree_1.tree_.threshold, tree_2.tree_.threshold\n+        )\n+    \n+    # Check that fitting with warm_start=False resets the ensemble\n+    clf.set_params(warm_start=False, n_estimators=10)\n+    clf.fit(X)\n+    assert len(clf.estimators_) == 10\n\\ No newline at end of file\n", "gold_standard_diff": "diff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst\nindex b27b0c8a59643..c061feb0b1d7c 100644\n--- a/doc/modules/outlier_detection.rst\n+++ b/doc/modules/outlier_detection.rst\n@@ -252,6 +252,19 @@ This algorithm is illustrated below.\n    :align: center\n    :scale: 75%\n \n+.. _iforest_warm_start:\n+\n+The :class:`ensemble.IsolationForest` supports ``warm_start=True`` which\n+allows you to add more trees to an already fitted model::\n+\n+  >>> from sklearn.ensemble import IsolationForest\n+  >>> import numpy as np\n+  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\n+  >>> clf = IsolationForest(n_estimators=10, warm_start=True)\n+  >>> clf.fit(X)  # fit 10 trees  # doctest: +SKIP\n+  >>> clf.set_params(n_estimators=20)  # add 10 more trees  # doctest: +SKIP\n+  >>> clf.fit(X)  # fit the added trees  # doctest: +SKIP\n+\n .. topic:: Examples:\n \n    * See :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py` for\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex d9ae08ed5342e..cdb6b0960f535 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -158,6 +158,10 @@ Support for Python 3.4 and below has been officially dropped.\n - |Enhancement| Minimized the validation of X in\n   :class:`ensemble.AdaBoostClassifier` and :class:`ensemble.AdaBoostRegressor`\n   :issue:`13174` by :user:`Christos Aridas <chkoar>`.\n+  \n+- |Enhancement| :class:`ensemble.IsolationForest` now exposes ``warm_start``\n+  parameter, allowing iterative addition of trees to an isolation \n+  forest. :issue:`13496` by :user:`Peter Marko <petibear>`.\n \n - |Efficiency| Make :class:`ensemble.IsolationForest` more memory efficient\n   by avoiding keeping in memory each tree prediction. :issue:`13260` by\ndiff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd36259e48..eac3519c7d9e2 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -120,6 +120,12 @@ class IsolationForest(BaseBagging, OutlierMixin):\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n+        .. versionadded:: 0.21\n \n     Attributes\n     ----------\n@@ -173,7 +179,8 @@ def __init__(self,\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -185,6 +192,7 @@ def __init__(self,\n             n_estimators=n_estimators,\n             max_samples=max_samples,\n             max_features=max_features,\n+            warm_start=warm_start,\n             n_jobs=n_jobs,\n             random_state=random_state,\n             verbose=verbose)\ndiff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py\nindex 67ba2d7f933e3..9ab679ef66e05 100644\n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -295,6 +295,28 @@ def test_score_samples():\n                        clf2.score_samples([[2., 2.]]))\n \n \n+@pytest.mark.filterwarnings('ignore:default contamination')\n+@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')\n+def test_iforest_warm_start():\n+    \"\"\"Test iterative addition of iTrees to an iForest \"\"\"\n+\n+    rng = check_random_state(0)\n+    X = rng.randn(20, 2)\n+\n+    # fit first 10 trees\n+    clf = IsolationForest(n_estimators=10, max_samples=20,\n+                          random_state=rng, warm_start=True)\n+    clf.fit(X)\n+    # remember the 1st tree\n+    tree_1 = clf.estimators_[0]\n+    # fit another 10 trees\n+    clf.set_params(n_estimators=20)\n+    clf.fit(X)\n+    # expecting 20 fitted trees and no overwritten trees\n+    assert len(clf.estimators_) == 20\n+    assert clf.estimators_[0] is tree_1\n+\n+\n @pytest.mark.filterwarnings('ignore:default contamination')\n @pytest.mark.filterwarnings('ignore:behaviour=\"old\"')\n def test_deprecation():\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/13496"}