diff --git a/add_test.py b/add_test.py
new file mode 100644
index 000000000..b63da4933
--- /dev/null
+++ b/add_test.py
@@ -0,0 +1,35 @@
+# Add regression test to the test file
+with open('sklearn/feature_extraction/tests/test_text.py', 'r', encoding='utf-8') as f:
+    lines = f.readlines()
+
+# Find where to insert the test (after the last assertion in test_strip_accents)
+insert_index = None
+for i, line in enumerate(lines):
+    if 'assert strip_accents_unicode(a) == expected' in line and i > 95:
+        insert_index = i + 1
+        # Skip empty lines
+        while insert_index < len(lines) and lines[insert_index].strip() == '':
+            insert_index += 1
+        break
+
+if insert_index:
+    # Insert the regression test
+    regression_test = '''
+    # Regression test for issue: strip_accents_unicode should handle NFKD strings
+    # https://github.com/scikit-learn/scikit-learn/issues/...
+    # Test with precomposed character (NFC form)
+    s1 = chr(241)  # LATIN SMALL LETTER N WITH TILDE: ñ
+    # Test with decomposed character (NFD/NFKD form)
+    s2 = chr(110) + chr(771)  # LATIN SMALL LETTER N + COMBINING TILDE: ñ
+    # Both should normalize to the same result
+    assert strip_accents_unicode(s1) == 'n'
+    assert strip_accents_unicode(s2) == 'n'
+    assert strip_accents_unicode(s1) == strip_accents_unicode(s2)
+'''
+    lines.insert(insert_index, regression_test)
+    
+    with open('sklearn/feature_extraction/tests/test_text.py', 'w', encoding='utf-8') as f:
+        f.writelines(lines)
+    print("Added regression test successfully")
+else:
+    print("Could not find insertion point")
diff --git a/fix_strip_accents.py b/fix_strip_accents.py
new file mode 100644
index 000000000..1a8ecad26
--- /dev/null
+++ b/fix_strip_accents.py
@@ -0,0 +1,24 @@
+import re
+
+# Read the file
+with open('/testbed/sklearn/feature_extraction/text.py', 'r', encoding='utf-8') as f:
+    content = f.read()
+
+# Find and replace the function
+old_code = """    normalized = unicodedata.normalize('NFKD', s)
+    if normalized == s:
+        return s
+    else:
+        return ''.join([c for c in normalized if not unicodedata.combining(c)])"""
+
+new_code = """    normalized = unicodedata.normalize('NFKD', s)
+    return ''.join([c for c in normalized if not unicodedata.combining(c)])"""
+
+# Replace the code
+content = content.replace(old_code, new_code)
+
+# Write back
+with open('/testbed/sklearn/feature_extraction/text.py', 'w', encoding='utf-8') as f:
+    f.write(content)
+
+print("Fixed strip_accents_unicode function")
diff --git a/sklearn/feature_extraction/tests/test_text.py b/sklearn/feature_extraction/tests/test_text.py
index 7b7697ff4..8a07db3ea 100644
--- a/sklearn/feature_extraction/tests/test_text.py
+++ b/sklearn/feature_extraction/tests/test_text.py
@@ -98,6 +98,17 @@ def test_strip_accents():
     assert strip_accents_unicode(a) == expected
 
 
+
+    # Regression test for issue: strip_accents_unicode should handle NFKD strings
+    # https://github.com/scikit-learn/scikit-learn/issues/...
+    # Test with precomposed character (NFC form)
+    s1 = chr(241)  # LATIN SMALL LETTER N WITH TILDE: ñ
+    # Test with decomposed character (NFD/NFKD form)
+    s2 = chr(110) + chr(771)  # LATIN SMALL LETTER N + COMBINING TILDE: ñ
+    # Both should normalize to the same result
+    assert strip_accents_unicode(s1) == 'n'
+    assert strip_accents_unicode(s2) == 'n'
+    assert strip_accents_unicode(s1) == strip_accents_unicode(s2)
 def test_to_ascii():
     # check some classical latin accentuated symbols
     a = 'àáâãäåçèéêë'
diff --git a/sklearn/feature_extraction/tests/test_text.py.backup b/sklearn/feature_extraction/tests/test_text.py.backup
new file mode 100644
index 000000000..7b7697ff4
--- /dev/null
+++ b/sklearn/feature_extraction/tests/test_text.py.backup
@@ -0,0 +1,1345 @@
+# -*- coding: utf-8 -*-
+from collections.abc import Mapping
+import re
+import warnings
+
+import pytest
+from scipy import sparse
+
+from sklearn.feature_extraction.text import strip_tags
+from sklearn.feature_extraction.text import strip_accents_unicode
+from sklearn.feature_extraction.text import strip_accents_ascii
+
+from sklearn.feature_extraction.text import HashingVectorizer
+from sklearn.feature_extraction.text import CountVectorizer
+from sklearn.feature_extraction.text import TfidfTransformer
+from sklearn.feature_extraction.text import TfidfVectorizer
+
+from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
+
+from sklearn.model_selection import train_test_split
+from sklearn.model_selection import cross_val_score
+from sklearn.model_selection import GridSearchCV
+from sklearn.pipeline import Pipeline
+from sklearn.svm import LinearSVC
+
+from sklearn.base import clone
+
+import numpy as np
+from numpy.testing import assert_array_almost_equal
+from numpy.testing import assert_array_equal
+from sklearn.utils import IS_PYPY
+from sklearn.exceptions import ChangedBehaviorWarning
+from sklearn.utils.testing import (assert_almost_equal,
+                                   assert_warns_message, assert_raise_message,
+                                   clean_warning_registry,
+                                   SkipTest, assert_no_warnings,
+                                   fails_if_pypy, assert_allclose_dense_sparse,
+                                   skip_if_32bit)
+from collections import defaultdict
+from functools import partial
+import pickle
+from io import StringIO
+
+JUNK_FOOD_DOCS = (
+    "the pizza pizza beer copyright",
+    "the pizza burger beer copyright",
+    "the the pizza beer beer copyright",
+    "the burger beer beer copyright",
+    "the coke burger coke copyright",
+    "the coke burger burger",
+)
+
+NOTJUNK_FOOD_DOCS = (
+    "the salad celeri copyright",
+    "the salad salad sparkling water copyright",
+    "the the celeri celeri copyright",
+    "the tomato tomato salad water",
+    "the tomato salad water copyright",
+)
+
+ALL_FOOD_DOCS = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
+
+
+def uppercase(s):
+    return strip_accents_unicode(s).upper()
+
+
+def strip_eacute(s):
+    return s.replace('é', 'e')
+
+
+def split_tokenize(s):
+    return s.split()
+
+
+def lazy_analyze(s):
+    return ['the_ultimate_feature']
+
+
+def test_strip_accents():
+    # check some classical latin accentuated symbols
+    a = 'àáâãäåçèéêë'
+    expected = 'aaaaaaceeee'
+    assert strip_accents_unicode(a) == expected
+
+    a = 'ìíîïñòóôõöùúûüý'
+    expected = 'iiiinooooouuuuy'
+    assert strip_accents_unicode(a) == expected
+
+    # check some arabic
+    a = '\u0625'  # alef with a hamza below: إ
+    expected = '\u0627'  # simple alef: ا
+    assert strip_accents_unicode(a) == expected
+
+    # mix letters accentuated and not
+    a = "this is à test"
+    expected = 'this is a test'
+    assert strip_accents_unicode(a) == expected
+
+
+def test_to_ascii():
+    # check some classical latin accentuated symbols
+    a = 'àáâãäåçèéêë'
+    expected = 'aaaaaaceeee'
+    assert strip_accents_ascii(a) == expected
+
+    a = "ìíîïñòóôõöùúûüý"
+    expected = 'iiiinooooouuuuy'
+    assert strip_accents_ascii(a) == expected
+
+    # check some arabic
+    a = '\u0625'  # halef with a hamza below
+    expected = ''  # halef has no direct ascii match
+    assert strip_accents_ascii(a) == expected
+
+    # mix letters accentuated and not
+    a = "this is à test"
+    expected = 'this is a test'
+    assert strip_accents_ascii(a) == expected
+
+
+@pytest.mark.parametrize('Vectorizer', (CountVectorizer, HashingVectorizer))
+def test_word_analyzer_unigrams(Vectorizer):
+    wa = Vectorizer(strip_accents='ascii').build_analyzer()
+    text = ("J'ai mangé du kangourou  ce midi, "
+            "c'était pas très bon.")
+    expected = ['ai', 'mange', 'du', 'kangourou', 'ce', 'midi',
+                'etait', 'pas', 'tres', 'bon']
+    assert wa(text) == expected
+
+    text = "This is a test, really.\n\n I met Harry yesterday."
+    expected = ['this', 'is', 'test', 'really', 'met', 'harry',
+                'yesterday']
+    assert wa(text) == expected
+
+    wa = Vectorizer(input='file').build_analyzer()
+    text = StringIO("This is a test with a file-like object!")
+    expected = ['this', 'is', 'test', 'with', 'file', 'like',
+                'object']
+    assert wa(text) == expected
+
+    # with custom preprocessor
+    wa = Vectorizer(preprocessor=uppercase).build_analyzer()
+    text = ("J'ai mangé du kangourou  ce midi, "
+            " c'était pas très bon.")
+    expected = ['AI', 'MANGE', 'DU', 'KANGOUROU', 'CE', 'MIDI',
+                'ETAIT', 'PAS', 'TRES', 'BON']
+    assert wa(text) == expected
+
+    # with custom tokenizer
+    wa = Vectorizer(tokenizer=split_tokenize,
+                    strip_accents='ascii').build_analyzer()
+    text = ("J'ai mangé du kangourou  ce midi, "
+            "c'était pas très bon.")
+    expected = ["j'ai", 'mange', 'du', 'kangourou', 'ce', 'midi,',
+                "c'etait", 'pas', 'tres', 'bon.']
+    assert wa(text) == expected
+
+
+def test_word_analyzer_unigrams_and_bigrams():
+    wa = CountVectorizer(analyzer="word", strip_accents='unicode',
+                         ngram_range=(1, 2)).build_analyzer()
+
+    text = "J'ai mangé du kangourou  ce midi, c'était pas très bon."
+    expected = ['ai', 'mange', 'du', 'kangourou', 'ce', 'midi',
+                'etait', 'pas', 'tres', 'bon', 'ai mange', 'mange du',
+                'du kangourou', 'kangourou ce', 'ce midi', 'midi etait',
+                'etait pas', 'pas tres', 'tres bon']
+    assert wa(text) == expected
+
+
+def test_unicode_decode_error():
+    # decode_error default to strict, so this should fail
+    # First, encode (as bytes) a unicode string.
+    text = "J'ai mangé du kangourou  ce midi, c'était pas très bon."
+    text_bytes = text.encode('utf-8')
+
+    # Then let the Analyzer try to decode it as ascii. It should fail,
+    # because we have given it an incorrect encoding.
+    wa = CountVectorizer(ngram_range=(1, 2), encoding='ascii').build_analyzer()
+    with pytest.raises(UnicodeDecodeError):
+        wa(text_bytes)
+
+    ca = CountVectorizer(analyzer='char', ngram_range=(3, 6),
+                         encoding='ascii').build_analyzer()
+    with pytest.raises(UnicodeDecodeError):
+        ca(text_bytes)
+
+
+def test_char_ngram_analyzer():
+    cnga = CountVectorizer(analyzer='char', strip_accents='unicode',
+                           ngram_range=(3, 6)).build_analyzer()
+
+    text = "J'ai mangé du kangourou  ce midi, c'était pas très bon"
+    expected = ["j'a", "'ai", 'ai ', 'i m', ' ma']
+    assert cnga(text)[:5] == expected
+    expected = ['s tres', ' tres ', 'tres b', 'res bo', 'es bon']
+    assert cnga(text)[-5:] == expected
+
+    text = "This \n\tis a test, really.\n\n I met Harry yesterday"
+    expected = ['thi', 'his', 'is ', 's i', ' is']
+    assert cnga(text)[:5] == expected
+
+    expected = [' yeste', 'yester', 'esterd', 'sterda', 'terday']
+    assert cnga(text)[-5:] == expected
+
+    cnga = CountVectorizer(input='file', analyzer='char',
+                           ngram_range=(3, 6)).build_analyzer()
+    text = StringIO("This is a test with a file-like object!")
+    expected = ['thi', 'his', 'is ', 's i', ' is']
+    assert cnga(text)[:5] == expected
+
+
+def test_char_wb_ngram_analyzer():
+    cnga = CountVectorizer(analyzer='char_wb', strip_accents='unicode',
+                           ngram_range=(3, 6)).build_analyzer()
+
+    text = "This \n\tis a test, really.\n\n I met Harry yesterday"
+    expected = [' th', 'thi', 'his', 'is ', ' thi']
+    assert cnga(text)[:5] == expected
+
+    expected = ['yester', 'esterd', 'sterda', 'terday', 'erday ']
+    assert cnga(text)[-5:] == expected
+
+    cnga = CountVectorizer(input='file', analyzer='char_wb',
+                           ngram_range=(3, 6)).build_analyzer()
+    text = StringIO("A test with a file-like object!")
+    expected = [' a ', ' te', 'tes', 'est', 'st ', ' tes']
+    assert cnga(text)[:6] == expected
+
+
+def test_word_ngram_analyzer():
+    cnga = CountVectorizer(analyzer='word', strip_accents='unicode',
+                           ngram_range=(3, 6)).build_analyzer()
+
+    text = "This \n\tis a test, really.\n\n I met Harry yesterday"
+    expected = ['this is test', 'is test really', 'test really met']
+    assert cnga(text)[:3] == expected
+
+    expected = ['test really met harry yesterday',
+                'this is test really met harry',
+                'is test really met harry yesterday']
+    assert cnga(text)[-3:] == expected
+
+    cnga_file = CountVectorizer(input='file', analyzer='word',
+                                ngram_range=(3, 6)).build_analyzer()
+    file = StringIO(text)
+    assert cnga_file(file) == cnga(text)
+
+
+def test_countvectorizer_custom_vocabulary():
+    vocab = {"pizza": 0, "beer": 1}
+    terms = set(vocab.keys())
+
+    # Try a few of the supported types.
+    for typ in [dict, list, iter, partial(defaultdict, int)]:
+        v = typ(vocab)
+        vect = CountVectorizer(vocabulary=v)
+        vect.fit(JUNK_FOOD_DOCS)
+        if isinstance(v, Mapping):
+            assert vect.vocabulary_ == vocab
+        else:
+            assert set(vect.vocabulary_) == terms
+        X = vect.transform(JUNK_FOOD_DOCS)
+        assert X.shape[1] == len(terms)
+        v = typ(vocab)
+        vect = CountVectorizer(vocabulary=v)
+        inv = vect.inverse_transform(X)
+        assert len(inv) == X.shape[0]
+
+
+def test_countvectorizer_custom_vocabulary_pipeline():
+    what_we_like = ["pizza", "beer"]
+    pipe = Pipeline([
+        ('count', CountVectorizer(vocabulary=what_we_like)),
+        ('tfidf', TfidfTransformer())])
+    X = pipe.fit_transform(ALL_FOOD_DOCS)
+    assert (set(pipe.named_steps['count'].vocabulary_) ==
+            set(what_we_like))
+    assert X.shape[1] == len(what_we_like)
+
+
+def test_countvectorizer_custom_vocabulary_repeated_indices():
+    vocab = {"pizza": 0, "beer": 0}
+    try:
+        CountVectorizer(vocabulary=vocab)
+    except ValueError as e:
+        assert "vocabulary contains repeated indices" in str(e).lower()
+
+
+def test_countvectorizer_custom_vocabulary_gap_index():
+    vocab = {"pizza": 1, "beer": 2}
+    try:
+        CountVectorizer(vocabulary=vocab)
+    except ValueError as e:
+        assert "doesn't contain index" in str(e).lower()
+
+
+def test_countvectorizer_stop_words():
+    cv = CountVectorizer()
+    cv.set_params(stop_words='english')
+    assert cv.get_stop_words() == ENGLISH_STOP_WORDS
+    cv.set_params(stop_words='_bad_str_stop_')
+    with pytest.raises(ValueError):
+        cv.get_stop_words()
+    cv.set_params(stop_words='_bad_unicode_stop_')
+    with pytest.raises(ValueError):
+        cv.get_stop_words()
+    stoplist = ['some', 'other', 'words']
+    cv.set_params(stop_words=stoplist)
+    assert cv.get_stop_words() == set(stoplist)
+
+
+def test_countvectorizer_empty_vocabulary():
+    try:
+        vect = CountVectorizer(vocabulary=[])
+        vect.fit(["foo"])
+        assert False, "we shouldn't get here"
+    except ValueError as e:
+        assert "empty vocabulary" in str(e).lower()
+
+    try:
+        v = CountVectorizer(max_df=1.0, stop_words="english")
+        # fit on stopwords only
+        v.fit(["to be or not to be", "and me too", "and so do you"])
+        assert False, "we shouldn't get here"
+    except ValueError as e:
+        assert "empty vocabulary" in str(e).lower()
+
+
+def test_fit_countvectorizer_twice():
+    cv = CountVectorizer()
+    X1 = cv.fit_transform(ALL_FOOD_DOCS[:5])
+    X2 = cv.fit_transform(ALL_FOOD_DOCS[5:])
+    assert X1.shape[1] != X2.shape[1]
+
+
+def test_tf_idf_smoothing():
+    X = [[1, 1, 1],
+         [1, 1, 0],
+         [1, 0, 0]]
+    tr = TfidfTransformer(smooth_idf=True, norm='l2')
+    tfidf = tr.fit_transform(X).toarray()
+    assert (tfidf >= 0).all()
+
+    # check normalization
+    assert_array_almost_equal((tfidf ** 2).sum(axis=1), [1., 1., 1.])
+
+    # this is robust to features with only zeros
+    X = [[1, 1, 0],
+         [1, 1, 0],
+         [1, 0, 0]]
+    tr = TfidfTransformer(smooth_idf=True, norm='l2')
+    tfidf = tr.fit_transform(X).toarray()
+    assert (tfidf >= 0).all()
+
+
+def test_tfidf_no_smoothing():
+    X = [[1, 1, 1],
+         [1, 1, 0],
+         [1, 0, 0]]
+    tr = TfidfTransformer(smooth_idf=False, norm='l2')
+    tfidf = tr.fit_transform(X).toarray()
+    assert (tfidf >= 0).all()
+
+    # check normalization
+    assert_array_almost_equal((tfidf ** 2).sum(axis=1), [1., 1., 1.])
+
+    # the lack of smoothing make IDF fragile in the presence of feature with
+    # only zeros
+    X = [[1, 1, 0],
+         [1, 1, 0],
+         [1, 0, 0]]
+    tr = TfidfTransformer(smooth_idf=False, norm='l2')
+
+    clean_warning_registry()
+    with warnings.catch_warnings(record=True) as w:
+        1. / np.array([0.])
+        numpy_provides_div0_warning = len(w) == 1
+
+    in_warning_message = 'divide by zero'
+    tfidf = assert_warns_message(RuntimeWarning, in_warning_message,
+                                 tr.fit_transform, X).toarray()
+    if not numpy_provides_div0_warning:
+        raise SkipTest("Numpy does not provide div 0 warnings.")
+
+
+def test_sublinear_tf():
+    X = [[1], [2], [3]]
+    tr = TfidfTransformer(sublinear_tf=True, use_idf=False, norm=None)
+    tfidf = tr.fit_transform(X).toarray()
+    assert tfidf[0] == 1
+    assert tfidf[1] > tfidf[0]
+    assert tfidf[2] > tfidf[1]
+    assert tfidf[1] < 2
+    assert tfidf[2] < 3
+
+
+def test_vectorizer():
+    # raw documents as an iterator
+    train_data = iter(ALL_FOOD_DOCS[:-1])
+    test_data = [ALL_FOOD_DOCS[-1]]
+    n_train = len(ALL_FOOD_DOCS) - 1
+
+    # test without vocabulary
+    v1 = CountVectorizer(max_df=0.5)
+    counts_train = v1.fit_transform(train_data)
+    if hasattr(counts_train, 'tocsr'):
+        counts_train = counts_train.tocsr()
+    assert counts_train[0, v1.vocabulary_["pizza"]] == 2
+
+    # build a vectorizer v1 with the same vocabulary as the one fitted by v1
+    v2 = CountVectorizer(vocabulary=v1.vocabulary_)
+
+    # compare that the two vectorizer give the same output on the test sample
+    for v in (v1, v2):
+        counts_test = v.transform(test_data)
+        if hasattr(counts_test, 'tocsr'):
+            counts_test = counts_test.tocsr()
+
+        vocabulary = v.vocabulary_
+        assert counts_test[0, vocabulary["salad"]] == 1
+        assert counts_test[0, vocabulary["tomato"]] == 1
+        assert counts_test[0, vocabulary["water"]] == 1
+
+        # stop word from the fixed list
+        assert "the" not in vocabulary
+
+        # stop word found automatically by the vectorizer DF thresholding
+        # words that are high frequent across the complete corpus are likely
+        # to be not informative (either real stop words of extraction
+        # artifacts)
+        assert "copyright" not in vocabulary
+
+        # not present in the sample
+        assert counts_test[0, vocabulary["coke"]] == 0
+        assert counts_test[0, vocabulary["burger"]] == 0
+        assert counts_test[0, vocabulary["beer"]] == 0
+        assert counts_test[0, vocabulary["pizza"]] == 0
+
+    # test tf-idf
+    t1 = TfidfTransformer(norm='l1')
+    tfidf = t1.fit(counts_train).transform(counts_train).toarray()
+    assert len(t1.idf_) == len(v1.vocabulary_)
+    assert tfidf.shape == (n_train, len(v1.vocabulary_))
+
+    # test tf-idf with new data
+    tfidf_test = t1.transform(counts_test).toarray()
+    assert tfidf_test.shape == (len(test_data), len(v1.vocabulary_))
+
+    # test tf alone
+    t2 = TfidfTransformer(norm='l1', use_idf=False)
+    tf = t2.fit(counts_train).transform(counts_train).toarray()
+    assert not hasattr(t2, "idf_")
+
+    # test idf transform with unlearned idf vector
+    t3 = TfidfTransformer(use_idf=True)
+    with pytest.raises(ValueError):
+        t3.transform(counts_train)
+
+    # test idf transform with incompatible n_features
+    X = [[1, 1, 5],
+         [1, 1, 0]]
+    t3.fit(X)
+    X_incompt = [[1, 3],
+                 [1, 3]]
+    with pytest.raises(ValueError):
+        t3.transform(X_incompt)
+
+    # L1-normalized term frequencies sum to one
+    assert_array_almost_equal(np.sum(tf, axis=1), [1.0] * n_train)
+
+    # test the direct tfidf vectorizer
+    # (equivalent to term count vectorizer + tfidf transformer)
+    train_data = iter(ALL_FOOD_DOCS[:-1])
+    tv = TfidfVectorizer(norm='l1')
+
+    tv.max_df = v1.max_df
+    tfidf2 = tv.fit_transform(train_data).toarray()
+    assert not tv.fixed_vocabulary_
+    assert_array_almost_equal(tfidf, tfidf2)
+
+    # test the direct tfidf vectorizer with new data
+    tfidf_test2 = tv.transform(test_data).toarray()
+    assert_array_almost_equal(tfidf_test, tfidf_test2)
+
+    # test transform on unfitted vectorizer with empty vocabulary
+    v3 = CountVectorizer(vocabulary=None)
+    with pytest.raises(ValueError):
+        v3.transform(train_data)
+
+    # ascii preprocessor?
+    v3.set_params(strip_accents='ascii', lowercase=False)
+    processor = v3.build_preprocessor()
+    text = ("J'ai mangé du kangourou  ce midi, "
+            "c'était pas très bon.")
+    expected = strip_accents_ascii(text)
+    result = processor(text)
+    assert expected == result
+
+    # error on bad strip_accents param
+    v3.set_params(strip_accents='_gabbledegook_', preprocessor=None)
+    with pytest.raises(ValueError):
+        v3.build_preprocessor()
+
+    # error with bad analyzer type
+    v3.set_params = '_invalid_analyzer_type_'
+    with pytest.raises(ValueError):
+        v3.build_analyzer()
+
+
+def test_tfidf_vectorizer_setters():
+    tv = TfidfVectorizer(norm='l2', use_idf=False, smooth_idf=False,
+                         sublinear_tf=False)
+    tv.norm = 'l1'
+    assert tv._tfidf.norm == 'l1'
+    tv.use_idf = True
+    assert tv._tfidf.use_idf
+    tv.smooth_idf = True
+    assert tv._tfidf.smooth_idf
+    tv.sublinear_tf = True
+    assert tv._tfidf.sublinear_tf
+
+
+# FIXME Remove copy parameter support in 0.24
+def test_tfidf_vectorizer_deprecationwarning():
+    msg = ("'copy' param is unused and has been deprecated since "
+           "version 0.22. Backward compatibility for 'copy' will "
+           "be removed in 0.24.")
+    with pytest.warns(DeprecationWarning, match=msg):
+        tv = TfidfVectorizer()
+        train_data = JUNK_FOOD_DOCS
+        tv.fit(train_data)
+        tv.transform(train_data, copy=True)
+
+
+@fails_if_pypy
+def test_hashing_vectorizer():
+    v = HashingVectorizer()
+    X = v.transform(ALL_FOOD_DOCS)
+    token_nnz = X.nnz
+    assert X.shape == (len(ALL_FOOD_DOCS), v.n_features)
+    assert X.dtype == v.dtype
+
+    # By default the hashed values receive a random sign and l2 normalization
+    # makes the feature values bounded
+    assert np.min(X.data) > -1
+    assert np.min(X.data) < 0
+    assert np.max(X.data) > 0
+    assert np.max(X.data) < 1
+
+    # Check that the rows are normalized
+    for i in range(X.shape[0]):
+        assert_almost_equal(np.linalg.norm(X[0].data, 2), 1.0)
+
+    # Check vectorization with some non-default parameters
+    v = HashingVectorizer(ngram_range=(1, 2), norm='l1')
+    X = v.transform(ALL_FOOD_DOCS)
+    assert X.shape == (len(ALL_FOOD_DOCS), v.n_features)
+    assert X.dtype == v.dtype
+
+    # ngrams generate more non zeros
+    ngrams_nnz = X.nnz
+    assert ngrams_nnz > token_nnz
+    assert ngrams_nnz < 2 * token_nnz
+
+    # makes the feature values bounded
+    assert np.min(X.data) > -1
+    assert np.max(X.data) < 1
+
+    # Check that the rows are normalized
+    for i in range(X.shape[0]):
+        assert_almost_equal(np.linalg.norm(X[0].data, 1), 1.0)
+
+
+def test_feature_names():
+    cv = CountVectorizer(max_df=0.5)
+
+    # test for Value error on unfitted/empty vocabulary
+    with pytest.raises(ValueError):
+        cv.get_feature_names()
+    assert not cv.fixed_vocabulary_
+
+    # test for vocabulary learned from data
+    X = cv.fit_transform(ALL_FOOD_DOCS)
+    n_samples, n_features = X.shape
+    assert len(cv.vocabulary_) == n_features
+
+    feature_names = cv.get_feature_names()
+    assert len(feature_names) == n_features
+    assert_array_equal(['beer', 'burger', 'celeri', 'coke', 'pizza',
+                        'salad', 'sparkling', 'tomato', 'water'],
+                       feature_names)
+
+    for idx, name in enumerate(feature_names):
+        assert idx == cv.vocabulary_.get(name)
+
+    # test for custom vocabulary
+    vocab = ['beer', 'burger', 'celeri', 'coke', 'pizza',
+             'salad', 'sparkling', 'tomato', 'water']
+
+    cv = CountVectorizer(vocabulary=vocab)
+    feature_names = cv.get_feature_names()
+    assert_array_equal(['beer', 'burger', 'celeri', 'coke', 'pizza', 'salad',
+                        'sparkling', 'tomato', 'water'], feature_names)
+    assert cv.fixed_vocabulary_
+
+    for idx, name in enumerate(feature_names):
+        assert idx == cv.vocabulary_.get(name)
+
+
+@pytest.mark.parametrize('Vectorizer', (CountVectorizer, TfidfVectorizer))
+def test_vectorizer_max_features(Vectorizer):
+    expected_vocabulary = {'burger', 'beer', 'salad', 'pizza'}
+    expected_stop_words = {'celeri', 'tomato', 'copyright', 'coke',
+                           'sparkling', 'water', 'the'}
+
+    # test bounded number of extracted features
+    vectorizer = Vectorizer(max_df=0.6, max_features=4)
+    vectorizer.fit(ALL_FOOD_DOCS)
+    assert set(vectorizer.vocabulary_) == expected_vocabulary
+    assert vectorizer.stop_words_ == expected_stop_words
+
+
+def test_count_vectorizer_max_features():
+    # Regression test: max_features didn't work correctly in 0.14.
+
+    cv_1 = CountVectorizer(max_features=1)
+    cv_3 = CountVectorizer(max_features=3)
+    cv_None = CountVectorizer(max_features=None)
+
+    counts_1 = cv_1.fit_transform(JUNK_FOOD_DOCS).sum(axis=0)
+    counts_3 = cv_3.fit_transform(JUNK_FOOD_DOCS).sum(axis=0)
+    counts_None = cv_None.fit_transform(JUNK_FOOD_DOCS).sum(axis=0)
+
+    features_1 = cv_1.get_feature_names()
+    features_3 = cv_3.get_feature_names()
+    features_None = cv_None.get_feature_names()
+
+    # The most common feature is "the", with frequency 7.
+    assert 7 == counts_1.max()
+    assert 7 == counts_3.max()
+    assert 7 == counts_None.max()
+
+    # The most common feature should be the same
+    assert "the" == features_1[np.argmax(counts_1)]
+    assert "the" == features_3[np.argmax(counts_3)]
+    assert "the" == features_None[np.argmax(counts_None)]
+
+
+def test_vectorizer_max_df():
+    test_data = ['abc', 'dea', 'eat']
+    vect = CountVectorizer(analyzer='char', max_df=1.0)
+    vect.fit(test_data)
+    assert 'a' in vect.vocabulary_.keys()
+    assert len(vect.vocabulary_.keys()) == 6
+    assert len(vect.stop_words_) == 0
+
+    vect.max_df = 0.5  # 0.5 * 3 documents -> max_doc_count == 1.5
+    vect.fit(test_data)
+    assert 'a' not in vect.vocabulary_.keys()  # {ae} ignored
+    assert len(vect.vocabulary_.keys()) == 4    # {bcdt} remain
+    assert 'a' in vect.stop_words_
+    assert len(vect.stop_words_) == 2
+
+    vect.max_df = 1
+    vect.fit(test_data)
+    assert 'a' not in vect.vocabulary_.keys()  # {ae} ignored
+    assert len(vect.vocabulary_.keys()) == 4    # {bcdt} remain
+    assert 'a' in vect.stop_words_
+    assert len(vect.stop_words_) == 2
+
+
+def test_vectorizer_min_df():
+    test_data = ['abc', 'dea', 'eat']
+    vect = CountVectorizer(analyzer='char', min_df=1)
+    vect.fit(test_data)
+    assert 'a' in vect.vocabulary_.keys()
+    assert len(vect.vocabulary_.keys()) == 6
+    assert len(vect.stop_words_) == 0
+
+    vect.min_df = 2
+    vect.fit(test_data)
+    assert 'c' not in vect.vocabulary_.keys()  # {bcdt} ignored
+    assert len(vect.vocabulary_.keys()) == 2    # {ae} remain
+    assert 'c' in vect.stop_words_
+    assert len(vect.stop_words_) == 4
+
+    vect.min_df = 0.8  # 0.8 * 3 documents -> min_doc_count == 2.4
+    vect.fit(test_data)
+    assert 'c' not in vect.vocabulary_.keys()  # {bcdet} ignored
+    assert len(vect.vocabulary_.keys()) == 1    # {a} remains
+    assert 'c' in vect.stop_words_
+    assert len(vect.stop_words_) == 5
+
+
+def test_count_binary_occurrences():
+    # by default multiple occurrences are counted as longs
+    test_data = ['aaabc', 'abbde']
+    vect = CountVectorizer(analyzer='char', max_df=1.0)
+    X = vect.fit_transform(test_data).toarray()
+    assert_array_equal(['a', 'b', 'c', 'd', 'e'], vect.get_feature_names())
+    assert_array_equal([[3, 1, 1, 0, 0],
+                        [1, 2, 0, 1, 1]], X)
+
+    # using boolean features, we can fetch the binary occurrence info
+    # instead.
+    vect = CountVectorizer(analyzer='char', max_df=1.0, binary=True)
+    X = vect.fit_transform(test_data).toarray()
+    assert_array_equal([[1, 1, 1, 0, 0],
+                        [1, 1, 0, 1, 1]], X)
+
+    # check the ability to change the dtype
+    vect = CountVectorizer(analyzer='char', max_df=1.0,
+                           binary=True, dtype=np.float32)
+    X_sparse = vect.fit_transform(test_data)
+    assert X_sparse.dtype == np.float32
+
+
+@fails_if_pypy
+def test_hashed_binary_occurrences():
+    # by default multiple occurrences are counted as longs
+    test_data = ['aaabc', 'abbde']
+    vect = HashingVectorizer(alternate_sign=False, analyzer='char', norm=None)
+    X = vect.transform(test_data)
+    assert np.max(X[0:1].data) == 3
+    assert np.max(X[1:2].data) == 2
+    assert X.dtype == np.float64
+
+    # using boolean features, we can fetch the binary occurrence info
+    # instead.
+    vect = HashingVectorizer(analyzer='char', alternate_sign=False,
+                             binary=True, norm=None)
+    X = vect.transform(test_data)
+    assert np.max(X.data) == 1
+    assert X.dtype == np.float64
+
+    # check the ability to change the dtype
+    vect = HashingVectorizer(analyzer='char', alternate_sign=False,
+                             binary=True, norm=None, dtype=np.float64)
+    X = vect.transform(test_data)
+    assert X.dtype == np.float64
+
+
+@pytest.mark.parametrize('Vectorizer', (CountVectorizer, TfidfVectorizer))
+def test_vectorizer_inverse_transform(Vectorizer):
+    # raw documents
+    data = ALL_FOOD_DOCS
+    vectorizer = Vectorizer()
+    transformed_data = vectorizer.fit_transform(data)
+    inversed_data = vectorizer.inverse_transform(transformed_data)
+    analyze = vectorizer.build_analyzer()
+    for doc, inversed_terms in zip(data, inversed_data):
+        terms = np.sort(np.unique(analyze(doc)))
+        inversed_terms = np.sort(np.unique(inversed_terms))
+        assert_array_equal(terms, inversed_terms)
+
+    # Test that inverse_transform also works with numpy arrays
+    transformed_data = transformed_data.toarray()
+    inversed_data2 = vectorizer.inverse_transform(transformed_data)
+    for terms, terms2 in zip(inversed_data, inversed_data2):
+        assert_array_equal(np.sort(terms), np.sort(terms2))
+
+
+def test_count_vectorizer_pipeline_grid_selection():
+    # raw documents
+    data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
+
+    # label junk food as -1, the others as +1
+    target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
+
+    # split the dataset for model development and final evaluation
+    train_data, test_data, target_train, target_test = train_test_split(
+        data, target, test_size=.2, random_state=0)
+
+    pipeline = Pipeline([('vect', CountVectorizer()),
+                         ('svc', LinearSVC())])
+
+    parameters = {
+        'vect__ngram_range': [(1, 1), (1, 2)],
+        'svc__loss': ('hinge', 'squared_hinge')
+    }
+
+    # find the best parameters for both the feature extraction and the
+    # classifier
+    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, cv=3)
+
+    # Check that the best model found by grid search is 100% correct on the
+    # held out evaluation set.
+    pred = grid_search.fit(train_data, target_train).predict(test_data)
+    assert_array_equal(pred, target_test)
+
+    # on this toy dataset bigram representation which is used in the last of
+    # the grid_search is considered the best estimator since they all converge
+    # to 100% accuracy models
+    assert grid_search.best_score_ == 1.0
+    best_vectorizer = grid_search.best_estimator_.named_steps['vect']
+    assert best_vectorizer.ngram_range == (1, 1)
+
+
+def test_vectorizer_pipeline_grid_selection():
+    # raw documents
+    data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
+
+    # label junk food as -1, the others as +1
+    target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
+
+    # split the dataset for model development and final evaluation
+    train_data, test_data, target_train, target_test = train_test_split(
+        data, target, test_size=.1, random_state=0)
+
+    pipeline = Pipeline([('vect', TfidfVectorizer()),
+                         ('svc', LinearSVC())])
+
+    parameters = {
+        'vect__ngram_range': [(1, 1), (1, 2)],
+        'vect__norm': ('l1', 'l2'),
+        'svc__loss': ('hinge', 'squared_hinge'),
+    }
+
+    # find the best parameters for both the feature extraction and the
+    # classifier
+    grid_search = GridSearchCV(pipeline, parameters, n_jobs=1)
+
+    # Check that the best model found by grid search is 100% correct on the
+    # held out evaluation set.
+    pred = grid_search.fit(train_data, target_train).predict(test_data)
+    assert_array_equal(pred, target_test)
+
+    # on this toy dataset bigram representation which is used in the last of
+    # the grid_search is considered the best estimator since they all converge
+    # to 100% accuracy models
+    assert grid_search.best_score_ == 1.0
+    best_vectorizer = grid_search.best_estimator_.named_steps['vect']
+    assert best_vectorizer.ngram_range == (1, 1)
+    assert best_vectorizer.norm == 'l2'
+    assert not best_vectorizer.fixed_vocabulary_
+
+
+def test_vectorizer_pipeline_cross_validation():
+    # raw documents
+    data = JUNK_FOOD_DOCS + NOTJUNK_FOOD_DOCS
+
+    # label junk food as -1, the others as +1
+    target = [-1] * len(JUNK_FOOD_DOCS) + [1] * len(NOTJUNK_FOOD_DOCS)
+
+    pipeline = Pipeline([('vect', TfidfVectorizer()),
+                         ('svc', LinearSVC())])
+
+    cv_scores = cross_val_score(pipeline, data, target, cv=3)
+    assert_array_equal(cv_scores, [1., 1., 1.])
+
+
+@fails_if_pypy
+def test_vectorizer_unicode():
+    # tests that the count vectorizer works with cyrillic.
+    document = (
+        "Машинное обучение — обширный подраздел искусственного "
+        "интеллекта, изучающий методы построения алгоритмов, "
+        "способных обучаться."
+        )
+
+    vect = CountVectorizer()
+    X_counted = vect.fit_transform([document])
+    assert X_counted.shape == (1, 12)
+
+    vect = HashingVectorizer(norm=None, alternate_sign=False)
+    X_hashed = vect.transform([document])
+    assert X_hashed.shape == (1, 2 ** 20)
+
+    # No collisions on such a small dataset
+    assert X_counted.nnz == X_hashed.nnz
+
+    # When norm is None and not alternate_sign, the tokens are counted up to
+    # collisions
+    assert_array_equal(np.sort(X_counted.data), np.sort(X_hashed.data))
+
+
+def test_tfidf_vectorizer_with_fixed_vocabulary():
+    # non regression smoke test for inheritance issues
+    vocabulary = ['pizza', 'celeri']
+    vect = TfidfVectorizer(vocabulary=vocabulary)
+    X_1 = vect.fit_transform(ALL_FOOD_DOCS)
+    X_2 = vect.transform(ALL_FOOD_DOCS)
+    assert_array_almost_equal(X_1.toarray(), X_2.toarray())
+    assert vect.fixed_vocabulary_
+
+
+def test_pickling_vectorizer():
+    instances = [
+        HashingVectorizer(),
+        HashingVectorizer(norm='l1'),
+        HashingVectorizer(binary=True),
+        HashingVectorizer(ngram_range=(1, 2)),
+        CountVectorizer(),
+        CountVectorizer(preprocessor=strip_tags),
+        CountVectorizer(analyzer=lazy_analyze),
+        CountVectorizer(preprocessor=strip_tags).fit(JUNK_FOOD_DOCS),
+        CountVectorizer(strip_accents=strip_eacute).fit(JUNK_FOOD_DOCS),
+        TfidfVectorizer(),
+        TfidfVectorizer(analyzer=lazy_analyze),
+        TfidfVectorizer().fit(JUNK_FOOD_DOCS),
+    ]
+
+    for orig in instances:
+        s = pickle.dumps(orig)
+        copy = pickle.loads(s)
+        assert type(copy) == orig.__class__
+        assert copy.get_params() == orig.get_params()
+        if IS_PYPY and isinstance(orig, HashingVectorizer):
+            continue
+        else:
+            assert_array_equal(
+                copy.fit_transform(JUNK_FOOD_DOCS).toarray(),
+                orig.fit_transform(JUNK_FOOD_DOCS).toarray())
+
+
+@pytest.mark.parametrize('factory', [
+    CountVectorizer.build_analyzer,
+    CountVectorizer.build_preprocessor,
+    CountVectorizer.build_tokenizer,
+])
+def test_pickling_built_processors(factory):
+    """Tokenizers cannot be pickled
+    https://github.com/scikit-learn/scikit-learn/issues/12833
+    """
+    vec = CountVectorizer()
+    function = factory(vec)
+    text = ("J'ai mangé du kangourou  ce midi, "
+            "c'était pas très bon.")
+    roundtripped_function = pickle.loads(pickle.dumps(function))
+    expected = function(text)
+    result = roundtripped_function(text)
+    assert result == expected
+
+
+def test_countvectorizer_vocab_sets_when_pickling():
+    # ensure that vocabulary of type set is coerced to a list to
+    # preserve iteration ordering after deserialization
+    rng = np.random.RandomState(0)
+    vocab_words = np.array(['beer', 'burger', 'celeri', 'coke', 'pizza',
+                            'salad', 'sparkling', 'tomato', 'water'])
+    for x in range(0, 100):
+        vocab_set = set(rng.choice(vocab_words, size=5, replace=False))
+        cv = CountVectorizer(vocabulary=vocab_set)
+        unpickled_cv = pickle.loads(pickle.dumps(cv))
+        cv.fit(ALL_FOOD_DOCS)
+        unpickled_cv.fit(ALL_FOOD_DOCS)
+        assert cv.get_feature_names() == unpickled_cv.get_feature_names()
+
+
+def test_countvectorizer_vocab_dicts_when_pickling():
+    rng = np.random.RandomState(0)
+    vocab_words = np.array(['beer', 'burger', 'celeri', 'coke', 'pizza',
+                            'salad', 'sparkling', 'tomato', 'water'])
+    for x in range(0, 100):
+        vocab_dict = dict()
+        words = rng.choice(vocab_words, size=5, replace=False)
+        for y in range(0, 5):
+            vocab_dict[words[y]] = y
+        cv = CountVectorizer(vocabulary=vocab_dict)
+        unpickled_cv = pickle.loads(pickle.dumps(cv))
+        cv.fit(ALL_FOOD_DOCS)
+        unpickled_cv.fit(ALL_FOOD_DOCS)
+        assert cv.get_feature_names() == unpickled_cv.get_feature_names()
+
+
+def test_stop_words_removal():
+    # Ensure that deleting the stop_words_ attribute doesn't affect transform
+
+    fitted_vectorizers = (
+        TfidfVectorizer().fit(JUNK_FOOD_DOCS),
+        CountVectorizer(preprocessor=strip_tags).fit(JUNK_FOOD_DOCS),
+        CountVectorizer(strip_accents=strip_eacute).fit(JUNK_FOOD_DOCS)
+    )
+
+    for vect in fitted_vectorizers:
+        vect_transform = vect.transform(JUNK_FOOD_DOCS).toarray()
+
+        vect.stop_words_ = None
+        stop_None_transform = vect.transform(JUNK_FOOD_DOCS).toarray()
+
+        delattr(vect, 'stop_words_')
+        stop_del_transform = vect.transform(JUNK_FOOD_DOCS).toarray()
+
+        assert_array_equal(stop_None_transform, vect_transform)
+        assert_array_equal(stop_del_transform, vect_transform)
+
+
+def test_pickling_transformer():
+    X = CountVectorizer().fit_transform(JUNK_FOOD_DOCS)
+    orig = TfidfTransformer().fit(X)
+    s = pickle.dumps(orig)
+    copy = pickle.loads(s)
+    assert type(copy) == orig.__class__
+    assert_array_equal(
+        copy.fit_transform(X).toarray(),
+        orig.fit_transform(X).toarray())
+
+
+def test_transformer_idf_setter():
+    X = CountVectorizer().fit_transform(JUNK_FOOD_DOCS)
+    orig = TfidfTransformer().fit(X)
+    copy = TfidfTransformer()
+    copy.idf_ = orig.idf_
+    assert_array_equal(
+        copy.transform(X).toarray(),
+        orig.transform(X).toarray())
+
+
+def test_tfidf_vectorizer_setter():
+    orig = TfidfVectorizer(use_idf=True)
+    orig.fit(JUNK_FOOD_DOCS)
+    copy = TfidfVectorizer(vocabulary=orig.vocabulary_, use_idf=True)
+    copy.idf_ = orig.idf_
+    assert_array_equal(
+        copy.transform(JUNK_FOOD_DOCS).toarray(),
+        orig.transform(JUNK_FOOD_DOCS).toarray())
+
+
+def test_tfidfvectorizer_invalid_idf_attr():
+    vect = TfidfVectorizer(use_idf=True)
+    vect.fit(JUNK_FOOD_DOCS)
+    copy = TfidfVectorizer(vocabulary=vect.vocabulary_, use_idf=True)
+    expected_idf_len = len(vect.idf_)
+    invalid_idf = [1.0] * (expected_idf_len + 1)
+    with pytest.raises(ValueError):
+        setattr(copy, 'idf_', invalid_idf)
+
+
+def test_non_unique_vocab():
+    vocab = ['a', 'b', 'c', 'a', 'a']
+    vect = CountVectorizer(vocabulary=vocab)
+    with pytest.raises(ValueError):
+        vect.fit([])
+
+
+@fails_if_pypy
+def test_hashingvectorizer_nan_in_docs():
+    # np.nan can appear when using pandas to load text fields from a csv file
+    # with missing values.
+    message = "np.nan is an invalid document, expected byte or unicode string."
+    exception = ValueError
+
+    def func():
+        hv = HashingVectorizer()
+        hv.fit_transform(['hello world', np.nan, 'hello hello'])
+
+    assert_raise_message(exception, message, func)
+
+
+def test_tfidfvectorizer_binary():
+    # Non-regression test: TfidfVectorizer used to ignore its "binary" param.
+    v = TfidfVectorizer(binary=True, use_idf=False, norm=None)
+    assert v.binary
+
+    X = v.fit_transform(['hello world', 'hello hello']).toarray()
+    assert_array_equal(X.ravel(), [1, 1, 1, 0])
+    X2 = v.transform(['hello world', 'hello hello']).toarray()
+    assert_array_equal(X2.ravel(), [1, 1, 1, 0])
+
+
+def test_tfidfvectorizer_export_idf():
+    vect = TfidfVectorizer(use_idf=True)
+    vect.fit(JUNK_FOOD_DOCS)
+    assert_array_almost_equal(vect.idf_, vect._tfidf.idf_)
+
+
+def test_vectorizer_vocab_clone():
+    vect_vocab = TfidfVectorizer(vocabulary=["the"])
+    vect_vocab_clone = clone(vect_vocab)
+    vect_vocab.fit(ALL_FOOD_DOCS)
+    vect_vocab_clone.fit(ALL_FOOD_DOCS)
+    assert vect_vocab_clone.vocabulary_ == vect_vocab.vocabulary_
+
+
+@pytest.mark.parametrize('Vectorizer',
+                         (CountVectorizer, TfidfVectorizer, HashingVectorizer))
+def test_vectorizer_string_object_as_input(Vectorizer):
+    message = ("Iterable over raw text documents expected, "
+               "string object received.")
+    vec = Vectorizer()
+    assert_raise_message(
+            ValueError, message, vec.fit_transform, "hello world!")
+    assert_raise_message(ValueError, message, vec.fit, "hello world!")
+    assert_raise_message(ValueError, message, vec.transform, "hello world!")
+
+
+@pytest.mark.parametrize("X_dtype", [np.float32, np.float64])
+def test_tfidf_transformer_type(X_dtype):
+    X = sparse.rand(10, 20000, dtype=X_dtype, random_state=42)
+    X_trans = TfidfTransformer().fit_transform(X)
+    assert X_trans.dtype == X.dtype
+
+
+def test_tfidf_transformer_sparse():
+    X = sparse.rand(10, 20000, dtype=np.float64, random_state=42)
+    X_csc = sparse.csc_matrix(X)
+    X_csr = sparse.csr_matrix(X)
+
+    X_trans_csc = TfidfTransformer().fit_transform(X_csc)
+    X_trans_csr = TfidfTransformer().fit_transform(X_csr)
+    assert_allclose_dense_sparse(X_trans_csc, X_trans_csr)
+    assert X_trans_csc.format == X_trans_csr.format
+
+
+@pytest.mark.parametrize(
+    "vectorizer_dtype, output_dtype, warning_expected",
+    [(np.int32, np.float64, True),
+     (np.int64, np.float64, True),
+     (np.float32, np.float32, False),
+     (np.float64, np.float64, False)]
+)
+def test_tfidf_vectorizer_type(vectorizer_dtype, output_dtype,
+                               warning_expected):
+    X = np.array(["numpy", "scipy", "sklearn"])
+    vectorizer = TfidfVectorizer(dtype=vectorizer_dtype)
+
+    warning_msg_match = "'dtype' should be used."
+    warning_cls = UserWarning
+    expected_warning_cls = warning_cls if warning_expected else None
+    with pytest.warns(expected_warning_cls,
+                      match=warning_msg_match) as record:
+        X_idf = vectorizer.fit_transform(X)
+    if expected_warning_cls is None:
+        relevant_warnings = [w for w in record
+                             if isinstance(w, warning_cls)]
+        assert len(relevant_warnings) == 0
+    assert X_idf.dtype == output_dtype
+
+
+@pytest.mark.parametrize("vec", [
+        HashingVectorizer(ngram_range=(2, 1)),
+        CountVectorizer(ngram_range=(2, 1)),
+        TfidfVectorizer(ngram_range=(2, 1))
+    ])
+def test_vectorizers_invalid_ngram_range(vec):
+    # vectorizers could be initialized with invalid ngram range
+    # test for raising error message
+    invalid_range = vec.ngram_range
+    message = ("Invalid value for ngram_range=%s "
+               "lower boundary larger than the upper boundary."
+               % str(invalid_range))
+    if isinstance(vec, HashingVectorizer):
+        pytest.xfail(reason='HashingVectorizer is not supported on PyPy')
+
+    assert_raise_message(
+        ValueError, message, vec.fit, ["good news everyone"])
+    assert_raise_message(
+        ValueError, message, vec.fit_transform, ["good news everyone"])
+
+    if isinstance(vec, HashingVectorizer):
+        assert_raise_message(
+            ValueError, message, vec.transform, ["good news everyone"])
+
+
+def _check_stop_words_consistency(estimator):
+    stop_words = estimator.get_stop_words()
+    tokenize = estimator.build_tokenizer()
+    preprocess = estimator.build_preprocessor()
+    return estimator._check_stop_words_consistency(stop_words, preprocess,
+                                                   tokenize)
+
+
+@fails_if_pypy
+def test_vectorizer_stop_words_inconsistent():
+    lstr = "['and', 'll', 've']"
+    message = ('Your stop_words may be inconsistent with your '
+               'preprocessing. Tokenizing the stop words generated '
+               'tokens %s not in stop_words.' % lstr)
+    for vec in [CountVectorizer(),
+                TfidfVectorizer(), HashingVectorizer()]:
+        vec.set_params(stop_words=["you've", "you", "you'll", 'AND'])
+        assert_warns_message(UserWarning, message, vec.fit_transform,
+                             ['hello world'])
+        # reset stop word validation
+        del vec._stop_words_id
+        assert _check_stop_words_consistency(vec) is False
+
+    # Only one warning per stop list
+    assert_no_warnings(vec.fit_transform, ['hello world'])
+    assert _check_stop_words_consistency(vec) is None
+
+    # Test caching of inconsistency assessment
+    vec.set_params(stop_words=["you've", "you", "you'll", 'blah', 'AND'])
+    assert_warns_message(UserWarning, message, vec.fit_transform,
+                         ['hello world'])
+
+
+@skip_if_32bit
+def test_countvectorizer_sort_features_64bit_sparse_indices():
+    """
+    Check that CountVectorizer._sort_features preserves the dtype of its sparse
+    feature matrix.
+
+    This test is skipped on 32bit platforms, see:
+        https://github.com/scikit-learn/scikit-learn/pull/11295
+    for more details.
+    """
+
+    X = sparse.csr_matrix((5, 5), dtype=np.int64)
+
+    # force indices and indptr to int64.
+    INDICES_DTYPE = np.int64
+    X.indices = X.indices.astype(INDICES_DTYPE)
+    X.indptr = X.indptr.astype(INDICES_DTYPE)
+
+    vocabulary = {
+            "scikit-learn": 0,
+            "is": 1,
+            "great!": 2
+            }
+
+    Xs = CountVectorizer()._sort_features(X, vocabulary)
+
+    assert INDICES_DTYPE == Xs.indices.dtype
+
+
+@fails_if_pypy
+@pytest.mark.parametrize('Estimator',
+                         [CountVectorizer, TfidfVectorizer, HashingVectorizer])
+def test_stop_word_validation_custom_preprocessor(Estimator):
+    data = [{'text': 'some text'}]
+
+    vec = Estimator()
+    assert _check_stop_words_consistency(vec) is True
+
+    vec = Estimator(preprocessor=lambda x: x['text'],
+                    stop_words=['and'])
+    assert _check_stop_words_consistency(vec) == 'error'
+    # checks are cached
+    assert _check_stop_words_consistency(vec) is None
+    vec.fit_transform(data)
+
+    class CustomEstimator(Estimator):
+        def build_preprocessor(self):
+            return lambda x: x['text']
+
+    vec = CustomEstimator(stop_words=['and'])
+    assert _check_stop_words_consistency(vec) == 'error'
+
+    vec = Estimator(tokenizer=lambda doc: re.compile(r'\w{1,}')
+                                            .findall(doc),
+                    stop_words=['and'])
+    assert _check_stop_words_consistency(vec) is True
+
+
+@pytest.mark.parametrize(
+    'Estimator',
+    [CountVectorizer,
+     TfidfVectorizer,
+     HashingVectorizer]
+)
+@pytest.mark.parametrize(
+    'input_type, err_type, err_msg',
+    [('filename', FileNotFoundError, ''),
+     ('file', AttributeError, "'str' object has no attribute 'read'")]
+)
+def test_callable_analyzer_error(Estimator, input_type, err_type, err_msg):
+    if issubclass(Estimator, HashingVectorizer):
+        pytest.xfail('HashingVectorizer is not supported on PyPy')
+    data = ['this is text, not file or filename']
+    with pytest.raises(err_type, match=err_msg):
+        Estimator(analyzer=lambda x: x.split(),
+                  input=input_type).fit_transform(data)
+
+
+@pytest.mark.parametrize(
+    'Estimator',
+    [CountVectorizer,
+     TfidfVectorizer,
+     pytest.param(HashingVectorizer, marks=fails_if_pypy)]
+)
+@pytest.mark.parametrize(
+    'analyzer', [lambda doc: open(doc, 'r'), lambda doc: doc.read()]
+)
+@pytest.mark.parametrize('input_type', ['file', 'filename'])
+def test_callable_analyzer_change_behavior(Estimator, analyzer, input_type):
+    data = ['this is text, not file or filename']
+    warn_msg = 'Since v0.21, vectorizer'
+    with pytest.raises((FileNotFoundError, AttributeError)):
+        with pytest.warns(ChangedBehaviorWarning, match=warn_msg) as records:
+            Estimator(analyzer=analyzer, input=input_type).fit_transform(data)
+    assert len(records) == 1
+    assert warn_msg in str(records[0])
+
+
+@pytest.mark.parametrize(
+    'Estimator',
+    [CountVectorizer,
+     TfidfVectorizer,
+     HashingVectorizer]
+)
+def test_callable_analyzer_reraise_error(tmpdir, Estimator):
+    # check if a custom exception from the analyzer is shown to the user
+    def analyzer(doc):
+        raise Exception("testing")
+
+    if issubclass(Estimator, HashingVectorizer):
+        pytest.xfail('HashingVectorizer is not supported on PyPy')
+
+    f = tmpdir.join("file.txt")
+    f.write("sample content\n")
+
+    with pytest.raises(Exception, match="testing"):
+        Estimator(analyzer=analyzer, input='file').fit_transform([f])
+
+
+@pytest.mark.parametrize(
+    'Vectorizer',
+    [CountVectorizer, HashingVectorizer, TfidfVectorizer]
+)
+@pytest.mark.parametrize(
+    'stop_words, tokenizer, preprocessor, ngram_range, token_pattern,'
+    'analyzer, unused_name, ovrd_name, ovrd_msg',
+    [(["you've", "you'll"], None, None, (1, 1), None, 'char',
+     "'stop_words'", "'analyzer'", "!= 'word'"),
+     (None, lambda s: s.split(), None, (1, 1), None, 'char',
+     "'tokenizer'", "'analyzer'", "!= 'word'"),
+     (None, lambda s: s.split(), None, (1, 1), r'\w+', 'word',
+      "'token_pattern'", "'tokenizer'", "is not None"),
+     (None, None, lambda s:s.upper(), (1, 1), r'\w+', lambda s:s.upper(),
+      "'preprocessor'", "'analyzer'", "is callable"),
+     (None, None, None, (1, 2), None, lambda s:s.upper(),
+      "'ngram_range'", "'analyzer'", "is callable"),
+     (None, None, None, (1, 1), r'\w+', 'char',
+      "'token_pattern'", "'analyzer'", "!= 'word'")]
+)
+def test_unused_parameters_warn(Vectorizer, stop_words,
+                                tokenizer, preprocessor,
+                                ngram_range, token_pattern,
+                                analyzer, unused_name, ovrd_name,
+                                ovrd_msg):
+
+    train_data = JUNK_FOOD_DOCS
+    # setting parameter and checking for corresponding warning messages
+    vect = Vectorizer()
+    vect.set_params(stop_words=stop_words, tokenizer=tokenizer,
+                    preprocessor=preprocessor, ngram_range=ngram_range,
+                    token_pattern=token_pattern, analyzer=analyzer)
+    msg = ("The parameter %s will not be used"
+           " since %s %s" % (unused_name, ovrd_name, ovrd_msg)
+           )
+    with pytest.warns(UserWarning, match=msg):
+        vect.fit(train_data)
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index bb5a9d646..11236d958 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -130,10 +130,7 @@ def strip_accents_unicode(s):
         ASCII equivalent.
     """
     normalized = unicodedata.normalize('NFKD', s)
-    if normalized == s:
-        return s
-    else:
-        return ''.join([c for c in normalized if not unicodedata.combining(c)])
+    return ''.join([c for c in normalized if not unicodedata.combining(c)])
 
 
 def strip_accents_ascii(s):
diff --git a/sklearn/feature_extraction/text.py.backup b/sklearn/feature_extraction/text.py.backup
new file mode 100644
index 000000000..bb5a9d646
--- /dev/null
+++ b/sklearn/feature_extraction/text.py.backup
@@ -0,0 +1,1811 @@
+# -*- coding: utf-8 -*-
+# Authors: Olivier Grisel <olivier.grisel@ensta.org>
+#          Mathieu Blondel <mathieu@mblondel.org>
+#          Lars Buitinck
+#          Robert Layton <robertlayton@gmail.com>
+#          Jochen Wersdörfer <jochen@wersdoerfer.de>
+#          Roman Sinayev <roman.sinayev@gmail.com>
+#
+# License: BSD 3 clause
+"""
+The :mod:`sklearn.feature_extraction.text` submodule gathers utilities to
+build feature vectors from text documents.
+"""
+
+import array
+from collections import defaultdict
+from collections.abc import Mapping
+from functools import partial
+import numbers
+from operator import itemgetter
+import re
+import unicodedata
+import warnings
+
+import numpy as np
+import scipy.sparse as sp
+
+from ..base import BaseEstimator, TransformerMixin
+from ..preprocessing import normalize
+from .hashing import FeatureHasher
+from .stop_words import ENGLISH_STOP_WORDS
+from ..utils.validation import check_is_fitted, check_array, FLOAT_DTYPES
+from ..utils import _IS_32BIT
+from ..utils.fixes import _astype_copy_false
+from ..exceptions import ChangedBehaviorWarning, NotFittedError
+
+
+__all__ = ['HashingVectorizer',
+           'CountVectorizer',
+           'ENGLISH_STOP_WORDS',
+           'TfidfTransformer',
+           'TfidfVectorizer',
+           'strip_accents_ascii',
+           'strip_accents_unicode',
+           'strip_tags']
+
+
+def _preprocess(doc, accent_function=None, lower=False):
+    """Chain together an optional series of text preprocessing steps to
+    apply to a document.
+
+    Parameters
+    ----------
+    doc: str
+        The string to preprocess
+    accent_function: callable
+        Function for handling accented characters. Common strategies include
+        normalizing and removing.
+    lower: bool
+        Whether to use str.lower to lowercase all fo the text
+
+    Returns
+    -------
+    doc: str
+        preprocessed string
+    """
+    if lower:
+        doc = doc.lower()
+    if accent_function is not None:
+        doc = accent_function(doc)
+    return doc
+
+
+def _analyze(doc, analyzer=None, tokenizer=None, ngrams=None,
+             preprocessor=None, decoder=None, stop_words=None):
+    """Chain together an optional series of text processing steps to go from
+    a single document to ngrams, with or without tokenizing or preprocessing.
+
+    If analyzer is used, only the decoder argument is used, as the analyzer is
+    intended to replace the preprocessor, tokenizer, and ngrams steps.
+
+    Parameters
+    ----------
+    analyzer: callable
+    tokenizer: callable
+    ngrams: callable
+    preprocessor: callable
+    decoder: callable
+    stop_words: list
+
+    Returns
+    -------
+    ngrams: list
+        A sequence of tokens, possibly with pairs, triples, etc.
+    """
+
+    if decoder is not None:
+        doc = decoder(doc)
+    if analyzer is not None:
+        doc = analyzer(doc)
+    else:
+        if preprocessor is not None:
+            doc = preprocessor(doc)
+        if tokenizer is not None:
+            doc = tokenizer(doc)
+        if ngrams is not None:
+            if stop_words is not None:
+                doc = ngrams(doc, stop_words)
+            else:
+                doc = ngrams(doc)
+    return doc
+
+
+def strip_accents_unicode(s):
+    """Transform accentuated unicode symbols into their simple counterpart
+
+    Warning: the python-level loop and join operations make this
+    implementation 20 times slower than the strip_accents_ascii basic
+    normalization.
+
+    Parameters
+    ----------
+    s : string
+        The string to strip
+
+    See also
+    --------
+    strip_accents_ascii
+        Remove accentuated char for any unicode symbol that has a direct
+        ASCII equivalent.
+    """
+    normalized = unicodedata.normalize('NFKD', s)
+    if normalized == s:
+        return s
+    else:
+        return ''.join([c for c in normalized if not unicodedata.combining(c)])
+
+
+def strip_accents_ascii(s):
+    """Transform accentuated unicode symbols into ascii or nothing
+
+    Warning: this solution is only suited for languages that have a direct
+    transliteration to ASCII symbols.
+
+    Parameters
+    ----------
+    s : string
+        The string to strip
+
+    See also
+    --------
+    strip_accents_unicode
+        Remove accentuated char for any unicode symbol.
+    """
+    nkfd_form = unicodedata.normalize('NFKD', s)
+    return nkfd_form.encode('ASCII', 'ignore').decode('ASCII')
+
+
+def strip_tags(s):
+    """Basic regexp based HTML / XML tag stripper function
+
+    For serious HTML/XML preprocessing you should rather use an external
+    library such as lxml or BeautifulSoup.
+
+    Parameters
+    ----------
+    s : string
+        The string to strip
+    """
+    return re.compile(r"<([^>]+)>", flags=re.UNICODE).sub(" ", s)
+
+
+def _check_stop_list(stop):
+    if stop == "english":
+        return ENGLISH_STOP_WORDS
+    elif isinstance(stop, str):
+        raise ValueError("not a built-in stop list: %s" % stop)
+    elif stop is None:
+        return None
+    else:  # assume it's a collection
+        return frozenset(stop)
+
+
+class VectorizerMixin:
+    """Provides common code for text vectorizers (tokenization logic)."""
+
+    _white_spaces = re.compile(r"\s\s+")
+
+    def decode(self, doc):
+        """Decode the input into a string of unicode symbols
+
+        The decoding strategy depends on the vectorizer parameters.
+
+        Parameters
+        ----------
+        doc : string
+            The string to decode
+        """
+        if self.input == 'filename':
+            with open(doc, 'rb') as fh:
+                doc = fh.read()
+
+        elif self.input == 'file':
+            doc = doc.read()
+
+        if isinstance(doc, bytes):
+            doc = doc.decode(self.encoding, self.decode_error)
+
+        if doc is np.nan:
+            raise ValueError("np.nan is an invalid document, expected byte or "
+                             "unicode string.")
+
+        return doc
+
+    def _word_ngrams(self, tokens, stop_words=None):
+        """Turn tokens into a sequence of n-grams after stop words filtering"""
+        # handle stop words
+        if stop_words is not None:
+            tokens = [w for w in tokens if w not in stop_words]
+
+        # handle token n-grams
+        min_n, max_n = self.ngram_range
+        if max_n != 1:
+            original_tokens = tokens
+            if min_n == 1:
+                # no need to do any slicing for unigrams
+                # just iterate through the original tokens
+                tokens = list(original_tokens)
+                min_n += 1
+            else:
+                tokens = []
+
+            n_original_tokens = len(original_tokens)
+
+            # bind method outside of loop to reduce overhead
+            tokens_append = tokens.append
+            space_join = " ".join
+
+            for n in range(min_n,
+                           min(max_n + 1, n_original_tokens + 1)):
+                for i in range(n_original_tokens - n + 1):
+                    tokens_append(space_join(original_tokens[i: i + n]))
+
+        return tokens
+
+    def _char_ngrams(self, text_document):
+        """Tokenize text_document into a sequence of character n-grams"""
+        # normalize white spaces
+        text_document = self._white_spaces.sub(" ", text_document)
+
+        text_len = len(text_document)
+        min_n, max_n = self.ngram_range
+        if min_n == 1:
+            # no need to do any slicing for unigrams
+            # iterate through the string
+            ngrams = list(text_document)
+            min_n += 1
+        else:
+            ngrams = []
+
+        # bind method outside of loop to reduce overhead
+        ngrams_append = ngrams.append
+
+        for n in range(min_n, min(max_n + 1, text_len + 1)):
+            for i in range(text_len - n + 1):
+                ngrams_append(text_document[i: i + n])
+        return ngrams
+
+    def _char_wb_ngrams(self, text_document):
+        """Whitespace sensitive char-n-gram tokenization.
+
+        Tokenize text_document into a sequence of character n-grams
+        operating only inside word boundaries. n-grams at the edges
+        of words are padded with space."""
+        # normalize white spaces
+        text_document = self._white_spaces.sub(" ", text_document)
+
+        min_n, max_n = self.ngram_range
+        ngrams = []
+
+        # bind method outside of loop to reduce overhead
+        ngrams_append = ngrams.append
+
+        for w in text_document.split():
+            w = ' ' + w + ' '
+            w_len = len(w)
+            for n in range(min_n, max_n + 1):
+                offset = 0
+                ngrams_append(w[offset:offset + n])
+                while offset + n < w_len:
+                    offset += 1
+                    ngrams_append(w[offset:offset + n])
+                if offset == 0:   # count a short word (w_len < n) only once
+                    break
+        return ngrams
+
+    def build_preprocessor(self):
+        """Return a function to preprocess the text before tokenization"""
+        if self.preprocessor is not None:
+            return self.preprocessor
+
+        # accent stripping
+        if not self.strip_accents:
+            strip_accents = None
+        elif callable(self.strip_accents):
+            strip_accents = self.strip_accents
+        elif self.strip_accents == 'ascii':
+            strip_accents = strip_accents_ascii
+        elif self.strip_accents == 'unicode':
+            strip_accents = strip_accents_unicode
+        else:
+            raise ValueError('Invalid value for "strip_accents": %s' %
+                             self.strip_accents)
+
+        return partial(
+            _preprocess, accent_function=strip_accents, lower=self.lowercase
+        )
+
+    def build_tokenizer(self):
+        """Return a function that splits a string into a sequence of tokens"""
+        if self.tokenizer is not None:
+            return self.tokenizer
+        token_pattern = re.compile(self.token_pattern)
+        return token_pattern.findall
+
+    def get_stop_words(self):
+        """Build or fetch the effective stop words list"""
+        return _check_stop_list(self.stop_words)
+
+    def _check_stop_words_consistency(self, stop_words, preprocess, tokenize):
+        """Check if stop words are consistent
+
+        Returns
+        -------
+        is_consistent : True if stop words are consistent with the preprocessor
+                        and tokenizer, False if they are not, None if the check
+                        was previously performed, "error" if it could not be
+                        performed (e.g. because of the use of a custom
+                        preprocessor / tokenizer)
+        """
+        if id(self.stop_words) == getattr(self, '_stop_words_id', None):
+            # Stop words are were previously validated
+            return None
+
+        # NB: stop_words is validated, unlike self.stop_words
+        try:
+            inconsistent = set()
+            for w in stop_words or ():
+                tokens = list(tokenize(preprocess(w)))
+                for token in tokens:
+                    if token not in stop_words:
+                        inconsistent.add(token)
+            self._stop_words_id = id(self.stop_words)
+
+            if inconsistent:
+                warnings.warn('Your stop_words may be inconsistent with '
+                              'your preprocessing. Tokenizing the stop '
+                              'words generated tokens %r not in '
+                              'stop_words.' % sorted(inconsistent))
+            return not inconsistent
+        except Exception:
+            # Failed to check stop words consistency (e.g. because a custom
+            # preprocessor or tokenizer was used)
+            self._stop_words_id = id(self.stop_words)
+            return 'error'
+
+    def _validate_custom_analyzer(self):
+        # This is to check if the given custom analyzer expects file or a
+        # filename instead of data.
+        # Behavior changed in v0.21, function could be removed in v0.23
+        import tempfile
+        with tempfile.NamedTemporaryFile() as f:
+            fname = f.name
+        # now we're sure fname doesn't exist
+
+        msg = ("Since v0.21, vectorizers pass the data to the custom analyzer "
+               "and not the file names or the file objects. This warning "
+               "will be removed in v0.23.")
+        try:
+            self.analyzer(fname)
+        except FileNotFoundError:
+            warnings.warn(msg, ChangedBehaviorWarning)
+        except AttributeError as e:
+            if str(e) == "'str' object has no attribute 'read'":
+                warnings.warn(msg, ChangedBehaviorWarning)
+        except Exception:
+            pass
+
+    def build_analyzer(self):
+        """Return a callable that handles preprocessing, tokenization
+
+        and n-grams generation.
+        """
+
+        if callable(self.analyzer):
+            if self.input in ['file', 'filename']:
+                self._validate_custom_analyzer()
+            return partial(
+                _analyze, analyzer=self.analyzer, decoder=self.decode
+            )
+
+        preprocess = self.build_preprocessor()
+
+        if self.analyzer == 'char':
+            return partial(_analyze, ngrams=self._char_ngrams,
+                           preprocessor=preprocess, decoder=self.decode)
+
+        elif self.analyzer == 'char_wb':
+
+            return partial(_analyze, ngrams=self._char_wb_ngrams,
+                           preprocessor=preprocess, decoder=self.decode)
+
+        elif self.analyzer == 'word':
+            stop_words = self.get_stop_words()
+            tokenize = self.build_tokenizer()
+            self._check_stop_words_consistency(stop_words, preprocess,
+                                               tokenize)
+            return partial(_analyze, ngrams=self._word_ngrams,
+                           tokenizer=tokenize, preprocessor=preprocess,
+                           decoder=self.decode, stop_words=stop_words)
+
+        else:
+            raise ValueError('%s is not a valid tokenization scheme/analyzer' %
+                             self.analyzer)
+
+    def _validate_vocabulary(self):
+        vocabulary = self.vocabulary
+        if vocabulary is not None:
+            if isinstance(vocabulary, set):
+                vocabulary = sorted(vocabulary)
+            if not isinstance(vocabulary, Mapping):
+                vocab = {}
+                for i, t in enumerate(vocabulary):
+                    if vocab.setdefault(t, i) != i:
+                        msg = "Duplicate term in vocabulary: %r" % t
+                        raise ValueError(msg)
+                vocabulary = vocab
+            else:
+                indices = set(vocabulary.values())
+                if len(indices) != len(vocabulary):
+                    raise ValueError("Vocabulary contains repeated indices.")
+                for i in range(len(vocabulary)):
+                    if i not in indices:
+                        msg = ("Vocabulary of size %d doesn't contain index "
+                               "%d." % (len(vocabulary), i))
+                        raise ValueError(msg)
+            if not vocabulary:
+                raise ValueError("empty vocabulary passed to fit")
+            self.fixed_vocabulary_ = True
+            self.vocabulary_ = dict(vocabulary)
+        else:
+            self.fixed_vocabulary_ = False
+
+    def _check_vocabulary(self):
+        """Check if vocabulary is empty or missing (not fitted)"""
+        if not hasattr(self, 'vocabulary_'):
+            self._validate_vocabulary()
+            if not self.fixed_vocabulary_:
+                raise NotFittedError("Vocabulary not fitted or provided")
+
+        if len(self.vocabulary_) == 0:
+            raise ValueError("Vocabulary is empty")
+
+    def _validate_params(self):
+        """Check validity of ngram_range parameter"""
+        min_n, max_m = self.ngram_range
+        if min_n > max_m:
+            raise ValueError(
+                "Invalid value for ngram_range=%s "
+                "lower boundary larger than the upper boundary."
+                % str(self.ngram_range))
+
+    def _warn_for_unused_params(self):
+
+        if self.tokenizer is not None and self.token_pattern is not None:
+            warnings.warn("The parameter 'token_pattern' will not be used"
+                          " since 'tokenizer' is not None'")
+
+        if self.preprocessor is not None and callable(self.analyzer):
+            warnings.warn("The parameter 'preprocessor' will not be used"
+                          " since 'analyzer' is callable'")
+
+        if (self.ngram_range != (1, 1) and self.ngram_range is not None
+                and callable(self.analyzer)):
+            warnings.warn("The parameter 'ngram_range' will not be used"
+                          " since 'analyzer' is callable'")
+        if self.analyzer != 'word' or callable(self.analyzer):
+            if self.stop_words is not None:
+                warnings.warn("The parameter 'stop_words' will not be used"
+                              " since 'analyzer' != 'word'")
+            if self.token_pattern is not None and \
+               self.token_pattern != r"(?u)\b\w\w+\b":
+                warnings.warn("The parameter 'token_pattern' will not be used"
+                              " since 'analyzer' != 'word'")
+            if self.tokenizer is not None:
+                warnings.warn("The parameter 'tokenizer' will not be used"
+                              " since 'analyzer' != 'word'")
+
+
+class HashingVectorizer(TransformerMixin, VectorizerMixin, BaseEstimator):
+    """Convert a collection of text documents to a matrix of token occurrences
+
+    It turns a collection of text documents into a scipy.sparse matrix holding
+    token occurrence counts (or binary occurrence information), possibly
+    normalized as token frequencies if norm='l1' or projected on the euclidean
+    unit sphere if norm='l2'.
+
+    This text vectorizer implementation uses the hashing trick to find the
+    token string name to feature integer index mapping.
+
+    This strategy has several advantages:
+
+    - it is very low memory scalable to large datasets as there is no need to
+      store a vocabulary dictionary in memory
+
+    - it is fast to pickle and un-pickle as it holds no state besides the
+      constructor parameters
+
+    - it can be used in a streaming (partial fit) or parallel pipeline as there
+      is no state computed during fit.
+
+    There are also a couple of cons (vs using a CountVectorizer with an
+    in-memory vocabulary):
+
+    - there is no way to compute the inverse transform (from feature indices to
+      string feature names) which can be a problem when trying to introspect
+      which features are most important to a model.
+
+    - there can be collisions: distinct tokens can be mapped to the same
+      feature index. However in practice this is rarely an issue if n_features
+      is large enough (e.g. 2 ** 18 for text classification problems).
+
+    - no IDF weighting as this would render the transformer stateful.
+
+    The hash function employed is the signed 32-bit version of Murmurhash3.
+
+    Read more in the :ref:`User Guide <text_feature_extraction>`.
+
+    Parameters
+    ----------
+
+    input : string {'filename', 'file', 'content'}
+        If 'filename', the sequence passed as an argument to fit is
+        expected to be a list of filenames that need reading to fetch
+        the raw content to analyze.
+
+        If 'file', the sequence items must have a 'read' method (file-like
+        object) that is called to fetch the bytes in memory.
+
+        Otherwise the input is expected to be a sequence of items that
+        can be of type string or byte.
+
+    encoding : string, default='utf-8'
+        If bytes or files are given to analyze, this encoding is used to
+        decode.
+
+    decode_error : {'strict', 'ignore', 'replace'}
+        Instruction on what to do if a byte sequence is given to analyze that
+        contains characters not of the given `encoding`. By default, it is
+        'strict', meaning that a UnicodeDecodeError will be raised. Other
+        values are 'ignore' and 'replace'.
+
+    strip_accents : {'ascii', 'unicode', None}
+        Remove accents and perform other character normalization
+        during the preprocessing step.
+        'ascii' is a fast method that only works on characters that have
+        an direct ASCII mapping.
+        'unicode' is a slightly slower method that works on any characters.
+        None (default) does nothing.
+
+        Both 'ascii' and 'unicode' use NFKD normalization from
+        :func:`unicodedata.normalize`.
+
+    lowercase : boolean, default=True
+        Convert all characters to lowercase before tokenizing.
+
+    preprocessor : callable or None (default)
+        Override the preprocessing (string transformation) stage while
+        preserving the tokenizing and n-grams generation steps.
+        Only applies if ``analyzer is not callable``.
+
+    tokenizer : callable or None (default)
+        Override the string tokenization step while preserving the
+        preprocessing and n-grams generation steps.
+        Only applies if ``analyzer == 'word'``.
+
+    stop_words : string {'english'}, list, or None (default)
+        If 'english', a built-in stop word list for English is used.
+        There are several known issues with 'english' and you should
+        consider an alternative (see :ref:`stop_words`).
+
+        If a list, that list is assumed to contain stop words, all of which
+        will be removed from the resulting tokens.
+        Only applies if ``analyzer == 'word'``.
+
+    token_pattern : string
+        Regular expression denoting what constitutes a "token", only used
+        if ``analyzer == 'word'``. The default regexp selects tokens of 2
+        or more alphanumeric characters (punctuation is completely ignored
+        and always treated as a token separator).
+
+    ngram_range : tuple (min_n, max_n), default=(1, 1)
+        The lower and upper boundary of the range of n-values for different
+        n-grams to be extracted. All values of n such that min_n <= n <= max_n
+        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only
+        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means
+        only bigrams.
+        Only applies if ``analyzer is not callable``.
+
+    analyzer : string, {'word', 'char', 'char_wb'} or callable
+        Whether the feature should be made of word or character n-grams.
+        Option 'char_wb' creates character n-grams only from text inside
+        word boundaries; n-grams at the edges of words are padded with space.
+
+        If a callable is passed it is used to extract the sequence of features
+        out of the raw, unprocessed input.
+
+        .. versionchanged:: 0.21
+
+        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is
+        first read from the file and then passed to the given callable
+        analyzer.
+
+    n_features : integer, default=(2 ** 20)
+        The number of features (columns) in the output matrices. Small numbers
+        of features are likely to cause hash collisions, but large numbers
+        will cause larger coefficient dimensions in linear learners.
+
+    binary : boolean, default=False.
+        If True, all non zero counts are set to 1. This is useful for discrete
+        probabilistic models that model binary events rather than integer
+        counts.
+
+    norm : 'l1', 'l2' or None, optional
+        Norm used to normalize term vectors. None for no normalization.
+
+    alternate_sign : boolean, optional, default True
+        When True, an alternating sign is added to the features as to
+        approximately conserve the inner product in the hashed space even for
+        small n_features. This approach is similar to sparse random projection.
+
+        .. versionadded:: 0.19
+
+    dtype : type, optional
+        Type of the matrix returned by fit_transform() or transform().
+
+    Examples
+    --------
+    >>> from sklearn.feature_extraction.text import HashingVectorizer
+    >>> corpus = [
+    ...     'This is the first document.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
+    ...     'Is this the first document?',
+    ... ]
+    >>> vectorizer = HashingVectorizer(n_features=2**4)
+    >>> X = vectorizer.fit_transform(corpus)
+    >>> print(X.shape)
+    (4, 16)
+
+    See also
+    --------
+    CountVectorizer, TfidfVectorizer
+
+    """
+    def __init__(self, input='content', encoding='utf-8',
+                 decode_error='strict', strip_accents=None,
+                 lowercase=True, preprocessor=None, tokenizer=None,
+                 stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
+                 ngram_range=(1, 1), analyzer='word', n_features=(2 ** 20),
+                 binary=False, norm='l2', alternate_sign=True,
+                 dtype=np.float64):
+        self.input = input
+        self.encoding = encoding
+        self.decode_error = decode_error
+        self.strip_accents = strip_accents
+        self.preprocessor = preprocessor
+        self.tokenizer = tokenizer
+        self.analyzer = analyzer
+        self.lowercase = lowercase
+        self.token_pattern = token_pattern
+        self.stop_words = stop_words
+        self.n_features = n_features
+        self.ngram_range = ngram_range
+        self.binary = binary
+        self.norm = norm
+        self.alternate_sign = alternate_sign
+        self.dtype = dtype
+
+    def partial_fit(self, X, y=None):
+        """Does nothing: this transformer is stateless.
+
+        This method is just there to mark the fact that this transformer
+        can work in a streaming setup.
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_features]
+            Training data.
+        """
+        return self
+
+    def fit(self, X, y=None):
+        """Does nothing: this transformer is stateless.
+
+        Parameters
+        ----------
+        X : array-like, shape [n_samples, n_features]
+            Training data.
+        """
+        # triggers a parameter validation
+        if isinstance(X, str):
+            raise ValueError(
+                "Iterable over raw text documents expected, "
+                "string object received.")
+
+        self._warn_for_unused_params()
+        self._validate_params()
+
+        self._get_hasher().fit(X, y=y)
+        return self
+
+    def transform(self, X):
+        """Transform a sequence of documents to a document-term matrix.
+
+        Parameters
+        ----------
+        X : iterable over raw text documents, length = n_samples
+            Samples. Each sample must be a text document (either bytes or
+            unicode strings, file name or file object depending on the
+            constructor argument) which will be tokenized and hashed.
+
+        Returns
+        -------
+        X : scipy.sparse matrix, shape = (n_samples, self.n_features)
+            Document-term matrix.
+        """
+        if isinstance(X, str):
+            raise ValueError(
+                "Iterable over raw text documents expected, "
+                "string object received.")
+
+        self._validate_params()
+
+        analyzer = self.build_analyzer()
+        X = self._get_hasher().transform(analyzer(doc) for doc in X)
+        if self.binary:
+            X.data.fill(1)
+        if self.norm is not None:
+            X = normalize(X, norm=self.norm, copy=False)
+        return X
+
+    def fit_transform(self, X, y=None):
+        """Transform a sequence of documents to a document-term matrix.
+
+        Parameters
+        ----------
+        X : iterable over raw text documents, length = n_samples
+            Samples. Each sample must be a text document (either bytes or
+            unicode strings, file name or file object depending on the
+            constructor argument) which will be tokenized and hashed.
+        y : any
+            Ignored. This parameter exists only for compatibility with
+            sklearn.pipeline.Pipeline.
+
+        Returns
+        -------
+        X : scipy.sparse matrix, shape = (n_samples, self.n_features)
+            Document-term matrix.
+        """
+        return self.fit(X, y).transform(X)
+
+    def _get_hasher(self):
+        return FeatureHasher(n_features=self.n_features,
+                             input_type='string', dtype=self.dtype,
+                             alternate_sign=self.alternate_sign)
+
+    def _more_tags(self):
+        return {'X_types': ['string']}
+
+
+def _document_frequency(X):
+    """Count the number of non-zero values for each feature in sparse X."""
+    if sp.isspmatrix_csr(X):
+        return np.bincount(X.indices, minlength=X.shape[1])
+    else:
+        return np.diff(X.indptr)
+
+
+class CountVectorizer(VectorizerMixin, BaseEstimator):
+    """Convert a collection of text documents to a matrix of token counts
+
+    This implementation produces a sparse representation of the counts using
+    scipy.sparse.csr_matrix.
+
+    If you do not provide an a-priori dictionary and you do not use an analyzer
+    that does some kind of feature selection then the number of features will
+    be equal to the vocabulary size found by analyzing the data.
+
+    Read more in the :ref:`User Guide <text_feature_extraction>`.
+
+    Parameters
+    ----------
+    input : string {'filename', 'file', 'content'}
+        If 'filename', the sequence passed as an argument to fit is
+        expected to be a list of filenames that need reading to fetch
+        the raw content to analyze.
+
+        If 'file', the sequence items must have a 'read' method (file-like
+        object) that is called to fetch the bytes in memory.
+
+        Otherwise the input is expected to be a sequence of items that
+        can be of type string or byte.
+
+    encoding : string, 'utf-8' by default.
+        If bytes or files are given to analyze, this encoding is used to
+        decode.
+
+    decode_error : {'strict', 'ignore', 'replace'}
+        Instruction on what to do if a byte sequence is given to analyze that
+        contains characters not of the given `encoding`. By default, it is
+        'strict', meaning that a UnicodeDecodeError will be raised. Other
+        values are 'ignore' and 'replace'.
+
+    strip_accents : {'ascii', 'unicode', None}
+        Remove accents and perform other character normalization
+        during the preprocessing step.
+        'ascii' is a fast method that only works on characters that have
+        an direct ASCII mapping.
+        'unicode' is a slightly slower method that works on any characters.
+        None (default) does nothing.
+
+        Both 'ascii' and 'unicode' use NFKD normalization from
+        :func:`unicodedata.normalize`.
+
+    lowercase : boolean, True by default
+        Convert all characters to lowercase before tokenizing.
+
+    preprocessor : callable or None (default)
+        Override the preprocessing (string transformation) stage while
+        preserving the tokenizing and n-grams generation steps.
+        Only applies if ``analyzer is not callable``.
+
+    tokenizer : callable or None (default)
+        Override the string tokenization step while preserving the
+        preprocessing and n-grams generation steps.
+        Only applies if ``analyzer == 'word'``.
+
+    stop_words : string {'english'}, list, or None (default)
+        If 'english', a built-in stop word list for English is used.
+        There are several known issues with 'english' and you should
+        consider an alternative (see :ref:`stop_words`).
+
+        If a list, that list is assumed to contain stop words, all of which
+        will be removed from the resulting tokens.
+        Only applies if ``analyzer == 'word'``.
+
+        If None, no stop words will be used. max_df can be set to a value
+        in the range [0.7, 1.0) to automatically detect and filter stop
+        words based on intra corpus document frequency of terms.
+
+    token_pattern : string
+        Regular expression denoting what constitutes a "token", only used
+        if ``analyzer == 'word'``. The default regexp select tokens of 2
+        or more alphanumeric characters (punctuation is completely ignored
+        and always treated as a token separator).
+
+    ngram_range : tuple (min_n, max_n), default=(1, 1)
+        The lower and upper boundary of the range of n-values for different
+        n-grams to be extracted. All values of n such that min_n <= n <= max_n
+        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only
+        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means
+        only bigrams.
+        Only applies if ``analyzer is not callable``.
+
+    analyzer : string, {'word', 'char', 'char_wb'} or callable
+        Whether the feature should be made of word or character n-grams.
+        Option 'char_wb' creates character n-grams only from text inside
+        word boundaries; n-grams at the edges of words are padded with space.
+
+        If a callable is passed it is used to extract the sequence of features
+        out of the raw, unprocessed input.
+
+        .. versionchanged:: 0.21
+
+        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is
+        first read from the file and then passed to the given callable
+        analyzer.
+
+    max_df : float in range [0.0, 1.0] or int, default=1.0
+        When building the vocabulary ignore terms that have a document
+        frequency strictly higher than the given threshold (corpus-specific
+        stop words).
+        If float, the parameter represents a proportion of documents, integer
+        absolute counts.
+        This parameter is ignored if vocabulary is not None.
+
+    min_df : float in range [0.0, 1.0] or int, default=1
+        When building the vocabulary ignore terms that have a document
+        frequency strictly lower than the given threshold. This value is also
+        called cut-off in the literature.
+        If float, the parameter represents a proportion of documents, integer
+        absolute counts.
+        This parameter is ignored if vocabulary is not None.
+
+    max_features : int or None, default=None
+        If not None, build a vocabulary that only consider the top
+        max_features ordered by term frequency across the corpus.
+
+        This parameter is ignored if vocabulary is not None.
+
+    vocabulary : Mapping or iterable, optional
+        Either a Mapping (e.g., a dict) where keys are terms and values are
+        indices in the feature matrix, or an iterable over terms. If not
+        given, a vocabulary is determined from the input documents. Indices
+        in the mapping should not be repeated and should not have any gap
+        between 0 and the largest index.
+
+    binary : boolean, default=False
+        If True, all non zero counts are set to 1. This is useful for discrete
+        probabilistic models that model binary events rather than integer
+        counts.
+
+    dtype : type, optional
+        Type of the matrix returned by fit_transform() or transform().
+
+    Attributes
+    ----------
+    vocabulary_ : dict
+        A mapping of terms to feature indices.
+
+    fixed_vocabulary_: boolean
+        True if a fixed vocabulary of term to indices mapping
+        is provided by the user
+
+    stop_words_ : set
+        Terms that were ignored because they either:
+
+          - occurred in too many documents (`max_df`)
+          - occurred in too few documents (`min_df`)
+          - were cut off by feature selection (`max_features`).
+
+        This is only available if no vocabulary was given.
+
+    Examples
+    --------
+    >>> from sklearn.feature_extraction.text import CountVectorizer
+    >>> corpus = [
+    ...     'This is the first document.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
+    ...     'Is this the first document?',
+    ... ]
+    >>> vectorizer = CountVectorizer()
+    >>> X = vectorizer.fit_transform(corpus)
+    >>> print(vectorizer.get_feature_names())
+    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
+    >>> print(X.toarray())
+    [[0 1 1 1 0 0 1 0 1]
+     [0 2 0 1 0 1 1 0 1]
+     [1 0 0 1 1 0 1 1 1]
+     [0 1 1 1 0 0 1 0 1]]
+
+    See also
+    --------
+    HashingVectorizer, TfidfVectorizer
+
+    Notes
+    -----
+    The ``stop_words_`` attribute can get large and increase the model size
+    when pickling. This attribute is provided only for introspection and can
+    be safely removed using delattr or set to None before pickling.
+    """
+
+    def __init__(self, input='content', encoding='utf-8',
+                 decode_error='strict', strip_accents=None,
+                 lowercase=True, preprocessor=None, tokenizer=None,
+                 stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
+                 ngram_range=(1, 1), analyzer='word',
+                 max_df=1.0, min_df=1, max_features=None,
+                 vocabulary=None, binary=False, dtype=np.int64):
+        self.input = input
+        self.encoding = encoding
+        self.decode_error = decode_error
+        self.strip_accents = strip_accents
+        self.preprocessor = preprocessor
+        self.tokenizer = tokenizer
+        self.analyzer = analyzer
+        self.lowercase = lowercase
+        self.token_pattern = token_pattern
+        self.stop_words = stop_words
+        self.max_df = max_df
+        self.min_df = min_df
+        if max_df < 0 or min_df < 0:
+            raise ValueError("negative value for max_df or min_df")
+        self.max_features = max_features
+        if max_features is not None:
+            if (not isinstance(max_features, numbers.Integral) or
+                    max_features <= 0):
+                raise ValueError(
+                    "max_features=%r, neither a positive integer nor None"
+                    % max_features)
+        self.ngram_range = ngram_range
+        self.vocabulary = vocabulary
+        self.binary = binary
+        self.dtype = dtype
+
+    def _sort_features(self, X, vocabulary):
+        """Sort features by name
+
+        Returns a reordered matrix and modifies the vocabulary in place
+        """
+        sorted_features = sorted(vocabulary.items())
+        map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)
+        for new_val, (term, old_val) in enumerate(sorted_features):
+            vocabulary[term] = new_val
+            map_index[old_val] = new_val
+
+        X.indices = map_index.take(X.indices, mode='clip')
+        return X
+
+    def _limit_features(self, X, vocabulary, high=None, low=None,
+                        limit=None):
+        """Remove too rare or too common features.
+
+        Prune features that are non zero in more samples than high or less
+        documents than low, modifying the vocabulary, and restricting it to
+        at most the limit most frequent.
+
+        This does not prune samples with zero features.
+        """
+        if high is None and low is None and limit is None:
+            return X, set()
+
+        # Calculate a mask based on document frequencies
+        dfs = _document_frequency(X)
+        mask = np.ones(len(dfs), dtype=bool)
+        if high is not None:
+            mask &= dfs <= high
+        if low is not None:
+            mask &= dfs >= low
+        if limit is not None and mask.sum() > limit:
+            tfs = np.asarray(X.sum(axis=0)).ravel()
+            mask_inds = (-tfs[mask]).argsort()[:limit]
+            new_mask = np.zeros(len(dfs), dtype=bool)
+            new_mask[np.where(mask)[0][mask_inds]] = True
+            mask = new_mask
+
+        new_indices = np.cumsum(mask) - 1  # maps old indices to new
+        removed_terms = set()
+        for term, old_index in list(vocabulary.items()):
+            if mask[old_index]:
+                vocabulary[term] = new_indices[old_index]
+            else:
+                del vocabulary[term]
+                removed_terms.add(term)
+        kept_indices = np.where(mask)[0]
+        if len(kept_indices) == 0:
+            raise ValueError("After pruning, no terms remain. Try a lower"
+                             " min_df or a higher max_df.")
+        return X[:, kept_indices], removed_terms
+
+    def _count_vocab(self, raw_documents, fixed_vocab):
+        """Create sparse feature matrix, and vocabulary where fixed_vocab=False
+        """
+        if fixed_vocab:
+            vocabulary = self.vocabulary_
+        else:
+            # Add a new value when a new vocabulary item is seen
+            vocabulary = defaultdict()
+            vocabulary.default_factory = vocabulary.__len__
+
+        analyze = self.build_analyzer()
+        j_indices = []
+        indptr = []
+
+        values = _make_int_array()
+        indptr.append(0)
+        for doc in raw_documents:
+            feature_counter = {}
+            for feature in analyze(doc):
+                try:
+                    feature_idx = vocabulary[feature]
+                    if feature_idx not in feature_counter:
+                        feature_counter[feature_idx] = 1
+                    else:
+                        feature_counter[feature_idx] += 1
+                except KeyError:
+                    # Ignore out-of-vocabulary items for fixed_vocab=True
+                    continue
+
+            j_indices.extend(feature_counter.keys())
+            values.extend(feature_counter.values())
+            indptr.append(len(j_indices))
+
+        if not fixed_vocab:
+            # disable defaultdict behaviour
+            vocabulary = dict(vocabulary)
+            if not vocabulary:
+                raise ValueError("empty vocabulary; perhaps the documents only"
+                                 " contain stop words")
+
+        if indptr[-1] > 2147483648:  # = 2**31 - 1
+            if _IS_32BIT:
+                raise ValueError(('sparse CSR array has {} non-zero '
+                                  'elements and requires 64 bit indexing, '
+                                  'which is unsupported with 32 bit Python.')
+                                 .format(indptr[-1]))
+            indices_dtype = np.int64
+
+        else:
+            indices_dtype = np.int32
+        j_indices = np.asarray(j_indices, dtype=indices_dtype)
+        indptr = np.asarray(indptr, dtype=indices_dtype)
+        values = np.frombuffer(values, dtype=np.intc)
+
+        X = sp.csr_matrix((values, j_indices, indptr),
+                          shape=(len(indptr) - 1, len(vocabulary)),
+                          dtype=self.dtype)
+        X.sort_indices()
+        return vocabulary, X
+
+    def fit(self, raw_documents, y=None):
+        """Learn a vocabulary dictionary of all tokens in the raw documents.
+
+        Parameters
+        ----------
+        raw_documents : iterable
+            An iterable which yields either str, unicode or file objects.
+
+        Returns
+        -------
+        self
+        """
+        self._warn_for_unused_params()
+        self.fit_transform(raw_documents)
+        return self
+
+    def fit_transform(self, raw_documents, y=None):
+        """Learn the vocabulary dictionary and return term-document matrix.
+
+        This is equivalent to fit followed by transform, but more efficiently
+        implemented.
+
+        Parameters
+        ----------
+        raw_documents : iterable
+            An iterable which yields either str, unicode or file objects.
+
+        Returns
+        -------
+        X : array, [n_samples, n_features]
+            Document-term matrix.
+        """
+        # We intentionally don't call the transform method to make
+        # fit_transform overridable without unwanted side effects in
+        # TfidfVectorizer.
+        if isinstance(raw_documents, str):
+            raise ValueError(
+                "Iterable over raw text documents expected, "
+                "string object received.")
+
+        self._validate_params()
+        self._validate_vocabulary()
+        max_df = self.max_df
+        min_df = self.min_df
+        max_features = self.max_features
+
+        vocabulary, X = self._count_vocab(raw_documents,
+                                          self.fixed_vocabulary_)
+
+        if self.binary:
+            X.data.fill(1)
+
+        if not self.fixed_vocabulary_:
+            X = self._sort_features(X, vocabulary)
+
+            n_doc = X.shape[0]
+            max_doc_count = (max_df
+                             if isinstance(max_df, numbers.Integral)
+                             else max_df * n_doc)
+            min_doc_count = (min_df
+                             if isinstance(min_df, numbers.Integral)
+                             else min_df * n_doc)
+            if max_doc_count < min_doc_count:
+                raise ValueError(
+                    "max_df corresponds to < documents than min_df")
+            X, self.stop_words_ = self._limit_features(X, vocabulary,
+                                                       max_doc_count,
+                                                       min_doc_count,
+                                                       max_features)
+
+            self.vocabulary_ = vocabulary
+
+        return X
+
+    def transform(self, raw_documents):
+        """Transform documents to document-term matrix.
+
+        Extract token counts out of raw text documents using the vocabulary
+        fitted with fit or the one provided to the constructor.
+
+        Parameters
+        ----------
+        raw_documents : iterable
+            An iterable which yields either str, unicode or file objects.
+
+        Returns
+        -------
+        X : sparse matrix, [n_samples, n_features]
+            Document-term matrix.
+        """
+        if isinstance(raw_documents, str):
+            raise ValueError(
+                "Iterable over raw text documents expected, "
+                "string object received.")
+        self._check_vocabulary()
+
+        # use the same matrix-building strategy as fit_transform
+        _, X = self._count_vocab(raw_documents, fixed_vocab=True)
+        if self.binary:
+            X.data.fill(1)
+        return X
+
+    def inverse_transform(self, X):
+        """Return terms per document with nonzero entries in X.
+
+        Parameters
+        ----------
+        X : {array, sparse matrix}, shape = [n_samples, n_features]
+
+        Returns
+        -------
+        X_inv : list of arrays, len = n_samples
+            List of arrays of terms.
+        """
+        self._check_vocabulary()
+
+        if sp.issparse(X):
+            # We need CSR format for fast row manipulations.
+            X = X.tocsr()
+        else:
+            # We need to convert X to a matrix, so that the indexing
+            # returns 2D objects
+            X = np.asmatrix(X)
+        n_samples = X.shape[0]
+
+        terms = np.array(list(self.vocabulary_.keys()))
+        indices = np.array(list(self.vocabulary_.values()))
+        inverse_vocabulary = terms[np.argsort(indices)]
+
+        return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()
+                for i in range(n_samples)]
+
+    def get_feature_names(self):
+        """Array mapping from feature integer indices to feature name"""
+
+        self._check_vocabulary()
+
+        return [t for t, i in sorted(self.vocabulary_.items(),
+                                     key=itemgetter(1))]
+
+    def _more_tags(self):
+        return {'X_types': ['string']}
+
+
+def _make_int_array():
+    """Construct an array.array of a type suitable for scipy.sparse indices."""
+    return array.array(str("i"))
+
+
+class TfidfTransformer(TransformerMixin, BaseEstimator):
+    """Transform a count matrix to a normalized tf or tf-idf representation
+
+    Tf means term-frequency while tf-idf means term-frequency times inverse
+    document-frequency. This is a common term weighting scheme in information
+    retrieval, that has also found good use in document classification.
+
+    The goal of using tf-idf instead of the raw frequencies of occurrence of a
+    token in a given document is to scale down the impact of tokens that occur
+    very frequently in a given corpus and that are hence empirically less
+    informative than features that occur in a small fraction of the training
+    corpus.
+
+    The formula that is used to compute the tf-idf for a term t of a document d
+    in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is
+    computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where
+    n is the total number of documents in the document set and df(t) is the
+    document frequency of t; the document frequency is the number of documents
+    in the document set that contain the term t. The effect of adding "1" to
+    the idf in the equation above is that terms with zero idf, i.e., terms
+    that occur in all documents in a training set, will not be entirely
+    ignored.
+    (Note that the idf formula above differs from the standard textbook
+    notation that defines the idf as
+    idf(t) = log [ n / (df(t) + 1) ]).
+
+    If ``smooth_idf=True`` (the default), the constant "1" is added to the
+    numerator and denominator of the idf as if an extra document was seen
+    containing every term in the collection exactly once, which prevents
+    zero divisions: idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.
+
+    Furthermore, the formulas used to compute tf and idf depend
+    on parameter settings that correspond to the SMART notation used in IR
+    as follows:
+
+    Tf is "n" (natural) by default, "l" (logarithmic) when
+    ``sublinear_tf=True``.
+    Idf is "t" when use_idf is given, "n" (none) otherwise.
+    Normalization is "c" (cosine) when ``norm='l2'``, "n" (none)
+    when ``norm=None``.
+
+    Read more in the :ref:`User Guide <text_feature_extraction>`.
+
+    Parameters
+    ----------
+    norm : 'l1', 'l2' or None, optional (default='l2')
+        Each output row will have unit norm, either:
+        * 'l2': Sum of squares of vector elements is 1. The cosine
+        similarity between two vectors is their dot product when l2 norm has
+        been applied.
+        * 'l1': Sum of absolute values of vector elements is 1.
+        See :func:`preprocessing.normalize`
+
+    use_idf : boolean (default=True)
+        Enable inverse-document-frequency reweighting.
+
+    smooth_idf : boolean (default=True)
+        Smooth idf weights by adding one to document frequencies, as if an
+        extra document was seen containing every term in the collection
+        exactly once. Prevents zero divisions.
+
+    sublinear_tf : boolean (default=False)
+        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).
+
+    Attributes
+    ----------
+    idf_ : array, shape (n_features)
+        The inverse document frequency (IDF) vector; only defined
+        if  ``use_idf`` is True.
+
+    References
+    ----------
+
+    .. [Yates2011] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern
+                   Information Retrieval. Addison Wesley, pp. 68-74.
+
+    .. [MRS2008] C.D. Manning, P. Raghavan and H. Schütze  (2008).
+                   Introduction to Information Retrieval. Cambridge University
+                   Press, pp. 118-120.
+    """
+
+    def __init__(self, norm='l2', use_idf=True, smooth_idf=True,
+                 sublinear_tf=False):
+        self.norm = norm
+        self.use_idf = use_idf
+        self.smooth_idf = smooth_idf
+        self.sublinear_tf = sublinear_tf
+
+    def fit(self, X, y=None):
+        """Learn the idf vector (global term weights)
+
+        Parameters
+        ----------
+        X : sparse matrix, [n_samples, n_features]
+            a matrix of term/token counts
+        """
+        X = check_array(X, accept_sparse=('csr', 'csc'))
+        if not sp.issparse(X):
+            X = sp.csr_matrix(X)
+        dtype = X.dtype if X.dtype in FLOAT_DTYPES else np.float64
+
+        if self.use_idf:
+            n_samples, n_features = X.shape
+            df = _document_frequency(X)
+            df = df.astype(dtype, **_astype_copy_false(df))
+
+            # perform idf smoothing if required
+            df += int(self.smooth_idf)
+            n_samples += int(self.smooth_idf)
+
+            # log+1 instead of log makes sure terms with zero idf don't get
+            # suppressed entirely.
+            idf = np.log(n_samples / df) + 1
+            self._idf_diag = sp.diags(idf, offsets=0,
+                                      shape=(n_features, n_features),
+                                      format='csr',
+                                      dtype=dtype)
+
+        return self
+
+    def transform(self, X, copy=True):
+        """Transform a count matrix to a tf or tf-idf representation
+
+        Parameters
+        ----------
+        X : sparse matrix, [n_samples, n_features]
+            a matrix of term/token counts
+
+        copy : boolean, default True
+            Whether to copy X and operate on the copy or perform in-place
+            operations.
+
+        Returns
+        -------
+        vectors : sparse matrix, [n_samples, n_features]
+        """
+        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES, copy=copy)
+        if not sp.issparse(X):
+            X = sp.csr_matrix(X, dtype=np.float64)
+
+        n_samples, n_features = X.shape
+
+        if self.sublinear_tf:
+            np.log(X.data, X.data)
+            X.data += 1
+
+        if self.use_idf:
+            check_is_fitted(self, msg='idf vector is not fitted')
+
+            expected_n_features = self._idf_diag.shape[0]
+            if n_features != expected_n_features:
+                raise ValueError("Input has n_features=%d while the model"
+                                 " has been trained with n_features=%d" % (
+                                     n_features, expected_n_features))
+            # *= doesn't work
+            X = X * self._idf_diag
+
+        if self.norm:
+            X = normalize(X, norm=self.norm, copy=False)
+
+        return X
+
+    @property
+    def idf_(self):
+        # if _idf_diag is not set, this will raise an attribute error,
+        # which means hasattr(self, "idf_") is False
+        return np.ravel(self._idf_diag.sum(axis=0))
+
+    @idf_.setter
+    def idf_(self, value):
+        value = np.asarray(value, dtype=np.float64)
+        n_features = value.shape[0]
+        self._idf_diag = sp.spdiags(value, diags=0, m=n_features,
+                                    n=n_features, format='csr')
+
+    def _more_tags(self):
+        return {'X_types': 'sparse'}
+
+
+class TfidfVectorizer(CountVectorizer):
+    """Convert a collection of raw documents to a matrix of TF-IDF features.
+
+    Equivalent to :class:`CountVectorizer` followed by
+    :class:`TfidfTransformer`.
+
+    Read more in the :ref:`User Guide <text_feature_extraction>`.
+
+    Parameters
+    ----------
+    input : string {'filename', 'file', 'content'}
+        If 'filename', the sequence passed as an argument to fit is
+        expected to be a list of filenames that need reading to fetch
+        the raw content to analyze.
+
+        If 'file', the sequence items must have a 'read' method (file-like
+        object) that is called to fetch the bytes in memory.
+
+        Otherwise the input is expected to be a sequence of items that
+        can be of type string or byte.
+
+    encoding : string, 'utf-8' by default.
+        If bytes or files are given to analyze, this encoding is used to
+        decode.
+
+    decode_error : {'strict', 'ignore', 'replace'} (default='strict')
+        Instruction on what to do if a byte sequence is given to analyze that
+        contains characters not of the given `encoding`. By default, it is
+        'strict', meaning that a UnicodeDecodeError will be raised. Other
+        values are 'ignore' and 'replace'.
+
+    strip_accents : {'ascii', 'unicode', None} (default=None)
+        Remove accents and perform other character normalization
+        during the preprocessing step.
+        'ascii' is a fast method that only works on characters that have
+        an direct ASCII mapping.
+        'unicode' is a slightly slower method that works on any characters.
+        None (default) does nothing.
+
+        Both 'ascii' and 'unicode' use NFKD normalization from
+        :func:`unicodedata.normalize`.
+
+    lowercase : boolean (default=True)
+        Convert all characters to lowercase before tokenizing.
+
+    preprocessor : callable or None (default=None)
+        Override the preprocessing (string transformation) stage while
+        preserving the tokenizing and n-grams generation steps.
+        Only applies if ``analyzer is not callable``.
+
+    tokenizer : callable or None (default=None)
+        Override the string tokenization step while preserving the
+        preprocessing and n-grams generation steps.
+        Only applies if ``analyzer == 'word'``.
+
+    analyzer : string, {'word', 'char', 'char_wb'} or callable
+        Whether the feature should be made of word or character n-grams.
+        Option 'char_wb' creates character n-grams only from text inside
+        word boundaries; n-grams at the edges of words are padded with space.
+
+        If a callable is passed it is used to extract the sequence of features
+        out of the raw, unprocessed input.
+
+        .. versionchanged:: 0.21
+
+        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is
+        first read from the file and then passed to the given callable
+        analyzer.
+
+    stop_words : string {'english'}, list, or None (default=None)
+        If a string, it is passed to _check_stop_list and the appropriate stop
+        list is returned. 'english' is currently the only supported string
+        value.
+        There are several known issues with 'english' and you should
+        consider an alternative (see :ref:`stop_words`).
+
+        If a list, that list is assumed to contain stop words, all of which
+        will be removed from the resulting tokens.
+        Only applies if ``analyzer == 'word'``.
+
+        If None, no stop words will be used. max_df can be set to a value
+        in the range [0.7, 1.0) to automatically detect and filter stop
+        words based on intra corpus document frequency of terms.
+
+    token_pattern : string
+        Regular expression denoting what constitutes a "token", only used
+        if ``analyzer == 'word'``. The default regexp selects tokens of 2
+        or more alphanumeric characters (punctuation is completely ignored
+        and always treated as a token separator).
+
+    ngram_range : tuple (min_n, max_n), default=(1, 1)
+        The lower and upper boundary of the range of n-values for different
+        n-grams to be extracted. All values of n such that min_n <= n <= max_n
+        will be used. For example an ``ngram_range`` of ``(1, 1)`` means only
+        unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means
+        only bigrams.
+        Only applies if ``analyzer is not callable``.
+
+    max_df : float in range [0.0, 1.0] or int (default=1.0)
+        When building the vocabulary ignore terms that have a document
+        frequency strictly higher than the given threshold (corpus-specific
+        stop words).
+        If float, the parameter represents a proportion of documents, integer
+        absolute counts.
+        This parameter is ignored if vocabulary is not None.
+
+    min_df : float in range [0.0, 1.0] or int (default=1)
+        When building the vocabulary ignore terms that have a document
+        frequency strictly lower than the given threshold. This value is also
+        called cut-off in the literature.
+        If float, the parameter represents a proportion of documents, integer
+        absolute counts.
+        This parameter is ignored if vocabulary is not None.
+
+    max_features : int or None (default=None)
+        If not None, build a vocabulary that only consider the top
+        max_features ordered by term frequency across the corpus.
+
+        This parameter is ignored if vocabulary is not None.
+
+    vocabulary : Mapping or iterable, optional (default=None)
+        Either a Mapping (e.g., a dict) where keys are terms and values are
+        indices in the feature matrix, or an iterable over terms. If not
+        given, a vocabulary is determined from the input documents.
+
+    binary : boolean (default=False)
+        If True, all non-zero term counts are set to 1. This does not mean
+        outputs will have only 0/1 values, only that the tf term in tf-idf
+        is binary. (Set idf and normalization to False to get 0/1 outputs.)
+
+    dtype : type, optional (default=float64)
+        Type of the matrix returned by fit_transform() or transform().
+
+    norm : 'l1', 'l2' or None, optional (default='l2')
+        Each output row will have unit norm, either:
+        * 'l2': Sum of squares of vector elements is 1. The cosine
+        similarity between two vectors is their dot product when l2 norm has
+        been applied.
+        * 'l1': Sum of absolute values of vector elements is 1.
+        See :func:`preprocessing.normalize`
+
+    use_idf : boolean (default=True)
+        Enable inverse-document-frequency reweighting.
+
+    smooth_idf : boolean (default=True)
+        Smooth idf weights by adding one to document frequencies, as if an
+        extra document was seen containing every term in the collection
+        exactly once. Prevents zero divisions.
+
+    sublinear_tf : boolean (default=False)
+        Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).
+
+    Attributes
+    ----------
+    vocabulary_ : dict
+        A mapping of terms to feature indices.
+
+    fixed_vocabulary_: boolean
+        True if a fixed vocabulary of term to indices mapping
+        is provided by the user
+
+    idf_ : array, shape (n_features)
+        The inverse document frequency (IDF) vector; only defined
+        if ``use_idf`` is True.
+
+    stop_words_ : set
+        Terms that were ignored because they either:
+
+          - occurred in too many documents (`max_df`)
+          - occurred in too few documents (`min_df`)
+          - were cut off by feature selection (`max_features`).
+
+        This is only available if no vocabulary was given.
+
+    Examples
+    --------
+    >>> from sklearn.feature_extraction.text import TfidfVectorizer
+    >>> corpus = [
+    ...     'This is the first document.',
+    ...     'This document is the second document.',
+    ...     'And this is the third one.',
+    ...     'Is this the first document?',
+    ... ]
+    >>> vectorizer = TfidfVectorizer()
+    >>> X = vectorizer.fit_transform(corpus)
+    >>> print(vectorizer.get_feature_names())
+    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']
+    >>> print(X.shape)
+    (4, 9)
+
+    See also
+    --------
+    CountVectorizer : Transforms text into a sparse matrix of n-gram counts.
+
+    TfidfTransformer : Performs the TF-IDF transformation from a provided
+        matrix of counts.
+
+    Notes
+    -----
+    The ``stop_words_`` attribute can get large and increase the model size
+    when pickling. This attribute is provided only for introspection and can
+    be safely removed using delattr or set to None before pickling.
+    """
+
+    def __init__(self, input='content', encoding='utf-8',
+                 decode_error='strict', strip_accents=None, lowercase=True,
+                 preprocessor=None, tokenizer=None, analyzer='word',
+                 stop_words=None, token_pattern=r"(?u)\b\w\w+\b",
+                 ngram_range=(1, 1), max_df=1.0, min_df=1,
+                 max_features=None, vocabulary=None, binary=False,
+                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,
+                 sublinear_tf=False):
+
+        super().__init__(
+            input=input, encoding=encoding, decode_error=decode_error,
+            strip_accents=strip_accents, lowercase=lowercase,
+            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,
+            stop_words=stop_words, token_pattern=token_pattern,
+            ngram_range=ngram_range, max_df=max_df, min_df=min_df,
+            max_features=max_features, vocabulary=vocabulary, binary=binary,
+            dtype=dtype)
+
+        self._tfidf = TfidfTransformer(norm=norm, use_idf=use_idf,
+                                       smooth_idf=smooth_idf,
+                                       sublinear_tf=sublinear_tf)
+
+    # Broadcast the TF-IDF parameters to the underlying transformer instance
+    # for easy grid search and repr
+
+    @property
+    def norm(self):
+        return self._tfidf.norm
+
+    @norm.setter
+    def norm(self, value):
+        self._tfidf.norm = value
+
+    @property
+    def use_idf(self):
+        return self._tfidf.use_idf
+
+    @use_idf.setter
+    def use_idf(self, value):
+        self._tfidf.use_idf = value
+
+    @property
+    def smooth_idf(self):
+        return self._tfidf.smooth_idf
+
+    @smooth_idf.setter
+    def smooth_idf(self, value):
+        self._tfidf.smooth_idf = value
+
+    @property
+    def sublinear_tf(self):
+        return self._tfidf.sublinear_tf
+
+    @sublinear_tf.setter
+    def sublinear_tf(self, value):
+        self._tfidf.sublinear_tf = value
+
+    @property
+    def idf_(self):
+        return self._tfidf.idf_
+
+    @idf_.setter
+    def idf_(self, value):
+        self._validate_vocabulary()
+        if hasattr(self, 'vocabulary_'):
+            if len(self.vocabulary_) != len(value):
+                raise ValueError("idf length = %d must be equal "
+                                 "to vocabulary size = %d" %
+                                 (len(value), len(self.vocabulary)))
+        self._tfidf.idf_ = value
+
+    def _check_params(self):
+        if self.dtype not in FLOAT_DTYPES:
+            warnings.warn("Only {} 'dtype' should be used. {} 'dtype' will "
+                          "be converted to np.float64."
+                          .format(FLOAT_DTYPES, self.dtype),
+                          UserWarning)
+
+    def fit(self, raw_documents, y=None):
+        """Learn vocabulary and idf from training set.
+
+        Parameters
+        ----------
+        raw_documents : iterable
+            an iterable which yields either str, unicode or file objects
+
+        Returns
+        -------
+        self : TfidfVectorizer
+        """
+        self._check_params()
+        self._warn_for_unused_params()
+        X = super().fit_transform(raw_documents)
+        self._tfidf.fit(X)
+        return self
+
+    def fit_transform(self, raw_documents, y=None):
+        """Learn vocabulary and idf, return term-document matrix.
+
+        This is equivalent to fit followed by transform, but more efficiently
+        implemented.
+
+        Parameters
+        ----------
+        raw_documents : iterable
+            an iterable which yields either str, unicode or file objects
+
+        Returns
+        -------
+        X : sparse matrix, [n_samples, n_features]
+            Tf-idf-weighted document-term matrix.
+        """
+        self._check_params()
+        X = super().fit_transform(raw_documents)
+        self._tfidf.fit(X)
+        # X is already a transformed view of raw_documents so
+        # we set copy to False
+        return self._tfidf.transform(X, copy=False)
+
+    def transform(self, raw_documents, copy="deprecated"):
+        """Transform documents to document-term matrix.
+
+        Uses the vocabulary and document frequencies (df) learned by fit (or
+        fit_transform).
+
+        Parameters
+        ----------
+        raw_documents : iterable
+            an iterable which yields either str, unicode or file objects
+
+        copy : boolean, default True
+            Whether to copy X and operate on the copy or perform in-place
+            operations.
+
+            .. deprecated:: 0.22
+               The `copy` parameter is unused and was deprecated in version
+               0.22 and will be removed in 0.24. This parameter will be
+               ignored.
+
+        Returns
+        -------
+        X : sparse matrix, [n_samples, n_features]
+            Tf-idf-weighted document-term matrix.
+        """
+        check_is_fitted(self, msg='The tfidf vector is not fitted')
+
+        # FIXME Remove copy parameter support in 0.24
+        if copy != "deprecated":
+            msg = ("'copy' param is unused and has been deprecated since "
+                   "version 0.22. Backward compatibility for 'copy' will "
+                   "be removed in 0.24.")
+            warnings.warn(msg, DeprecationWarning)
+        X = super().transform(raw_documents)
+        return self._tfidf.transform(X, copy=False)
+
+    def _more_tags(self):
+        return {'X_types': ['string'], '_skip_test': True}
diff --git a/test_comprehensive.py b/test_comprehensive.py
new file mode 100644
index 000000000..2ff40ef5a
--- /dev/null
+++ b/test_comprehensive.py
@@ -0,0 +1,51 @@
+from sklearn.feature_extraction.text import strip_accents_unicode
+import unicodedata
+
+# Test cases
+test_cases = [
+    # (input, expected_output, description)
+    ('ñ', 'n', 'Precomposed LATIN SMALL LETTER N WITH TILDE'),
+    ('n\u0303', 'n', 'Decomposed n + COMBINING TILDE'),
+    ('café', 'cafe', 'Precomposed é'),
+    ('cafe\u0301', 'cafe', 'Decomposed e + COMBINING ACUTE'),
+    ('naïve', 'naive', 'Precomposed ï'),
+    ('nai\u0308ve', 'naive', 'Decomposed i + COMBINING DIAERESIS'),
+    ('', '', 'Empty string'),
+    ('hello', 'hello', 'ASCII only string'),
+    ('Übung', 'Ubung', 'German umlaut'),
+    ('U\u0308bung', 'Ubung', 'Decomposed German umlaut'),
+    ('résumé', 'resume', 'Multiple accents'),
+    ('re\u0301sume\u0301', 'resume', 'Multiple decomposed accents'),
+    ('ñoño', 'nono', 'Multiple tildes'),
+    ('n\u0303on\u0303o', 'nono', 'Multiple decomposed tildes'),
+]
+
+print("Testing strip_accents_unicode...")
+all_passed = True
+
+for input_str, expected, description in test_cases:
+    result = strip_accents_unicode(input_str)
+    passed = result == expected
+    all_passed &= passed
+    
+    status = "✓" if passed else "✗"
+    print(f"{status} {description}")
+    print(f"  Input: {repr(input_str)}")
+    print(f"  Expected: {repr(expected)}")
+    print(f"  Got: {repr(result)}")
+    
+    # Check that strings already in NFKD form are handled correctly
+    nfkd_form = unicodedata.normalize('NFKD', input_str)
+    result_nfkd = strip_accents_unicode(nfkd_form)
+    nfkd_passed = result_nfkd == expected
+    all_passed &= nfkd_passed
+    
+    if nfkd_form != input_str:
+        status_nfkd = "✓" if nfkd_passed else "✗"
+        print(f"  {status_nfkd} NFKD form: {repr(nfkd_form)} -> {repr(result_nfkd)}")
+    print()
+
+if all_passed:
+    print("All tests passed!")
+else:
+    print("Some tests failed!")
diff --git a/test_countvectorizer.py b/test_countvectorizer.py
new file mode 100644
index 000000000..eda050507
--- /dev/null
+++ b/test_countvectorizer.py
@@ -0,0 +1,30 @@
+from sklearn.feature_extraction.text import CountVectorizer
+
+# Test CountVectorizer with strip_accents='unicode'
+vectorizer = CountVectorizer(strip_accents='unicode')
+
+# Test documents with different forms of accented characters
+docs = [
+    'café',        # Precomposed é
+    'cafe\u0301',  # Decomposed e + combining acute
+    'niño',        # Precomposed ñ
+    'nin\u0303o',  # Decomposed n + combining tilde
+]
+
+# Fit and transform
+X = vectorizer.fit_transform(docs)
+feature_names = vectorizer.get_feature_names()
+
+print("Feature names:", feature_names)
+print("\nDocument-term matrix:")
+print(X.toarray())
+
+# Check that all forms are normalized to the same features
+expected_features = ['cafe', 'nino']
+assert set(feature_names) == set(expected_features), f"Expected {expected_features}, got {feature_names}"
+
+# Check that each document has the same representation
+expected_matrix = [[1, 0], [1, 0], [0, 1], [0, 1]]
+assert X.toarray().tolist() == expected_matrix, "Documents with different Unicode forms should have same representation"
+
+print("\n✓ CountVectorizer correctly handles both precomposed and decomposed Unicode forms")
diff --git a/test_issue.py b/test_issue.py
new file mode 100644
index 000000000..9f1b7fc2e
--- /dev/null
+++ b/test_issue.py
@@ -0,0 +1,25 @@
+from sklearn.feature_extraction.text import strip_accents_unicode
+
+# This string contains one code point, "LATIN SMALL LETTER N WITH TILDE"
+s1 = chr(241)
+
+# This string contains two code points, "LATIN SMALL LETTER N" followed by "COMBINING TILDE"
+s2 = chr(110) + chr(771)
+
+# They are visually identical, as expected
+print("s1:", repr(s1))
+print("s2:", repr(s2))
+
+# The tilde is removed from s1, as expected
+result1 = strip_accents_unicode(s1)
+print("strip_accents_unicode(s1):", repr(result1))
+
+# But strip_accents_unicode returns s2 unchanged
+result2 = strip_accents_unicode(s2)
+print("strip_accents_unicode(s2):", repr(result2))
+print("s2 unchanged:", result2 == s2)
+
+# Expected: both should be normalized to 'n'
+print("\nExpected both to be 'n', but got:")
+print("  s1 ->", repr(result1))
+print("  s2 ->", repr(result2))
diff --git a/test_regression.py b/test_regression.py
new file mode 100644
index 000000000..5d8dcef00
--- /dev/null
+++ b/test_regression.py
@@ -0,0 +1,26 @@
+"""
+Regression test for issue: strip_accents_unicode fails to strip accents 
+from strings that are already in NFKD form
+"""
+from sklearn.feature_extraction.text import strip_accents_unicode
+
+def test_strip_accents_nfkd_regression():
+    """Test that strip_accents_unicode works on strings already in NFKD form."""
+    # This string contains one code point, "LATIN SMALL LETTER N WITH TILDE"
+    s1 = chr(241)
+    
+    # This string contains two code points, "LATIN SMALL LETTER N" followed by "COMBINING TILDE"
+    s2 = chr(110) + chr(771)
+    
+    # Both should be normalized to 'n'
+    result1 = strip_accents_unicode(s1)
+    result2 = strip_accents_unicode(s2)
+    
+    assert result1 == 'n', f"Expected 'n' but got {repr(result1)}"
+    assert result2 == 'n', f"Expected 'n' but got {repr(result2)}"
+    assert result1 == result2, f"Both forms should normalize to the same result"
+    
+    print("✓ Regression test passed: Both precomposed and decomposed forms are correctly handled")
+
+if __name__ == "__main__":
+    test_strip_accents_nfkd_regression()
diff --git a/test_tfidf.py b/test_tfidf.py
new file mode 100644
index 000000000..23d0aa17f
--- /dev/null
+++ b/test_tfidf.py
@@ -0,0 +1,24 @@
+from sklearn.feature_extraction.text import TfidfVectorizer
+
+# Test TfidfVectorizer with strip_accents='unicode'
+vectorizer = TfidfVectorizer(strip_accents='unicode')
+
+# Test documents with different forms of accented characters
+docs = [
+    'café niño',           # Precomposed
+    'cafe\u0301 nin\u0303o',  # Decomposed
+]
+
+# Fit and transform
+X = vectorizer.fit_transform(docs)
+feature_names = vectorizer.get_feature_names()
+
+print("Feature names:", feature_names)
+print("\nTF-IDF matrix:")
+print(X.toarray())
+
+# Both documents should have identical representation
+assert list(feature_names) == ['cafe', 'nino']
+assert (X.toarray()[0] == X.toarray()[1]).all()
+
+print("\n✓ TfidfVectorizer correctly handles both precomposed and decomposed Unicode forms")
