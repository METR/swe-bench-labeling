{"instance_id": "scikit-learn__scikit-learn-10297", "score": "C", "diff": "diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex 911c74ae3..6469cdbcf 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1301,6 +1301,12 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n         weights inversely proportional to class frequencies in the input data\n         as ``n_samples / (n_classes * np.bincount(y))``\n \n+    store_cv_values : boolean, default=False\n+        Flag indicating if the cross-validation values corresponding to\n+        each alpha should be stored in the `cv_values_` attribute (see\n+        below). This flag is only compatible with `cv=None` (i.e. using\n+        Generalized Cross-Validation).\n+\n     Attributes\n     ----------\n     cv_values_ : array, shape = [n_samples, n_alphas] or \\\n@@ -1333,10 +1339,11 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n     advantage of the multi-variate response support in Ridge.\n     \"\"\"\n     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n-                 normalize=False, scoring=None, cv=None, class_weight=None):\n+                 normalize=False, scoring=None, cv=None, class_weight=None,\n+                 store_cv_values=False):\n         super(RidgeClassifierCV, self).__init__(\n             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n-            scoring=scoring, cv=cv)\n+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)\n         self.class_weight = class_weight\n \n     def fit(self, X, y, sample_weight=None):\ndiff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\nindex ee44da5d5..492c5bf2b 100644\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -598,6 +598,43 @@ def test_ridgecv_store_cv_values():\n     assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))\n \n \n+def test_ridge_classifier_cv_store_cv_values():\n+    # Test RidgeClassifierCV's store_cv_values attribute.\n+    rng = np.random.RandomState(42)\n+\n+    n_samples = 50\n+    n_features = 5\n+    x = rng.randn(n_samples, n_features)\n+    alphas = [1e-1, 1e0, 1e1]\n+    n_alphas = len(alphas)\n+\n+    # Test binary classification\n+    y_binary = rng.randint(0, 2, n_samples)\n+    r_binary = RidgeClassifierCV(alphas=alphas, store_cv_values=True)\n+    r_binary.fit(x, y_binary)\n+    assert_equal(r_binary.cv_values_.shape, (n_samples, 1, n_alphas))\n+\n+    # Test multiclass classification  \n+    n_classes = 3\n+    y_multi = rng.randint(0, n_classes, n_samples)\n+    r_multi = RidgeClassifierCV(alphas=alphas, store_cv_values=True)\n+    r_multi.fit(x, y_multi)\n+    assert_equal(r_multi.cv_values_.shape, (n_samples, n_classes, n_alphas))\n+\n+    # Test store_cv_values=False (default)\n+    r_no_store = RidgeClassifierCV(alphas=alphas, store_cv_values=False)\n+    r_no_store.fit(x, y_binary)\n+    assert not hasattr(r_no_store, 'cv_values_')\n+\n+    # Test that store_cv_values=True with cv != None raises ValueError\n+    from sklearn.model_selection import KFold\n+    r_cv_error = RidgeClassifierCV(alphas=alphas, cv=KFold(3), \n+                                   store_cv_values=True)\n+    assert_raise_message(ValueError, \n+                         \"cv!=None and store_cv_values=True  are incompatible\",\n+                         r_cv_error.fit, x, y_binary)\n+\n+\n def test_ridgecv_sample_weight():\n     rng = np.random.RandomState(0)\n     alphas = (0.1, 1.0, 10.0)\n", "gold_standard_diff": "diff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst\nindex dfba937a361ad..b01ac19a4b6c4 100644\n--- a/doc/whats_new/v0.20.rst\n+++ b/doc/whats_new/v0.20.rst\n@@ -221,6 +221,12 @@ Classifiers and regressors\n   callable and b) the input to the NearestNeighbors model is sparse.\n   :issue:`9579` by :user:`Thomas Kober <tttthomasssss>`.\n \n+- Fixed a bug in :class:`linear_model.RidgeClassifierCV` where\n+  the parameter ``store_cv_values`` was not implemented though\n+  it was documented in ``cv_values`` as a way to set up the storage\n+  of cross-validation values for different alphas. :issue:`10297` by \n+  :user:`Mabel Villalba-Jim\u00e9nez <mabelvj>`.\n+  \n - Fixed a bug in :class:`naive_bayes.MultinomialNB` which did not accept vector\n   valued pseudocounts (alpha).\n   :issue:`10346` by :user:`Tobias Madsen <TobiasMadsen>`\ndiff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py\nindex 911c74ae3b926..1d80cacac9a1d 100644\n--- a/sklearn/linear_model/ridge.py\n+++ b/sklearn/linear_model/ridge.py\n@@ -1212,18 +1212,18 @@ class RidgeCV(_BaseRidgeCV, RegressorMixin):\n \n     store_cv_values : boolean, default=False\n         Flag indicating if the cross-validation values corresponding to\n-        each alpha should be stored in the `cv_values_` attribute (see\n-        below). This flag is only compatible with `cv=None` (i.e. using\n+        each alpha should be stored in the ``cv_values_`` attribute (see\n+        below). This flag is only compatible with ``cv=None`` (i.e. using\n         Generalized Cross-Validation).\n \n     Attributes\n     ----------\n     cv_values_ : array, shape = [n_samples, n_alphas] or \\\n         shape = [n_samples, n_targets, n_alphas], optional\n-        Cross-validation values for each alpha (if `store_cv_values=True` and \\\n-        `cv=None`). After `fit()` has been called, this attribute will \\\n-        contain the mean squared errors (by default) or the values of the \\\n-        `{loss,score}_func` function (if provided in the constructor).\n+        Cross-validation values for each alpha (if ``store_cv_values=True``\\\n+        and ``cv=None``). After ``fit()`` has been called, this attribute \\\n+        will contain the mean squared errors (by default) or the values \\\n+        of the ``{loss,score}_func`` function (if provided in the constructor).\n \n     coef_ : array, shape = [n_features] or [n_targets, n_features]\n         Weight vector(s).\n@@ -1301,14 +1301,19 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n         weights inversely proportional to class frequencies in the input data\n         as ``n_samples / (n_classes * np.bincount(y))``\n \n+    store_cv_values : boolean, default=False\n+        Flag indicating if the cross-validation values corresponding to\n+        each alpha should be stored in the ``cv_values_`` attribute (see\n+        below). This flag is only compatible with ``cv=None`` (i.e. using\n+        Generalized Cross-Validation).\n+\n     Attributes\n     ----------\n-    cv_values_ : array, shape = [n_samples, n_alphas] or \\\n-    shape = [n_samples, n_responses, n_alphas], optional\n-        Cross-validation values for each alpha (if `store_cv_values=True` and\n-    `cv=None`). After `fit()` has been called, this attribute will contain \\\n-    the mean squared errors (by default) or the values of the \\\n-    `{loss,score}_func` function (if provided in the constructor).\n+    cv_values_ : array, shape = [n_samples, n_targets, n_alphas], optional\n+        Cross-validation values for each alpha (if ``store_cv_values=True`` and\n+        ``cv=None``). After ``fit()`` has been called, this attribute will\n+        contain the mean squared errors (by default) or the values of the\n+        ``{loss,score}_func`` function (if provided in the constructor).\n \n     coef_ : array, shape = [n_features] or [n_targets, n_features]\n         Weight vector(s).\n@@ -1333,10 +1338,11 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):\n     advantage of the multi-variate response support in Ridge.\n     \"\"\"\n     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,\n-                 normalize=False, scoring=None, cv=None, class_weight=None):\n+                 normalize=False, scoring=None, cv=None, class_weight=None,\n+                 store_cv_values=False):\n         super(RidgeClassifierCV, self).__init__(\n             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,\n-            scoring=scoring, cv=cv)\n+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)\n         self.class_weight = class_weight\n \n     def fit(self, X, y, sample_weight=None):\ndiff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py\nindex ee44da5d56b86..d1865dfe72d85 100644\n--- a/sklearn/linear_model/tests/test_ridge.py\n+++ b/sklearn/linear_model/tests/test_ridge.py\n@@ -575,8 +575,7 @@ def test_class_weights_cv():\n \n \n def test_ridgecv_store_cv_values():\n-    # Test _RidgeCV's store_cv_values attribute.\n-    rng = rng = np.random.RandomState(42)\n+    rng = np.random.RandomState(42)\n \n     n_samples = 8\n     n_features = 5\n@@ -589,13 +588,38 @@ def test_ridgecv_store_cv_values():\n     # with len(y.shape) == 1\n     y = rng.randn(n_samples)\n     r.fit(x, y)\n-    assert_equal(r.cv_values_.shape, (n_samples, n_alphas))\n+    assert r.cv_values_.shape == (n_samples, n_alphas)\n+\n+    # with len(y.shape) == 2\n+    n_targets = 3\n+    y = rng.randn(n_samples, n_targets)\n+    r.fit(x, y)\n+    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)\n+\n+\n+def test_ridge_classifier_cv_store_cv_values():\n+    x = np.array([[-1.0, -1.0], [-1.0, 0], [-.8, -1.0],\n+                  [1.0, 1.0], [1.0, 0.0]])\n+    y = np.array([1, 1, 1, -1, -1])\n+\n+    n_samples = x.shape[0]\n+    alphas = [1e-1, 1e0, 1e1]\n+    n_alphas = len(alphas)\n+\n+    r = RidgeClassifierCV(alphas=alphas, store_cv_values=True)\n+\n+    # with len(y.shape) == 1\n+    n_targets = 1\n+    r.fit(x, y)\n+    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)\n \n     # with len(y.shape) == 2\n-    n_responses = 3\n-    y = rng.randn(n_samples, n_responses)\n+    y = np.array([[1, 1, 1, -1, -1],\n+                  [1, -1, 1, -1, 1],\n+                  [-1, -1, 1, -1, -1]]).transpose()\n+    n_targets = y.shape[1]\n     r.fit(x, y)\n-    assert_equal(r.cv_values_.shape, (n_samples, n_responses, n_alphas))\n+    assert r.cv_values_.shape == (n_samples, n_targets, n_alphas)\n \n \n def test_ridgecv_sample_weight():\n@@ -618,7 +642,7 @@ def test_ridgecv_sample_weight():\n         gs = GridSearchCV(Ridge(), parameters, cv=cv)\n         gs.fit(X, y, sample_weight=sample_weight)\n \n-        assert_equal(ridgecv.alpha_, gs.best_estimator_.alpha)\n+        assert ridgecv.alpha_ == gs.best_estimator_.alpha\n         assert_array_almost_equal(ridgecv.coef_, gs.best_estimator_.coef_)\n \n \n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/10297"}