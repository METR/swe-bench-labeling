diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py
index 8a1bd3625..c665370ab 100644
--- a/sklearn/ensemble/iforest.py
+++ b/sklearn/ensemble/iforest.py
@@ -120,6 +120,10 @@ class IsolationForest(BaseBagging, OutlierMixin):
     verbose : int, optional (default=0)
         Controls the verbosity of the tree building process.
 
+    warm_start : bool, optional (default=False)
+        When set to ``True``, reuse the solution of the previous call to fit
+        and add more estimators to the ensemble, otherwise, just fit a whole
+        new forest. See :term:`the Glossary <warm_start>`.
 
     Attributes
     ----------
@@ -173,7 +177,8 @@ class IsolationForest(BaseBagging, OutlierMixin):
                  n_jobs=None,
                  behaviour='old',
                  random_state=None,
-                 verbose=0):
+                 verbose=0,
+                 warm_start=False):
         super().__init__(
             base_estimator=ExtraTreeRegressor(
                 max_features=1,
@@ -187,7 +192,8 @@ class IsolationForest(BaseBagging, OutlierMixin):
             max_features=max_features,
             n_jobs=n_jobs,
             random_state=random_state,
-            verbose=verbose)
+            verbose=verbose,
+            warm_start=warm_start)
 
         self.behaviour = behaviour
         self.contamination = contamination
diff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py
index 67ba2d7f9..6c1d27b2d 100644
--- a/sklearn/ensemble/tests/test_iforest.py
+++ b/sklearn/ensemble/tests/test_iforest.py
@@ -345,6 +345,52 @@ def test_iforest_chunks_works1(
     assert mocked_get_chunk.call_count == n_predict_calls
 
 
+@pytest.mark.filterwarnings('ignore:default contamination')
+@pytest.mark.filterwarnings('ignore:behaviour="old"')
+def test_iforest_warm_start():
+    """Test IsolationForest warm_start implementation."""
+    X = iris.data
+    # Use same random state to ensure reproducibility
+    rng = check_random_state(42)
+    
+    # Fit a forest with warm_start=True, then increase n_estimators
+    clf_ws = IsolationForest(n_estimators=50, random_state=rng,
+                            warm_start=True)  
+    clf_ws.fit(X)
+    assert len(clf_ws.estimators_) == 50
+    
+    # Store decision function for first 50 trees
+    decision_50 = clf_ws.decision_function(X)
+    
+    # Add 50 more estimators  
+    clf_ws.set_params(n_estimators=100)
+    clf_ws.fit(X)
+    assert len(clf_ws.estimators_) == 100
+    
+    # Store decision function for 100 trees
+    decision_100 = clf_ws.decision_function(X)
+    
+    # Decision function should have changed after adding trees
+    assert not np.allclose(decision_50, decision_100)
+    
+    # Fit a new forest directly with 100 estimators
+    clf_no_ws = IsolationForest(n_estimators=100, random_state=rng,
+                                warm_start=False)
+    clf_no_ws.fit(X)
+    
+    # Check that both have the same number of estimators
+    assert len(clf_no_ws.estimators_) == len(clf_ws.estimators_)
+    
+    # Test that reducing n_estimators raises an error  
+    clf_ws.set_params(n_estimators=10)
+    assert_raises(ValueError, clf_ws.fit, X)
+    
+    # Test that warm start with the same n_estimators warns
+    clf_ws.set_params(n_estimators=100)
+    with pytest.warns(UserWarning, match="Warm-start fitting without"):
+        clf_ws.fit(X)
+
+
 # idem with chunk_size = 5 rows
 @patch(
     "sklearn.ensemble.iforest.get_chunk_n_rows",
