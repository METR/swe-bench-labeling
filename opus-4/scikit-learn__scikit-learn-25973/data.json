{"instance_id": "scikit-learn__scikit-learn-25973", "score": "C", "diff": "diff --git a/sklearn/feature_selection/_sequential.py b/sklearn/feature_selection/_sequential.py\nindex e983c55de..dacf4ab20 100644\n--- a/sklearn/feature_selection/_sequential.py\n+++ b/sklearn/feature_selection/_sequential.py\n@@ -13,7 +13,7 @@ from ..utils._param_validation import HasMethods, Hidden, Interval, StrOptions\n from ..utils._param_validation import RealNotInt\n from ..utils._tags import _safe_tags\n from ..utils.validation import check_is_fitted\n-from ..model_selection import cross_val_score\n+from ..model_selection import cross_val_score, check_cv\n from ..metrics import get_scorer_names\n \n \n@@ -259,6 +259,9 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator\n         if self.tol is not None and self.tol < 0 and self.direction == \"forward\":\n             raise ValueError(\"tol must be positive when doing forward selection\")\n \n+        # Validate cv to ensure it's reusable across multiple calls\n+        cv = check_cv(self.cv, y, classifier=tags.get(\"requires_positive_y\", False))\n+\n         cloned_estimator = clone(self.estimator)\n \n         # the current mask corresponds to the set of features:\n@@ -275,7 +278,7 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator\n         is_auto_select = self.tol is not None and self.n_features_to_select == \"auto\"\n         for _ in range(n_iterations):\n             new_feature_idx, new_score = self._get_best_new_feature_score(\n-                cloned_estimator, X, y, current_mask\n+                cloned_estimator, X, y, current_mask, cv\n             )\n             if is_auto_select and ((new_score - old_score) < self.tol):\n                 break\n@@ -291,7 +294,7 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator\n \n         return self\n \n-    def _get_best_new_feature_score(self, estimator, X, y, current_mask):\n+    def _get_best_new_feature_score(self, estimator, X, y, current_mask, cv):\n         # Return the best new feature and its score to add to the current_mask,\n         # i.e. return the best new feature and its score to add (resp. remove)\n         # when doing forward selection (resp. backward selection).\n@@ -309,7 +312,7 @@ class SequentialFeatureSelector(SelectorMixin, MetaEstimatorMixin, BaseEstimator\n                 estimator,\n                 X_new,\n                 y,\n-                cv=self.cv,\n+                cv=cv,\n                 scoring=self.scoring,\n                 n_jobs=self.n_jobs,\n             ).mean()\ndiff --git a/sklearn/feature_selection/tests/test_sequential.py b/sklearn/feature_selection/tests/test_sequential.py\nindex f6451a360..2df20800f 100644\n--- a/sklearn/feature_selection/tests/test_sequential.py\n+++ b/sklearn/feature_selection/tests/test_sequential.py\n@@ -9,8 +9,10 @@ from sklearn.feature_selection import SequentialFeatureSelector\n from sklearn.datasets import make_regression, make_blobs\n from sklearn.linear_model import LinearRegression\n from sklearn.ensemble import HistGradientBoostingRegressor\n-from sklearn.model_selection import cross_val_score\n+from sklearn.model_selection import cross_val_score, LeaveOneGroupOut, KFold\n from sklearn.cluster import KMeans\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.datasets import make_classification\n \n \n def test_bad_n_features_to_select():\ndiff --git a/test_cv_iterable_fix.py b/test_cv_iterable_fix.py\nnew file mode 100644\nindex 000000000..3b3acb68b\n--- /dev/null\n+++ b/test_cv_iterable_fix.py\n@@ -0,0 +1,75 @@\n+\"\"\"\n+Test that SequentialFeatureSelector works with iterable cv parameter.\n+This is a regression test for the issue where passing an iterable of splits\n+would fail because the iterator gets exhausted after the first feature evaluation.\n+\"\"\"\n+import pytest\n+from sklearn.datasets import make_classification\n+from sklearn.feature_selection import SequentialFeatureSelector\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.model_selection import LeaveOneGroupOut, KFold\n+import numpy as np\n+\n+\n+def test_sequential_feature_selector_with_iterable_cv():\n+    \"\"\"Test that SequentialFeatureSelector works with iterator/generator cv.\"\"\"\n+    X, y = make_classification(n_samples=50, n_features=10, random_state=42)\n+    \n+    # Test with LeaveOneGroupOut iterator\n+    groups = np.zeros_like(y, dtype=int)\n+    groups[y.size//2:] = 1\n+    \n+    cv = LeaveOneGroupOut()\n+    splits = cv.split(X, y, groups=groups)\n+    \n+    clf = KNeighborsClassifier(n_neighbors=5)\n+    seq = SequentialFeatureSelector(clf, n_features_to_select=3, scoring='accuracy', cv=splits)\n+    \n+    # This should not raise an IndexError anymore\n+    seq.fit(X, y)\n+    \n+    assert seq.n_features_to_select_ == 3\n+    assert seq.transform(X).shape == (50, 3)\n+\n+\n+def test_sequential_feature_selector_with_list_of_splits():\n+    \"\"\"Test that SequentialFeatureSelector works with a list of splits.\"\"\"\n+    X, y = make_classification(n_samples=50, n_features=10, random_state=42)\n+    \n+    # Test with a list of splits from KFold\n+    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n+    splits_list = list(cv.split(X))\n+    \n+    clf = KNeighborsClassifier(n_neighbors=5)\n+    seq = SequentialFeatureSelector(clf, n_features_to_select=4, scoring='accuracy', cv=splits_list)\n+    \n+    seq.fit(X, y)\n+    \n+    assert seq.n_features_to_select_ == 4\n+    assert seq.transform(X).shape == (50, 4)\n+\n+\n+def test_sequential_feature_selector_backwards_compatibility():\n+    \"\"\"Test that existing behavior with cv objects and integers still works.\"\"\"\n+    X, y = make_classification(n_samples=50, n_features=10, random_state=42)\n+    clf = KNeighborsClassifier(n_neighbors=5)\n+    \n+    # Test with CV splitter object\n+    cv = KFold(n_splits=3, shuffle=True, random_state=42)\n+    seq1 = SequentialFeatureSelector(clf, n_features_to_select=3, scoring='accuracy', cv=cv)\n+    seq1.fit(X, y)\n+    \n+    assert seq1.n_features_to_select_ == 3\n+    \n+    # Test with integer cv\n+    seq2 = SequentialFeatureSelector(clf, n_features_to_select=3, scoring='accuracy', cv=3)\n+    seq2.fit(X, y)\n+    \n+    assert seq2.n_features_to_select_ == 3\n+\n+\n+if __name__ == \"__main__\":\n+    test_sequential_feature_selector_with_iterable_cv()\n+    test_sequential_feature_selector_with_list_of_splits()\n+    test_sequential_feature_selector_backwards_compatibility()\n+    print(\"All tests passed!\")\ndiff --git a/test_fix_comprehensive.py b/test_fix_comprehensive.py\nnew file mode 100644\nindex 000000000..60f695059\n--- /dev/null\n+++ b/test_fix_comprehensive.py\n@@ -0,0 +1,67 @@\n+from sklearn.datasets import make_classification\n+from sklearn.feature_selection import SequentialFeatureSelector\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.model_selection import LeaveOneGroupOut, KFold\n+\n+import numpy as np\n+\n+# Test 1: Using iterator splits from LeaveOneGroupOut\n+print(\"Test 1: Using iterator splits...\")\n+X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n+\n+groups = np.zeros_like(y, dtype=int)\n+groups[y.size//2:] = 1\n+\n+cv = LeaveOneGroupOut()\n+splits = cv.split(X, y, groups=groups)\n+\n+clf = KNeighborsClassifier(n_neighbors=5)\n+\n+seq = SequentialFeatureSelector(clf, n_features_to_select=5, scoring='accuracy', cv=splits)\n+seq.fit(X, y)\n+\n+print(\"Number of selected features:\", seq.n_features_to_select_)\n+print(\"Selected features shape:\", seq.transform(X).shape)\n+assert seq.n_features_to_select_ == 5\n+assert seq.transform(X).shape == (100, 5)\n+print(\"\u2713 Test 1 passed!\")\n+\n+# Test 2: Using list of splits from KFold\n+print(\"\\nTest 2: Using list of splits from KFold...\")\n+cv = KFold(n_splits=3, shuffle=True, random_state=42)\n+splits_list = list(cv.split(X))\n+\n+seq2 = SequentialFeatureSelector(clf, n_features_to_select=7, scoring='accuracy', cv=splits_list)\n+seq2.fit(X, y)\n+\n+print(\"Number of selected features:\", seq2.n_features_to_select_)\n+print(\"Selected features shape:\", seq2.transform(X).shape)\n+assert seq2.n_features_to_select_ == 7\n+assert seq2.transform(X).shape == (100, 7)\n+print(\"\u2713 Test 2 passed!\")\n+\n+# Test 3: Using CV splitter object directly \n+print(\"\\nTest 3: Using CV splitter object directly...\")\n+cv = KFold(n_splits=5, shuffle=True, random_state=42)\n+\n+seq3 = SequentialFeatureSelector(clf, n_features_to_select=3, scoring='accuracy', cv=cv)\n+seq3.fit(X, y)\n+\n+print(\"Number of selected features:\", seq3.n_features_to_select_)\n+print(\"Selected features shape:\", seq3.transform(X).shape)\n+assert seq3.n_features_to_select_ == 3\n+assert seq3.transform(X).shape == (100, 3)\n+print(\"\u2713 Test 3 passed!\")\n+\n+# Test 4: Using integer cv\n+print(\"\\nTest 4: Using integer cv...\")\n+seq4 = SequentialFeatureSelector(clf, n_features_to_select=4, scoring='accuracy', cv=3)\n+seq4.fit(X, y)\n+\n+print(\"Number of selected features:\", seq4.n_features_to_select_)\n+print(\"Selected features shape:\", seq4.transform(X).shape) \n+assert seq4.n_features_to_select_ == 4\n+assert seq4.transform(X).shape == (100, 4)\n+print(\"\u2713 Test 4 passed!\")\n+\n+print(\"\\n\u2705 All tests passed!\")\ndiff --git a/test_sequential_splits.py b/test_sequential_splits.py\nnew file mode 100644\nindex 000000000..cd49b4026\n--- /dev/null\n+++ b/test_sequential_splits.py\n@@ -0,0 +1,19 @@\n+from sklearn.datasets import make_classification\n+from sklearn.feature_selection import SequentialFeatureSelector\n+from sklearn.neighbors import KNeighborsClassifier\n+from sklearn.model_selection import LeaveOneGroupOut\n+\n+import numpy as np\n+\n+X, y = make_classification()\n+\n+groups = np.zeros_like(y, dtype=int)\n+groups[y.size//2:] = 1\n+\n+cv = LeaveOneGroupOut()\n+splits = cv.split(X, y, groups=groups)\n+\n+clf = KNeighborsClassifier(n_neighbors=5)\n+\n+seq = SequentialFeatureSelector(clf, n_features_to_select=5, scoring='accuracy', cv=splits)\n+seq.fit(X, y)\n", "gold_standard_diff": "From 9d4a3562b87e79f309b4f71fdc0d2b4ae9983372 Mon Sep 17 00:00:00 2001\nFrom: Charlie-XIAO <yx2436@nyu.edu>\nDate: Sat, 25 Mar 2023 20:39:21 +0800\nSubject: [PATCH 1/7] checked cv to make sure generators are supported\n\n---\n sklearn/feature_selection/_sequential.py      | 12 ++++----\n .../tests/test_sequential.py                  | 30 +++++++++++++++++--\n 2 files changed, 35 insertions(+), 7 deletions(-)\n\ndiff --git a/sklearn/feature_selection/_sequential.py b/sklearn/feature_selection/_sequential.py\nindex e983c55de7d25..2498cd53b39f6 100644\n--- a/sklearn/feature_selection/_sequential.py\n+++ b/sklearn/feature_selection/_sequential.py\n@@ -8,12 +8,12 @@\n import warnings\n \n from ._base import SelectorMixin\n-from ..base import BaseEstimator, MetaEstimatorMixin, clone\n+from ..base import BaseEstimator, MetaEstimatorMixin, clone, is_classifier\n from ..utils._param_validation import HasMethods, Hidden, Interval, StrOptions\n from ..utils._param_validation import RealNotInt\n from ..utils._tags import _safe_tags\n from ..utils.validation import check_is_fitted\n-from ..model_selection import cross_val_score\n+from ..model_selection import cross_val_score, check_cv\n from ..metrics import get_scorer_names\n \n \n@@ -259,6 +259,8 @@ def fit(self, X, y=None):\n         if self.tol is not None and self.tol < 0 and self.direction == \"forward\":\n             raise ValueError(\"tol must be positive when doing forward selection\")\n \n+        cv = check_cv(self.cv, y, classifier=is_classifier(self.estimator))\n+\n         cloned_estimator = clone(self.estimator)\n \n         # the current mask corresponds to the set of features:\n@@ -275,7 +277,7 @@ def fit(self, X, y=None):\n         is_auto_select = self.tol is not None and self.n_features_to_select == \"auto\"\n         for _ in range(n_iterations):\n             new_feature_idx, new_score = self._get_best_new_feature_score(\n-                cloned_estimator, X, y, current_mask\n+                cloned_estimator, X, y, cv, current_mask\n             )\n             if is_auto_select and ((new_score - old_score) < self.tol):\n                 break\n@@ -291,7 +293,7 @@ def fit(self, X, y=None):\n \n         return self\n \n-    def _get_best_new_feature_score(self, estimator, X, y, current_mask):\n+    def _get_best_new_feature_score(self, estimator, X, y, cv, current_mask):\n         # Return the best new feature and its score to add to the current_mask,\n         # i.e. return the best new feature and its score to add (resp. remove)\n         # when doing forward selection (resp. backward selection).\n@@ -309,7 +311,7 @@ def _get_best_new_feature_score(self, estimator, X, y, current_mask):\n                 estimator,\n                 X_new,\n                 y,\n-                cv=self.cv,\n+                cv=cv,\n                 scoring=self.scoring,\n                 n_jobs=self.n_jobs,\n             ).mean()\ndiff --git a/sklearn/feature_selection/tests/test_sequential.py b/sklearn/feature_selection/tests/test_sequential.py\nindex f6451a36005ac..e99d19d7b4b88 100644\n--- a/sklearn/feature_selection/tests/test_sequential.py\n+++ b/sklearn/feature_selection/tests/test_sequential.py\n@@ -6,11 +6,12 @@\n from sklearn.preprocessing import StandardScaler\n from sklearn.pipeline import make_pipeline\n from sklearn.feature_selection import SequentialFeatureSelector\n-from sklearn.datasets import make_regression, make_blobs\n+from sklearn.datasets import make_regression, make_blobs, make_classification\n from sklearn.linear_model import LinearRegression\n from sklearn.ensemble import HistGradientBoostingRegressor\n-from sklearn.model_selection import cross_val_score\n+from sklearn.model_selection import cross_val_score, LeaveOneGroupOut\n from sklearn.cluster import KMeans\n+from sklearn.neighbors import KNeighborsClassifier\n \n \n def test_bad_n_features_to_select():\n@@ -314,3 +315,28 @@ def test_backward_neg_tol():\n \n     assert 0 < sfs.get_support().sum() < X.shape[1]\n     assert new_score < initial_score\n+\n+\n+def test_cv_generator_support():\n+    \"\"\"Check that we support cv that is a generator.\n+\n+    non-regression test for #25957\n+    \"\"\"\n+    X, y = make_classification()\n+\n+    groups = np.zeros_like(y, dtype=int)\n+    groups[y.size // 2 :] = 1\n+\n+    cv = LeaveOneGroupOut()\n+    splits = cv.split(X, y, groups=groups)\n+\n+    knc = KNeighborsClassifier(n_neighbors=5)\n+\n+    sfs = SequentialFeatureSelector(\n+        knc, n_features_to_select=5, scoring=\"accuracy\", cv=splits\n+    )\n+\n+    try:\n+        sfs.fit(X, y)\n+    except Exception:\n+        pytest.fail(\"Failed to support cv that is a generator\")\n\nFrom c958c880a7e685a5d436d533aaea8529ca2988d5 Mon Sep 17 00:00:00 2001\nFrom: Charlie-XIAO <yx2436@nyu.edu>\nDate: Sat, 25 Mar 2023 21:31:50 +0800\nSubject: [PATCH 2/7] added changelog\n\n---\n doc/whats_new/v1.3.rst | 3 +++\n 1 file changed, 3 insertions(+)\n\ndiff --git a/doc/whats_new/v1.3.rst b/doc/whats_new/v1.3.rst\nindex 51b4214145216..6fd62e9dd1caf 100644\n--- a/doc/whats_new/v1.3.rst\n+++ b/doc/whats_new/v1.3.rst\n@@ -146,6 +146,9 @@ Changelog\n - |Enhancement| All selectors in :mod:`sklearn.feature_selection` will preserve\n   a DataFrame's dtype when transformed. :pr:`25102` by `Thomas Fan`_.\n \n+- |Fix| :class:`feature_selection.SequentialFeatureSelector` now does not throw\n+  IndexError when `cv` is a generator. :pr:`25973` by `Yao Xiao <Charlie-XIAO>`.\n+\n :mod:`sklearn.base`\n ...................\n \n\nFrom 4192f3ad1eb60f791d5f61353c2387d49d7e4c5d Mon Sep 17 00:00:00 2001\nFrom: Charlie-XIAO <yx2436@nyu.edu>\nDate: Sat, 25 Mar 2023 21:34:04 +0800\nSubject: [PATCH 3/7] modified regression test description and errmsg a bit\n\n---\n sklearn/feature_selection/tests/test_sequential.py | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/sklearn/feature_selection/tests/test_sequential.py b/sklearn/feature_selection/tests/test_sequential.py\nindex e99d19d7b4b88..2568ae80b5053 100644\n--- a/sklearn/feature_selection/tests/test_sequential.py\n+++ b/sklearn/feature_selection/tests/test_sequential.py\n@@ -318,7 +318,7 @@ def test_backward_neg_tol():\n \n \n def test_cv_generator_support():\n-    \"\"\"Check that we support cv that is a generator.\n+    \"\"\"Check that we does not throw exception when cv is generator\n \n     non-regression test for #25957\n     \"\"\"\n@@ -339,4 +339,4 @@ def test_cv_generator_support():\n     try:\n         sfs.fit(X, y)\n     except Exception:\n-        pytest.fail(\"Failed to support cv that is a generator\")\n+        pytest.fail(\"Should not throw exception when cv is generator\")\n\nFrom 2141626813206987f7ea1dafab3e2103f158bcf9 Mon Sep 17 00:00:00 2001\nFrom: Charlie-XIAO <yx2436@nyu.edu>\nDate: Sat, 25 Mar 2023 22:43:04 +0800\nSubject: [PATCH 4/7] removed lines uncovered by test\n\n---\n sklearn/feature_selection/tests/test_sequential.py | 8 ++------\n 1 file changed, 2 insertions(+), 6 deletions(-)\n\ndiff --git a/sklearn/feature_selection/tests/test_sequential.py b/sklearn/feature_selection/tests/test_sequential.py\nindex 2568ae80b5053..02208731cb977 100644\n--- a/sklearn/feature_selection/tests/test_sequential.py\n+++ b/sklearn/feature_selection/tests/test_sequential.py\n@@ -318,7 +318,7 @@ def test_backward_neg_tol():\n \n \n def test_cv_generator_support():\n-    \"\"\"Check that we does not throw exception when cv is generator\n+    \"\"\"Check that no exception raised when cv is generator\n \n     non-regression test for #25957\n     \"\"\"\n@@ -335,8 +335,4 @@ def test_cv_generator_support():\n     sfs = SequentialFeatureSelector(\n         knc, n_features_to_select=5, scoring=\"accuracy\", cv=splits\n     )\n-\n-    try:\n-        sfs.fit(X, y)\n-    except Exception:\n-        pytest.fail(\"Should not throw exception when cv is generator\")\n+    sfs.fit(X, y)\n\nFrom 4515c23c71a77bd1de8683c1a8d36b523ff27bc9 Mon Sep 17 00:00:00 2001\nFrom: Charlie-XIAO <yx2436@nyu.edu>\nDate: Tue, 28 Mar 2023 23:33:12 +0800\nSubject: [PATCH 5/7] resolved conversations\n\n---\n sklearn/feature_selection/tests/test_sequential.py | 8 +++-----\n 1 file changed, 3 insertions(+), 5 deletions(-)\n\ndiff --git a/sklearn/feature_selection/tests/test_sequential.py b/sklearn/feature_selection/tests/test_sequential.py\nindex 02208731cb977..7c02d32696178 100644\n--- a/sklearn/feature_selection/tests/test_sequential.py\n+++ b/sklearn/feature_selection/tests/test_sequential.py\n@@ -322,7 +322,7 @@ def test_cv_generator_support():\n \n     non-regression test for #25957\n     \"\"\"\n-    X, y = make_classification()\n+    X, y = make_classification(random_state=0)\n \n     groups = np.zeros_like(y, dtype=int)\n     groups[y.size // 2 :] = 1\n@@ -330,9 +330,7 @@ def test_cv_generator_support():\n     cv = LeaveOneGroupOut()\n     splits = cv.split(X, y, groups=groups)\n \n-    knc = KNeighborsClassifier(n_neighbors=5)\n+    knc = KNeighborsClassifier(n_neighbors=5, random_state=0)\n \n-    sfs = SequentialFeatureSelector(\n-        knc, n_features_to_select=5, scoring=\"accuracy\", cv=splits\n-    )\n+    sfs = SequentialFeatureSelector(knc, n_features_to_select=5, cv=splits)\n     sfs.fit(X, y)\n\nFrom 591e19e711d1d2fbc4ac8ccb36210495fc75f7c0 Mon Sep 17 00:00:00 2001\nFrom: Charlie-XIAO <yx2436@nyu.edu>\nDate: Wed, 29 Mar 2023 01:29:48 +0800\nSubject: [PATCH 6/7] test random_state=0 removed: knc does not take random\n state\n\n---\n sklearn/feature_selection/tests/test_sequential.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/sklearn/feature_selection/tests/test_sequential.py b/sklearn/feature_selection/tests/test_sequential.py\nindex 7c02d32696178..98134addc39e7 100644\n--- a/sklearn/feature_selection/tests/test_sequential.py\n+++ b/sklearn/feature_selection/tests/test_sequential.py\n@@ -330,7 +330,7 @@ def test_cv_generator_support():\n     cv = LeaveOneGroupOut()\n     splits = cv.split(X, y, groups=groups)\n \n-    knc = KNeighborsClassifier(n_neighbors=5, random_state=0)\n+    knc = KNeighborsClassifier(n_neighbors=5)\n \n     sfs = SequentialFeatureSelector(knc, n_features_to_select=5, cv=splits)\n     sfs.fit(X, y)\n\nFrom e6c6eea1872ee42ec6d0bc290ed7ae0bedd81b12 Mon Sep 17 00:00:00 2001\nFrom: Charlie-XIAO <yx2436@nyu.edu>\nDate: Wed, 29 Mar 2023 04:05:23 +0800\nSubject: [PATCH 7/7] modified changelog\n\n---\n doc/whats_new/v1.3.rst | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/doc/whats_new/v1.3.rst b/doc/whats_new/v1.3.rst\nindex 6fd62e9dd1caf..add8f896dcb1a 100644\n--- a/doc/whats_new/v1.3.rst\n+++ b/doc/whats_new/v1.3.rst\n@@ -146,8 +146,8 @@ Changelog\n - |Enhancement| All selectors in :mod:`sklearn.feature_selection` will preserve\n   a DataFrame's dtype when transformed. :pr:`25102` by `Thomas Fan`_.\n \n-- |Fix| :class:`feature_selection.SequentialFeatureSelector` now does not throw\n-  IndexError when `cv` is a generator. :pr:`25973` by `Yao Xiao <Charlie-XIAO>`.\n+- |Fix| :class:`feature_selection.SequentialFeatureSelector`'s `cv` parameter\n+  now supports generators. :pr:`25973` by `Yao Xiao <Charlie-XIAO>`.\n \n :mod:`sklearn.base`\n ...................\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/25973"}