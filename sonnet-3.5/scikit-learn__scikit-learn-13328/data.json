{"instance_id": "scikit-learn__scikit-learn-13328", "score": "I", "diff": "diff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py\nindex 65c686400..ae2d52985 100644\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -48,6 +48,7 @@ def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n         Returns the derivative of the Huber loss with respect to each\n         coefficient, intercept and the scale as a vector.\n     \"\"\"\n+    X = np.asarray(X, dtype=float)\n     _, n_features = X.shape\n     fit_intercept = (n_features + 2 == w.shape[0])\n     if fit_intercept:\n@@ -251,7 +252,7 @@ class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):\n         self : object\n         \"\"\"\n         X, y = check_X_y(\n-            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)\n+            X, y, copy=False, accept_sparse=['csr'], y_numeric=True, dtype=float)\n         if sample_weight is not None:\n             sample_weight = np.array(sample_weight)\n             check_consistent_length(y, sample_weight)\n", "gold_standard_diff": "From cff2c4f9ea5e8802b054fb47b58dc8ab2b489f41 Mon Sep 17 00:00:00 2001\nFrom: Alexandre Gramfort <alexandre.gramfort@m4x.org>\nDate: Thu, 28 Feb 2019 13:44:40 +0100\nSubject: [PATCH 1/5] make sure huber works with boolean X\n\n---\n sklearn/linear_model/huber.py            | 3 ++-\n sklearn/linear_model/tests/test_huber.py | 7 +++++++\n 2 files changed, 9 insertions(+), 1 deletion(-)\n\ndiff --git a/sklearn/linear_model/huber.py b/sklearn/linear_model/huber.py\nindex cd17b0fe33c00..676c5f02124dc 100644\n--- a/sklearn/linear_model/huber.py\n+++ b/sklearn/linear_model/huber.py\n@@ -251,7 +251,8 @@ def fit(self, X, y, sample_weight=None):\n         self : object\n         \"\"\"\n         X, y = check_X_y(\n-            X, y, copy=False, accept_sparse=['csr'], y_numeric=True)\n+            X, y, copy=False, accept_sparse=['csr'], y_numeric=True,\n+            dtype=[np.float64, np.float32])\n         if sample_weight is not None:\n             sample_weight = np.array(sample_weight)\n             check_consistent_length(y, sample_weight)\ndiff --git a/sklearn/linear_model/tests/test_huber.py b/sklearn/linear_model/tests/test_huber.py\nindex 6a8b26133d5ac..9d9955dd6153b 100644\n--- a/sklearn/linear_model/tests/test_huber.py\n+++ b/sklearn/linear_model/tests/test_huber.py\n@@ -199,3 +199,10 @@ def test_huber_better_r2_score():\n \n     # The huber model should also fit poorly on the outliers.\n     assert_greater(ridge_outlier_score, huber_outlier_score)\n+\n+\n+def test_huber_bool():\n+    # Test that it does not crash with bool data\n+    X, y = make_regression(n_samples=200, n_features=2, noise=4.0, random_state=0)\n+    X_bool = X > 0\n+    huber = HuberRegressor().fit(X_bool, y)\n\nFrom b4363abb9450943ce2a27ba1d37f994b4965c31c Mon Sep 17 00:00:00 2001\nFrom: Alexandre Gramfort <alexandre.gramfort@m4x.org>\nDate: Thu, 28 Feb 2019 13:46:48 +0100\nSubject: [PATCH 2/5] update what's new\n\n---\n doc/whats_new/v0.21.rst | 7 ++++++-\n 1 file changed, 6 insertions(+), 1 deletion(-)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex e3e3ec9f88816..9518baf8816ea 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -42,7 +42,7 @@ Support for Python 3.4 and below has been officially dropped.\n     See version doc/whats_new/v0.20.rst for structure. Entries should be\n     prefixed with one of the labels: |MajorFeature|, |Feature|, |Efficiency|,\n     |Enhancement|, |Fix| or |API|. They should be under a heading for the\n-    relevant module (or *Multiple Modules* or *Miscellaneous*), and within each\n+    relevant module (or *Mufltiple Modules* or *Miscellaneous*), and within each\n     section should be ordered according to the label ordering above. Entries\n     should end with: :issue:`123456` by :user:`Joe Bloggs <joeongithub>`.\n \n@@ -234,6 +234,11 @@ Support for Python 3.4 and below has been officially dropped.\n   now supports fitting the intercept (i.e. ``fit_intercept=True``) when\n   inputs are sparse . :issue:`13336` by :user:`Bartosz Telenczuk <btel>`\n \n+- |Fix| Fixed a bug in :class:`linear_model.HuberRegressor` that was\n+  broken when X was of dtype bool.\n+  :issue:`13314` by `Alexandre Gramfort`_.\n+\n+\n :mod:`sklearn.manifold`\n ............................\n \n\nFrom 48a7ccae3b477d036ab605e983d4a5efa71c3c6c Mon Sep 17 00:00:00 2001\nFrom: Alexandre Gramfort <alexandre.gramfort@m4x.org>\nDate: Thu, 28 Feb 2019 21:22:37 +0100\nSubject: [PATCH 3/5] lint\n\n---\n sklearn/linear_model/tests/test_huber.py | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n\ndiff --git a/sklearn/linear_model/tests/test_huber.py b/sklearn/linear_model/tests/test_huber.py\nindex 9d9955dd6153b..58f983cf75b3a 100644\n--- a/sklearn/linear_model/tests/test_huber.py\n+++ b/sklearn/linear_model/tests/test_huber.py\n@@ -205,4 +205,4 @@ def test_huber_bool():\n     # Test that it does not crash with bool data\n     X, y = make_regression(n_samples=200, n_features=2, noise=4.0, random_state=0)\n     X_bool = X > 0\n-    huber = HuberRegressor().fit(X_bool, y)\n+    HuberRegressor().fit(X_bool, y)\n\nFrom 83e134f4f776dc8ad1ad099c84f6ffa4e95fbe93 Mon Sep 17 00:00:00 2001\nFrom: Alexandre Gramfort <alexandre.gramfort@m4x.org>\nDate: Fri, 1 Mar 2019 16:35:17 +0100\nSubject: [PATCH 4/5] review\n\n---\n doc/whats_new/v0.21.rst | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex 9518baf8816ea..ed5f270d441b8 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -42,7 +42,7 @@ Support for Python 3.4 and below has been officially dropped.\n     See version doc/whats_new/v0.20.rst for structure. Entries should be\n     prefixed with one of the labels: |MajorFeature|, |Feature|, |Efficiency|,\n     |Enhancement|, |Fix| or |API|. They should be under a heading for the\n-    relevant module (or *Mufltiple Modules* or *Miscellaneous*), and within each\n+    relevant module (or *Multiple Modules* or *Miscellaneous*), and within each\n     section should be ordered according to the label ordering above. Entries\n     should end with: :issue:`123456` by :user:`Joe Bloggs <joeongithub>`.\n \n@@ -236,7 +236,7 @@ Support for Python 3.4 and below has been officially dropped.\n \n - |Fix| Fixed a bug in :class:`linear_model.HuberRegressor` that was\n   broken when X was of dtype bool.\n-  :issue:`13314` by `Alexandre Gramfort`_.\n+  :issue:`13328` by `Alexandre Gramfort`_.\n \n \n :mod:`sklearn.manifold`\n\nFrom 07d1ac5798faa720cccba6c1fe84620d4115b0c3 Mon Sep 17 00:00:00 2001\nFrom: Alexandre Gramfort <alexandre.gramfort@m4x.org>\nDate: Sat, 2 Mar 2019 08:52:46 +0100\nSubject: [PATCH 5/5] pep8\n\n---\n sklearn/linear_model/tests/test_huber.py | 25 +++++++++++++++---------\n 1 file changed, 16 insertions(+), 9 deletions(-)\n\ndiff --git a/sklearn/linear_model/tests/test_huber.py b/sklearn/linear_model/tests/test_huber.py\nindex 58f983cf75b3a..156ac72958d01 100644\n--- a/sklearn/linear_model/tests/test_huber.py\n+++ b/sklearn/linear_model/tests/test_huber.py\n@@ -53,8 +53,12 @@ def test_huber_gradient():\n     rng = np.random.RandomState(1)\n     X, y = make_regression_with_outliers()\n     sample_weight = rng.randint(1, 3, (y.shape[0]))\n-    loss_func = lambda x, *args: _huber_loss_and_gradient(x, *args)[0]\n-    grad_func = lambda x, *args: _huber_loss_and_gradient(x, *args)[1]\n+\n+    def loss_func(x, *args):\n+        return _huber_loss_and_gradient(x, *args)[0]\n+\n+    def grad_func(x, *args):\n+        return _huber_loss_and_gradient(x, *args)[1]\n \n     # Check using optimize.check_grad that the gradients are equal.\n     for _ in range(5):\n@@ -76,10 +80,10 @@ def test_huber_sample_weights():\n     huber_coef = huber.coef_\n     huber_intercept = huber.intercept_\n \n-    # Rescale coefs before comparing with assert_array_almost_equal to make sure\n-    # that the number of decimal places used is somewhat insensitive to the\n-    # amplitude of the coefficients and therefore to the scale of the data\n-    # and the regularization parameter\n+    # Rescale coefs before comparing with assert_array_almost_equal to make\n+    # sure that the number of decimal places used is somewhat insensitive to\n+    # the amplitude of the coefficients and therefore to the scale of the\n+    # data and the regularization parameter\n     scale = max(np.mean(np.abs(huber.coef_)),\n                 np.mean(np.abs(huber.intercept_)))\n \n@@ -167,7 +171,8 @@ def test_huber_and_sgd_same_results():\n def test_huber_warm_start():\n     X, y = make_regression_with_outliers()\n     huber_warm = HuberRegressor(\n-        fit_intercept=True, alpha=1.0, max_iter=10000, warm_start=True, tol=1e-1)\n+        fit_intercept=True, alpha=1.0, max_iter=10000, warm_start=True,\n+        tol=1e-1)\n     huber_warm.fit(X, y)\n     huber_warm_coef = huber_warm.coef_.copy()\n     huber_warm.fit(X, y)\n@@ -190,7 +195,8 @@ def test_huber_better_r2_score():\n     huber_outlier_score = huber.score(X[~mask], y[~mask])\n \n     # The Ridge regressor should be influenced by the outliers and hence\n-    # give a worse score on the non-outliers as compared to the huber regressor.\n+    # give a worse score on the non-outliers as compared to the huber\n+    # regressor.\n     ridge = Ridge(fit_intercept=True, alpha=0.01)\n     ridge.fit(X, y)\n     ridge_score = ridge.score(X[mask], y[mask])\n@@ -203,6 +209,7 @@ def test_huber_better_r2_score():\n \n def test_huber_bool():\n     # Test that it does not crash with bool data\n-    X, y = make_regression(n_samples=200, n_features=2, noise=4.0, random_state=0)\n+    X, y = make_regression(n_samples=200, n_features=2, noise=4.0,\n+                           random_state=0)\n     X_bool = X > 0\n     HuberRegressor().fit(X_bool, y)\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/13328"}