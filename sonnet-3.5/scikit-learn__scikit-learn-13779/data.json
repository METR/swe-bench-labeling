{"instance_id": "scikit-learn__scikit-learn-13779", "score": "I", "diff": "diff --git a/sklearn/ensemble/voting.py b/sklearn/ensemble/voting.py\nindex 7afa7180c..4374cf43b 100644\n--- a/sklearn/ensemble/voting.py\n+++ b/sklearn/ensemble/voting.py\n@@ -53,8 +53,7 @@ class _BaseVoting(_BaseComposition, TransformerMixin):\n         \"\"\"Get the weights of not `None` estimators\"\"\"\n         if self.weights is None:\n             return None\n-        return [w for est, w in zip(self.estimators,\n-                                    self.weights) if est[1] is not None]\n+        return [w for (_, est), w in zip(self.estimators, self.weights) if est is not None]\n \n     def _predict(self, X):\n         \"\"\"Collect results from clf.predict calls. \"\"\"\n@@ -70,6 +69,9 @@ class _BaseVoting(_BaseComposition, TransformerMixin):\n                                  ' should be a list of (string, estimator)'\n                                  ' tuples')\n \n+        # Filter out None estimators\n+        self.estimators = [(name, est) for name, est in self.estimators if est is not None]\n+\n         if (self.weights is not None and\n                 len(self.weights) != len(self.estimators)):\n             raise ValueError('Number of `estimators` and weights must be equal'\n@@ -85,19 +87,18 @@ class _BaseVoting(_BaseComposition, TransformerMixin):\n         names, clfs = zip(*self.estimators)\n         self._validate_names(names)\n \n-        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n-        if n_isnone == len(self.estimators):\n+        if len(self.estimators) == 0:\n             raise ValueError('All estimators are None. At least one is '\n                              'required!')\n \n         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                 delayed(_parallel_fit_estimator)(clone(clf), X, y,\n                                                  sample_weight=sample_weight)\n-                for clf in clfs if clf is not None)\n+                for _, clf in self.estimators)\n \n         self.named_estimators_ = Bunch()\n-        for k, e in zip(self.estimators, self.estimators_):\n-            self.named_estimators_[k[0]] = e\n+        for (name, _), fitted_est in zip(self.estimators, self.estimators_):\n+            self.named_estimators_[name] = fitted_est\n         return self\n \n     def set_params(self, **params):\n@@ -270,6 +271,9 @@ class VotingClassifier(_BaseVoting, ClassifierMixin):\n         self.classes_ = self.le_.classes_\n         transformed_y = self.le_.transform(y)\n \n+        # Filter out None estimators\n+        self.estimators = [(name, est) for name, est in self.estimators if est is not None]\n+\n         return super().fit(X, transformed_y, sample_weight)\n \n     def predict(self, X):\n", "gold_standard_diff": "diff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex 473bbf91878d5..3dcb8b31773c7 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -319,6 +319,11 @@ Support for Python 3.4 and below has been officially dropped.\n   :pr:`12599` by :user:`Trevor Stephens<trevorstephens>` and\n   :user:`Nicolas Hug<NicolasHug>`.\n \n+- |Fix| :class:`ensemble.VotingClassifier` and\n+  :class:`ensemble.VotingRegressor` were failing during ``fit`` in one\n+  of the estimators was set to ``None`` and ``sample_weight`` was not ``None``.\n+  :pr:`13779` by :user:`Guillaume Lemaitre <glemaitre>`.\n+\n :mod:`sklearn.externals`\n ........................\n \ndiff --git a/sklearn/ensemble/tests/test_voting.py b/sklearn/ensemble/tests/test_voting.py\nindex 2a19bc9a64dc0..0a4a9eab8c160 100644\n--- a/sklearn/ensemble/tests/test_voting.py\n+++ b/sklearn/ensemble/tests/test_voting.py\n@@ -8,9 +8,11 @@\n from sklearn.utils.testing import assert_equal\n from sklearn.utils.testing import assert_raise_message\n from sklearn.exceptions import NotFittedError\n+from sklearn.linear_model import LinearRegression\n from sklearn.linear_model import LogisticRegression\n from sklearn.naive_bayes import GaussianNB\n from sklearn.ensemble import RandomForestClassifier\n+from sklearn.ensemble import RandomForestRegressor\n from sklearn.ensemble import VotingClassifier, VotingRegressor\n from sklearn.model_selection import GridSearchCV\n from sklearn import datasets\n@@ -507,3 +509,25 @@ def test_transform():\n             eclf3.transform(X).swapaxes(0, 1).reshape((4, 6)),\n             eclf2.transform(X)\n     )\n+\n+\n+@pytest.mark.filterwarnings('ignore: Default solver will be changed')  # 0.22\n+@pytest.mark.filterwarnings('ignore: Default multi_class will')  # 0.22\n+@pytest.mark.parametrize(\n+    \"X, y, voter\",\n+    [(X, y, VotingClassifier(\n+        [('lr', LogisticRegression()),\n+         ('rf', RandomForestClassifier(n_estimators=5))])),\n+     (X_r, y_r, VotingRegressor(\n+         [('lr', LinearRegression()),\n+          ('rf', RandomForestRegressor(n_estimators=5))]))]\n+)\n+def test_none_estimator_with_weights(X, y, voter):\n+    # check that an estimator can be set to None and passing some weight\n+    # regression test for\n+    # https://github.com/scikit-learn/scikit-learn/issues/13777\n+    voter.fit(X, y, sample_weight=np.ones(y.shape))\n+    voter.set_params(lr=None)\n+    voter.fit(X, y, sample_weight=np.ones(y.shape))\n+    y_pred = voter.predict(X)\n+    assert y_pred.shape == y.shape\ndiff --git a/sklearn/ensemble/voting.py b/sklearn/ensemble/voting.py\nindex 7afa7180cc5f3..d8e14b152d3ab 100644\n--- a/sklearn/ensemble/voting.py\n+++ b/sklearn/ensemble/voting.py\n@@ -78,6 +78,8 @@ def fit(self, X, y, sample_weight=None):\n \n         if sample_weight is not None:\n             for name, step in self.estimators:\n+                if step is None:\n+                    continue\n                 if not has_fit_parameter(step, 'sample_weight'):\n                     raise ValueError('Underlying estimator \\'%s\\' does not'\n                                      ' support sample weights.' % name)\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/13779"}