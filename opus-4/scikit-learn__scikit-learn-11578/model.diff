diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index e4ea696ce..0e3dd63ae 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -922,7 +922,13 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         check_input=False, max_squared_sum=max_squared_sum,
         sample_weight=sample_weight)
 
-    log_reg = LogisticRegression(fit_intercept=fit_intercept)
+    log_reg = LogisticRegression(penalty=penalty, dual=dual, tol=tol, C=1.,
+                                 fit_intercept=fit_intercept,
+                                 intercept_scaling=intercept_scaling,
+                                 class_weight=class_weight,
+                                 random_state=random_state, solver=solver,
+                                 max_iter=max_iter, multi_class=multi_class,
+                                 verbose=verbose)
 
     # The score method of Logistic Regression has a classes_ attribute.
     if multi_class == 'ovr':
diff --git a/test_comprehensive.py b/test_comprehensive.py
new file mode 100644
index 000000000..8879f48b3
--- /dev/null
+++ b/test_comprehensive.py
@@ -0,0 +1,117 @@
+import numpy as np
+from sklearn import preprocessing, linear_model, utils
+from sklearn.model_selection import train_test_split
+from sklearn.metrics import log_loss
+
+def test_log_reg_scoring_path_multinomial():
+    """Test that _log_reg_scoring_path respects multi_class parameter"""
+    np.random.seed(42)
+    
+    # Generate multi-class data
+    n_samples, n_features = 200, 5
+    n_classes = 3
+    X = np.random.randn(n_samples, n_features)
+    y = np.random.choice(n_classes, size=n_samples)
+    
+    # Split data
+    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
+    train_idx = np.arange(len(X_train))
+    test_idx = np.arange(len(X_train), len(X_train) + len(X_test))
+    
+    # Concatenate back for the function
+    X_full = np.vstack([X_train, X_test])
+    y_full = np.hstack([y_train, y_test])
+    
+    # Test with multi_class='multinomial'
+    coefs, Cs, scores, n_iter = linear_model.logistic._log_reg_scoring_path(
+        X_full, y_full, train_idx, test_idx, 
+        scoring='neg_log_loss', 
+        multi_class='multinomial',
+        solver='lbfgs',
+        fit_intercept=True
+    )
+    
+    # Manually calculate what the score should be
+    # Use the first C value
+    coef = coefs[0]
+    
+    # Create a LogisticRegression instance with multinomial
+    lr = linear_model.LogisticRegression(
+        multi_class='multinomial', 
+        solver='lbfgs',
+        fit_intercept=True
+    )
+    lr.classes_ = np.unique(y_train)
+    lr.coef_ = coef[:, :-1]
+    lr.intercept_ = coef[:, -1]
+    
+    # Get probabilities using the multinomial approach
+    y_pred_proba = lr.predict_proba(X_test)
+    expected_score = -log_loss(y_test, y_pred_proba)
+    
+    actual_score = scores[0]
+    
+    print(f"Expected score (multinomial): {expected_score}")
+    print(f"Actual score from _log_reg_scoring_path: {actual_score}")
+    print(f"Difference: {abs(expected_score - actual_score)}")
+    print(f"Scores match: {np.allclose(expected_score, actual_score)}")
+    
+    # Also test with multi_class='ovr' to show the difference
+    coefs_ovr, Cs_ovr, scores_ovr, n_iter_ovr = linear_model.logistic._log_reg_scoring_path(
+        X_full, y_full, train_idx, test_idx, 
+        scoring='neg_log_loss', 
+        multi_class='ovr',
+        solver='lbfgs',
+        fit_intercept=True
+    )
+    
+    print(f"\nOvR score: {scores_ovr[0]}")
+    print(f"Multinomial score: {scores[0]}")
+    print(f"OvR and multinomial give different scores: {not np.allclose(scores[0], scores_ovr[0])}")
+
+def test_other_parameters_passed():
+    """Test that other parameters are correctly passed to LogisticRegression"""
+    np.random.seed(42)
+    
+    # Generate binary data
+    n_samples, n_features = 100, 3
+    X = np.random.randn(n_samples, n_features)
+    y = np.random.choice([0, 1], size=n_samples)
+    
+    # Split data
+    train_idx = np.arange(80)
+    test_idx = np.arange(80, 100)
+    
+    # Test with various parameters
+    params = {
+        'penalty': 'l2',
+        'dual': False,
+        'tol': 1e-3,
+        'solver': 'lbfgs',
+        'max_iter': 50,
+        'verbose': 0,
+        'random_state': 42,
+        'intercept_scaling': 2.0,
+        'class_weight': {0: 1, 1: 2}
+    }
+    
+    coefs, Cs, scores, n_iter = linear_model.logistic._log_reg_scoring_path(
+        X, y, train_idx, test_idx, 
+        scoring=None,  # Use default accuracy scorer
+        multi_class='ovr',
+        fit_intercept=True,
+        **params
+    )
+    
+    print("\nParameter passing test completed successfully!")
+    print(f"Shape of coefficients: {coefs[0].shape}")
+    print(f"Number of iterations: {n_iter[0]}")
+
+if __name__ == "__main__":
+    print("Testing multi_class parameter handling...")
+    test_log_reg_scoring_path_multinomial()
+    
+    print("\n" + "="*60 + "\n")
+    
+    print("Testing other parameters are passed correctly...")
+    test_other_parameters_passed()
diff --git a/test_fix.py b/test_fix.py
new file mode 100644
index 000000000..dbeeb468d
--- /dev/null
+++ b/test_fix.py
@@ -0,0 +1,62 @@
+import numpy as np
+from sklearn import preprocessing, linear_model, utils
+
+def ovr_approach(decision_function):
+    probs = 1. / (1. + np.exp(-decision_function))
+    probs = probs / probs.sum(axis=1).reshape((probs.shape[0], -1))
+    return probs
+
+def score_from_probs(probs, y_bin):
+    return (y_bin*np.log(probs)).sum(axis=1).mean()
+
+np.random.seed(seed=1234)
+
+samples  = 200
+features = 5
+folds    = 10
+
+# Use a "probabilistic" scorer
+scorer = 'neg_log_loss'
+
+x = np.random.random(size=(samples, features))
+y = np.random.choice(['a', 'b', 'c'], size=samples)
+
+test  = np.random.choice(range(samples), size=int(samples/float(folds)), replace=False)
+train = [idx for idx in range(samples) if idx not in test]
+
+# Binarize the labels for y[test]
+lb = preprocessing.label.LabelBinarizer()
+lb.fit(y[test])
+y_bin = lb.transform(y[test])
+
+# What does _log_reg_scoring_path give us for the score?
+coefs, _, scores, _ = linear_model.logistic._log_reg_scoring_path(x, y, train, test, fit_intercept=True, scoring=scorer, multi_class='multinomial')
+
+# Choose a single C to look at, for simplicity
+c_index = 0
+coefs = coefs[c_index]
+scores = scores[c_index]
+
+# Initialise a LogisticRegression() instance with multi_class='multinomial'
+log_reg = linear_model.LogisticRegression(fit_intercept=True, multi_class='multinomial')
+log_reg.coef_      = coefs[:, :-1]
+log_reg.intercept_ = coefs[:, -1]
+log_reg.classes_ = np.unique(y[train])
+
+dec_fn = log_reg.decision_function(x[test])
+probs_builtin = log_reg.predict_proba(x[test])
+
+# multinomial approach
+probs_multi = utils.extmath.softmax(dec_fn)
+
+print('score returned by _log_reg_scoring_path')
+print(scores)
+
+print('score calculated via multinomial method')
+print(score_from_probs(probs_multi, y_bin))
+
+print('Difference between _log_reg_scoring_path score and multinomial score:')
+print(abs(scores - score_from_probs(probs_multi, y_bin)))
+
+print('\nAre the scores now equal (within floating point precision)?')
+print(np.allclose(scores, score_from_probs(probs_multi, y_bin)))
diff --git a/test_issue.py b/test_issue.py
new file mode 100644
index 000000000..c45c14498
--- /dev/null
+++ b/test_issue.py
@@ -0,0 +1,110 @@
+import numpy as np
+from sklearn import preprocessing, linear_model, utils
+
+def ovr_approach(decision_function):
+    
+    probs = 1. / (1. + np.exp(-decision_function))
+    probs = probs / probs.sum(axis=1).reshape((probs.shape[0], -1))
+    
+    return probs
+
+def score_from_probs(probs, y_bin):
+    
+    return (y_bin*np.log(probs)).sum(axis=1).mean()
+    
+    
+np.random.seed(seed=1234)
+
+samples  = 200
+features = 5
+folds    = 10
+
+# Use a "probabilistic" scorer
+scorer = 'neg_log_loss'
+
+x = np.random.random(size=(samples, features))
+y = np.random.choice(['a', 'b', 'c'], size=samples)
+
+test  = np.random.choice(range(samples), size=int(samples/float(folds)), replace=False)
+train = [idx for idx in range(samples) if idx not in test]
+
+# Binarize the labels for y[test]
+lb = preprocessing.label.LabelBinarizer()
+lb.fit(y[test])
+y_bin = lb.transform(y[test])
+
+# What does _log_reg_scoring_path give us for the score?
+coefs, _, scores, _ = linear_model.logistic._log_reg_scoring_path(x, y, train, test, fit_intercept=True, scoring=scorer, multi_class='multinomial')
+
+# Choose a single C to look at, for simplicity
+c_index = 0
+coefs = coefs[c_index]
+scores = scores[c_index]
+
+# Initialise a LogisticRegression() instance, as in 
+# https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/logistic.py#L922
+existing_log_reg = linear_model.LogisticRegression(fit_intercept=True)
+existing_log_reg.coef_      = coefs[:, :-1]
+existing_log_reg.intercept_ = coefs[:, -1]
+
+existing_dec_fn = existing_log_reg.decision_function(x[test])
+
+existing_probs_builtin = existing_log_reg.predict_proba(x[test])
+
+# OvR approach
+existing_probs_ovr = ovr_approach(existing_dec_fn)
+
+# multinomial approach
+existing_probs_multi = utils.extmath.softmax(existing_dec_fn)
+
+# If we initialise our LogisticRegression() instance, with multi_class='multinomial'
+new_log_reg = linear_model.LogisticRegression(fit_intercept=True, multi_class='multinomial')
+new_log_reg.coef_      = coefs[:, :-1]
+new_log_reg.intercept_ = coefs[:, -1]
+
+new_dec_fn = new_log_reg.decision_function(x[test])
+
+new_probs_builtin = new_log_reg.predict_proba(x[test])
+
+# OvR approach
+new_probs_ovr = ovr_approach(new_dec_fn)
+
+# multinomial approach
+new_probs_multi = utils.extmath.softmax(new_dec_fn)
+
+print('score returned by _log_reg_scoring_path')
+print(scores)
+# -1.10566998
+
+print('OvR LR decision function == multinomial LR decision function?')
+print((existing_dec_fn == new_dec_fn).all())
+# True
+
+print('score calculated via OvR method (either decision function)')
+print(score_from_probs(existing_probs_ovr, y_bin))
+# -1.10566997908
+
+print('score calculated via multinomial method (either decision function)')
+print(score_from_probs(existing_probs_multi, y_bin))
+# -1.11426297223
+
+print('probs predicted by existing_log_reg.predict_proba() == probs generated via the OvR approach?')
+print((existing_probs_builtin == existing_probs_ovr).all())
+# True
+
+print('probs predicted by existing_log_reg.predict_proba() == probs generated via the multinomial approach?')
+print((existing_probs_builtin == existing_probs_multi).any())
+# False
+
+print('probs predicted by new_log_reg.predict_proba() == probs generated via the OvR approach?')
+print((new_probs_builtin == new_probs_ovr).all())
+# False
+
+print('probs predicted by new_log_reg.predict_proba() == probs generated via the multinomial approach?')
+print((new_probs_builtin == new_probs_multi).any())
+# True
+
+# So even though multi_class='multinomial' was specified in _log_reg_scoring_path(), 
+# the score it returned was the score calculated via OvR, not multinomial.
+# We can see that log_reg.predict_proba() returns the OvR predicted probabilities,
+# not the multinomial predicted probabilities.
diff --git a/test_neg_log_loss_multinomial.py b/test_neg_log_loss_multinomial.py
new file mode 100644
index 000000000..2a9d5aace
--- /dev/null
+++ b/test_neg_log_loss_multinomial.py
@@ -0,0 +1,68 @@
+import numpy as np
+from sklearn.linear_model import LogisticRegressionCV
+from sklearn.metrics import log_loss
+from sklearn.model_selection import StratifiedKFold
+
+def test_logreg_cv_multinomial_scorer():
+    """Test that LogisticRegressionCV with scoring='neg_log_loss' and 
+    multi_class='multinomial' uses multinomial probabilities, not OvR"""
+    
+    # Generate multi-class data
+    np.random.seed(42)
+    n_samples, n_features, n_classes = 200, 5, 3
+    X = np.random.randn(n_samples, n_features)
+    y = np.random.choice(n_classes, size=n_samples)
+    
+    # Fit LogisticRegressionCV with multinomial
+    lr_cv = LogisticRegressionCV(
+        cv=3, 
+        multi_class='multinomial', 
+        solver='lbfgs',
+        scoring='neg_log_loss',
+        Cs=[1.0]  # Single C value for simplicity
+    )
+    lr_cv.fit(X, y)
+    
+    # Get the best score from CV
+    cv_score = lr_cv.scores_[1].mean(axis=0)[0]  # class 1, mean over folds, first C
+    
+    # Manually compute what the score should be using multinomial
+    # Use the same CV splits
+    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=None)
+    manual_scores = []
+    
+    for train_idx, test_idx in skf.split(X, y):
+        X_train, X_test = X[train_idx], X[test_idx]
+        y_train, y_test = y[train_idx], y[test_idx]
+        
+        # Fit a regular LogisticRegression with same parameters
+        from sklearn.linear_model import LogisticRegression
+        lr = LogisticRegression(
+            C=1.0,
+            multi_class='multinomial',
+            solver='lbfgs',
+            random_state=None,
+            max_iter=100
+        )
+        lr.fit(X_train, y_train)
+        
+        # Get multinomial probabilities
+        y_pred_proba = lr.predict_proba(X_test)
+        
+        # Calculate neg_log_loss
+        score = -log_loss(y_test, y_pred_proba)
+        manual_scores.append(score)
+    
+    manual_score = np.mean(manual_scores)
+    
+    print(f"CV score from LogisticRegressionCV: {cv_score}")
+    print(f"Manual score using multinomial: {manual_score}")
+    print(f"Difference: {abs(cv_score - manual_score)}")
+    
+    # The scores should be close (allowing for some randomness in CV splits)
+    assert abs(cv_score - manual_score) < 0.1, \
+        f"LogisticRegressionCV score {cv_score} differs too much from expected {manual_score}"
+
+if __name__ == "__main__":
+    test_logreg_cv_multinomial_scorer()
+    print("Test passed!")
