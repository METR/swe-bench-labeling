{"instance_id": "pytest-dev__pytest-7490", "score": "I", "diff": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..db6eae06f 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -199,7 +199,14 @@ class Xfail:\n \n def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n     \"\"\"Evaluate xfail marks on item, returning Xfail if triggered.\"\"\"\n-    for mark in item.iter_markers(name=\"xfail\"):\n+    xfail_marks = list(item.iter_markers(name=\"xfail\"))\n+    \n+    # Check for dynamically added xfail markers\n+    if hasattr(item, 'own_markers'):\n+        dynamic_marks = [mark for mark in item.own_markers if mark.name == 'xfail']\n+        xfail_marks.extend(dynamic_marks)\n+    \n+    for mark in xfail_marks:\n         run = mark.kwargs.get(\"run\", True)\n         strict = mark.kwargs.get(\"strict\", item.config.getini(\"xfail_strict\"))\n         raises = mark.kwargs.get(\"raises\", None)\n", "gold_standard_diff": "From 78f2dc08fa998622bb52edd992e04e09918712da Mon Sep 17 00:00:00 2001\nFrom: Ran Benita <ran@unusedvar.com>\nDate: Mon, 13 Jul 2020 23:17:39 +0300\nSubject: [PATCH 1/2] skipping: slight simplification\n\n---\n src/_pytest/skipping.py | 4 +---\n 1 file changed, 1 insertion(+), 3 deletions(-)\n\ndiff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996a2..24c89eb6dea 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -231,11 +231,9 @@ def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n \n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n-    item._store[skipped_by_mark_key] = False\n-\n     skipped = evaluate_skip_marks(item)\n+    item._store[skipped_by_mark_key] = skipped is not None\n     if skipped:\n-        item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n     if not item.config.option.runxfail:\n\nFrom ccad10a82908d7a12cd6024e00be11af413edf1c Mon Sep 17 00:00:00 2001\nFrom: Ran Benita <ran@unusedvar.com>\nDate: Mon, 13 Jul 2020 21:34:07 +0300\nSubject: [PATCH 2/2] skipping: fix dynamic xfail mark added in runtest not\n respected\n\nIf a test runtest phase (not setup) dynamically adds a pytest.mark.xfail\nmark to the item, it should be respected, but it wasn't. This regressed\nin 3e6fe92b7ea3c120e8024a970bf37a7c6c137714 (not released).\n\nFix it by just always refreshing the mark if needed. This is mostly what\nwas done before but in a more roundabout way.\n---\n src/_pytest/skipping.py  | 17 ++++++++++-------\n testing/test_skipping.py | 28 ++++++++++++++++++++++++++++\n 2 files changed, 38 insertions(+), 7 deletions(-)\n\ndiff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 24c89eb6dea..e333e78df9b 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -236,10 +236,9 @@ def pytest_runtest_setup(item: Item) -> None:\n     if skipped:\n         skip(skipped.reason)\n \n-    if not item.config.option.runxfail:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n-        if xfailed and not xfailed.run:\n-            xfail(\"[NOTRUN] \" + xfailed.reason)\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+    if xfailed and not item.config.option.runxfail and not xfailed.run:\n+        xfail(\"[NOTRUN] \" + xfailed.reason)\n \n \n @hookimpl(hookwrapper=True)\n@@ -248,12 +247,16 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     if xfailed is None:\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n \n-    if not item.config.option.runxfail:\n-        if xfailed and not xfailed.run:\n-            xfail(\"[NOTRUN] \" + xfailed.reason)\n+    if xfailed and not item.config.option.runxfail and not xfailed.run:\n+        xfail(\"[NOTRUN] \" + xfailed.reason)\n \n     yield\n \n+    # The test run may have added an xfail mark dynamically.\n+    xfailed = item._store.get(xfailed_key, None)\n+    if xfailed is None:\n+        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\ndiff --git a/testing/test_skipping.py b/testing/test_skipping.py\nindex 8fceb37aa71..61de0b3e177 100644\n--- a/testing/test_skipping.py\n+++ b/testing/test_skipping.py\n@@ -1,6 +1,7 @@\n import sys\n \n import pytest\n+from _pytest.pytester import Testdir\n from _pytest.runner import runtestprotocol\n from _pytest.skipping import evaluate_skip_marks\n from _pytest.skipping import evaluate_xfail_marks\n@@ -425,6 +426,33 @@ def test_this2(arg):\n         result = testdir.runpytest(p)\n         result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n \n+    def test_dynamic_xfail_set_during_runtest_failed(self, testdir: Testdir) -> None:\n+        # Issue #7486.\n+        p = testdir.makepyfile(\n+            \"\"\"\n+            import pytest\n+            def test_this(request):\n+                request.node.add_marker(pytest.mark.xfail(reason=\"xfail\"))\n+                assert 0\n+        \"\"\"\n+        )\n+        result = testdir.runpytest(p)\n+        result.assert_outcomes(xfailed=1)\n+\n+    def test_dynamic_xfail_set_during_runtest_passed_strict(\n+        self, testdir: Testdir\n+    ) -> None:\n+        # Issue #7486.\n+        p = testdir.makepyfile(\n+            \"\"\"\n+            import pytest\n+            def test_this(request):\n+                request.node.add_marker(pytest.mark.xfail(reason=\"xfail\", strict=True))\n+        \"\"\"\n+        )\n+        result = testdir.runpytest(p)\n+        result.assert_outcomes(failed=1)\n+\n     @pytest.mark.parametrize(\n         \"expected, actual, matchline\",\n         [\n", "gold_standard_pr_link": "https://github.com/pytest-dev/pytest/pull/7490"}