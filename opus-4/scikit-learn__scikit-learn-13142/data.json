{"instance_id": "scikit-learn__scikit-learn-13142", "score": "C", "diff": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex bd34333c0..8920bef18 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -257,11 +257,6 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n-        # Always do a final e-step to guarantee that the labels returned by\n-        # fit_predict(X) are always consistent with fit(X).predict(X)\n-        # for any value of max_iter and tol (and any random_state).\n-        _, log_resp = self._e_step(X)\n-\n         if not self.converged_:\n             warnings.warn('Initialization %d did not converge. '\n                           'Try different init parameters, '\n@@ -273,6 +268,11 @@ class BaseMixture(DensityMixin, BaseEstimator, metaclass=ABCMeta):\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n \n+        # Always do a final e-step to guarantee that the labels returned by\n+        # fit_predict(X) are always consistent with fit(X).predict(X)\n+        # for any value of max_iter and tol (and any random_state).\n+        _, log_resp = self._e_step(X)\n+\n         return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\ndiff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py\nindex 4d549ccd7..51bfa173c 100644\n--- a/sklearn/mixture/tests/test_gaussian_mixture.py\n+++ b/sklearn/mixture/tests/test_gaussian_mixture.py\n@@ -598,6 +598,21 @@ def test_gaussian_mixture_fit_predict(seed, max_iter, tol):\n         assert_greater(adjusted_rand_score(Y, Y_pred2), .95)\n \n \n+def test_gaussian_mixture_fit_predict_n_init():\n+    # Test that fit_predict and predict are consistent when n_init > 1\n+    rng = np.random.RandomState(42)\n+    X = rng.randn(1000, 5)\n+    \n+    # Test with various n_init values\n+    for n_init in [1, 5, 10]:\n+        for covar_type in COVARIANCE_TYPE:\n+            gm = GaussianMixture(n_components=3, n_init=n_init, \n+                                covariance_type=covar_type, random_state=rng)\n+            labels_fit_predict = gm.fit_predict(X)\n+            labels_predict = gm.predict(X)\n+            assert_array_equal(labels_fit_predict, labels_predict)\n+\n+\n def test_gaussian_mixture_fit():\n     # recover the ground truth\n     rng = np.random.RandomState(0)\ndiff --git a/test_bayesian_issue.py b/test_bayesian_issue.py\nnew file mode 100644\nindex 000000000..d517c161b\n--- /dev/null\n+++ b/test_bayesian_issue.py\n@@ -0,0 +1,17 @@\n+from sklearn.mixture import BayesianGaussianMixture\n+from sklearn.utils.testing import assert_array_equal\n+import numpy as np\n+\n+np.random.seed(42)\n+X = np.random.randn(1000, 5)\n+\n+print('BayesianGaussianMixture with n_init=5')\n+bgm = BayesianGaussianMixture(n_components=5, n_init=5)\n+c1 = bgm.fit_predict(X)\n+c2 = bgm.predict(X)\n+try:\n+    assert_array_equal(c1, c2)\n+    print('Arrays are equal')\n+except AssertionError as e:\n+    print('Arrays are NOT equal')\n+    print('Number of differences:', np.sum(c1 != c2))\ndiff --git a/test_comprehensive.py b/test_comprehensive.py\nnew file mode 100644\nindex 000000000..e7706d656\n--- /dev/null\n+++ b/test_comprehensive.py\n@@ -0,0 +1,44 @@\n+from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n+from sklearn.utils.testing import assert_array_equal\n+import numpy as np\n+\n+# Test with different seeds and settings\n+test_cases = [\n+    {'n_components': 3, 'n_init': 5, 'seed': 0},\n+    {'n_components': 5, 'n_init': 10, 'seed': 42},\n+    {'n_components': 2, 'n_init': 3, 'seed': 123},\n+]\n+\n+for case in test_cases:\n+    print(f\"\\nTesting with n_components={case['n_components']}, n_init={case['n_init']}, seed={case['seed']}\")\n+    \n+    np.random.seed(case['seed'])\n+    X = np.random.randn(500, 4)\n+    \n+    # Test GaussianMixture\n+    gm = GaussianMixture(n_components=case['n_components'], \n+                         n_init=case['n_init'], \n+                         random_state=case['seed'])\n+    c1 = gm.fit_predict(X)\n+    c2 = gm.predict(X)\n+    \n+    try:\n+        assert_array_equal(c1, c2)\n+        print(\"  GaussianMixture: PASS\")\n+    except AssertionError:\n+        print(f\"  GaussianMixture: FAIL (differences: {np.sum(c1 != c2)})\")\n+    \n+    # Test BayesianGaussianMixture\n+    bgm = BayesianGaussianMixture(n_components=case['n_components'], \n+                                  n_init=case['n_init'], \n+                                  random_state=case['seed'])\n+    c1 = bgm.fit_predict(X)\n+    c2 = bgm.predict(X)\n+    \n+    try:\n+        assert_array_equal(c1, c2)\n+        print(\"  BayesianGaussianMixture: PASS\")\n+    except AssertionError:\n+        print(f\"  BayesianGaussianMixture: FAIL (differences: {np.sum(c1 != c2)})\")\n+\n+print(\"\\nAll tests completed!\")\ndiff --git a/test_issue.py b/test_issue.py\nnew file mode 100644\nindex 000000000..ff99d0b39\n--- /dev/null\n+++ b/test_issue.py\n@@ -0,0 +1,23 @@\n+from sklearn.mixture import GaussianMixture\n+from sklearn.utils.testing import assert_array_equal\n+import numpy as np\n+\n+np.random.seed(42)\n+X = np.random.randn(1000, 5)\n+\n+print('no n_init')\n+gm = GaussianMixture(n_components=5)\n+c1 = gm.fit_predict(X)\n+c2 = gm.predict(X)\n+assert_array_equal(c1, c2)\n+\n+print('n_init=5')\n+gm = GaussianMixture(n_components=5, n_init=5)\n+c1 = gm.fit_predict(X)\n+c2 = gm.predict(X)\n+try:\n+    assert_array_equal(c1, c2)\n+    print('Arrays are equal')\n+except AssertionError as e:\n+    print('Arrays are NOT equal')\n+    print('Number of differences:', np.sum(c1 != c2))\ndiff --git a/test_original_issue.py b/test_original_issue.py\nnew file mode 100644\nindex 000000000..4102b232b\n--- /dev/null\n+++ b/test_original_issue.py\n@@ -0,0 +1,15 @@\n+from sklearn.mixture import GaussianMixture\n+from sklearn.utils.testing import assert_array_equal\n+import numpy\n+X = numpy.random.randn(1000,5)\n+print('no n_init')\n+gm = GaussianMixture(n_components=5)\n+c1 = gm.fit_predict(X)\n+c2 = gm.predict(X)\n+assert_array_equal(c1,c2)\n+print('n_init=5')\n+gm = GaussianMixture(n_components=5, n_init=5)\n+c1 = gm.fit_predict(X)\n+c2 = gm.predict(X)\n+assert_array_equal(c1,c2)\n+print('All tests passed!')\n", "gold_standard_diff": "From 7dcb38007dd399f657d346e967fbe5dec20e9c7d Mon Sep 17 00:00:00 2001\nFrom: jeremie du boisberranger <jeremiedbb@yahoo.fr>\nDate: Tue, 12 Feb 2019 15:28:15 +0100\nSubject: [PATCH 1/2] move _e_step after set_params & add test\n\n---\n sklearn/mixture/base.py                        | 10 +++++-----\n sklearn/mixture/tests/test_gaussian_mixture.py |  9 +++++++++\n 2 files changed, 14 insertions(+), 5 deletions(-)\n\ndiff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex bd34333c0630b..8920bef181226 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -257,11 +257,6 @@ def fit_predict(self, X, y=None):\n                 best_params = self._get_parameters()\n                 best_n_iter = n_iter\n \n-        # Always do a final e-step to guarantee that the labels returned by\n-        # fit_predict(X) are always consistent with fit(X).predict(X)\n-        # for any value of max_iter and tol (and any random_state).\n-        _, log_resp = self._e_step(X)\n-\n         if not self.converged_:\n             warnings.warn('Initialization %d did not converge. '\n                           'Try different init parameters, '\n@@ -273,6 +268,11 @@ def fit_predict(self, X, y=None):\n         self.n_iter_ = best_n_iter\n         self.lower_bound_ = max_lower_bound\n \n+        # Always do a final e-step to guarantee that the labels returned by\n+        # fit_predict(X) are always consistent with fit(X).predict(X)\n+        # for any value of max_iter and tol (and any random_state).\n+        _, log_resp = self._e_step(X)\n+\n         return log_resp.argmax(axis=1)\n \n     def _e_step(self, X):\ndiff --git a/sklearn/mixture/tests/test_gaussian_mixture.py b/sklearn/mixture/tests/test_gaussian_mixture.py\nindex 4d549ccd7b9d1..3559ecf9ba0c4 100644\n--- a/sklearn/mixture/tests/test_gaussian_mixture.py\n+++ b/sklearn/mixture/tests/test_gaussian_mixture.py\n@@ -598,6 +598,15 @@ def test_gaussian_mixture_fit_predict(seed, max_iter, tol):\n         assert_greater(adjusted_rand_score(Y, Y_pred2), .95)\n \n \n+def test_gaussian_mixture_fit_predict_n_init():\n+    # Check that fit_predict is equivalent to fit.predict, when n_init > 1\n+    X = np.random.RandomState(0).randn(1000, 5)\n+    gm = GaussianMixture(n_components=5, n_init=5, random_state=0)\n+    y_pred1 = gm.fit_predict(X)\n+    y_pred2 = gm.predict(X)\n+    assert_array_equal(y_pred1, y_pred2)\n+\n+\n def test_gaussian_mixture_fit():\n     # recover the ground truth\n     rng = np.random.RandomState(0)\n\nFrom 032f39a78983e210988f17339e1552b37895c295 Mon Sep 17 00:00:00 2001\nFrom: jeremie du boisberranger <jeremiedbb@yahoo.fr>\nDate: Thu, 14 Feb 2019 10:13:32 +0100\nSubject: [PATCH 2/2] add test for bayesian & what's new\n\n---\n doc/whats_new/v0.21.rst                        | 9 +++++++++\n sklearn/mixture/tests/test_bayesian_mixture.py | 9 +++++++++\n 2 files changed, 18 insertions(+)\n\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex 7355f75b83d4e..af0ee6df24974 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -291,6 +291,15 @@ Support for Python 3.4 and below has been officially dropped.\n   affects all ensemble methods using decision trees.\n   :issue:`12344` by :user:`Adrin Jalali <adrinjalali>`.\n \n+:mod:`sklearn.mixture`\n+......................\n+\n+- |Fix| Fixed a bug in :class:`mixture.BaseMixture` and therefore on estimators\n+  based on it, i.e. :class:`mixture.GaussianMixture` and\n+  :class:`mixture.BayesianGaussianMixture`, where ``fit_predict`` and\n+  ``fit.predict`` were not equivalent. :issue:`13142` by\n+  :user:`J\u00e9r\u00e9mie du Boisberranger <jeremiedbb>`.\n+\n \n Multiple modules\n ................\ndiff --git a/sklearn/mixture/tests/test_bayesian_mixture.py b/sklearn/mixture/tests/test_bayesian_mixture.py\nindex c3503a632238e..58df89aabe47f 100644\n--- a/sklearn/mixture/tests/test_bayesian_mixture.py\n+++ b/sklearn/mixture/tests/test_bayesian_mixture.py\n@@ -451,6 +451,15 @@ def test_bayesian_mixture_fit_predict(seed, max_iter, tol):\n         assert_array_equal(Y_pred1, Y_pred2)\n \n \n+def test_bayesian_mixture_fit_predict_n_init():\n+    # Check that fit_predict is equivalent to fit.predict, when n_init > 1\n+    X = np.random.RandomState(0).randn(1000, 5)\n+    gm = BayesianGaussianMixture(n_components=5, n_init=10, random_state=0)\n+    y_pred1 = gm.fit_predict(X)\n+    y_pred2 = gm.predict(X)\n+    assert_array_equal(y_pred1, y_pred2)\n+\n+\n def test_bayesian_mixture_predict_predict_proba():\n     # this is the same test as test_gaussian_mixture_predict_predict_proba()\n     rng = np.random.RandomState(0)\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/13142"}