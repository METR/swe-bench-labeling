{"instance_id": "scikit-learn__scikit-learn-13135", "score": "C", "diff": "diff --git a/sklearn/preprocessing/_discretization.py b/sklearn/preprocessing/_discretization.py\nindex dd969c12b..362d4b7d3 100644\n--- a/sklearn/preprocessing/_discretization.py\n+++ b/sklearn/preprocessing/_discretization.py\n@@ -172,6 +172,8 @@ class KBinsDiscretizer(BaseEstimator, TransformerMixin):\n                 # 1D k-means procedure\n                 km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\n                 centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n+                # Sort centers to ensure bin_edges are ordered\n+                centers = np.sort(centers)\n                 bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\n                 bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\n \ndiff --git a/sklearn/preprocessing/_discretization.py.backup b/sklearn/preprocessing/_discretization.py.backup\nnew file mode 100644\nindex 000000000..dd969c12b\n--- /dev/null\n+++ b/sklearn/preprocessing/_discretization.py.backup\n@@ -0,0 +1,295 @@\n+# -*- coding: utf-8 -*-\n+\n+# Author: Henry Lin <hlin117@gmail.com>\n+#         Tom Dupr\u00e9 la Tour\n+\n+# License: BSD\n+\n+\n+import numbers\n+import numpy as np\n+import warnings\n+\n+from . import OneHotEncoder\n+\n+from ..base import BaseEstimator, TransformerMixin\n+from ..utils.validation import check_array\n+from ..utils.validation import check_is_fitted\n+from ..utils.validation import FLOAT_DTYPES\n+\n+\n+class KBinsDiscretizer(BaseEstimator, TransformerMixin):\n+    \"\"\"Bin continuous data into intervals.\n+\n+    Read more in the :ref:`User Guide <preprocessing_discretization>`.\n+\n+    Parameters\n+    ----------\n+    n_bins : int or array-like, shape (n_features,) (default=5)\n+        The number of bins to produce. Raises ValueError if ``n_bins < 2``.\n+\n+    encode : {'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')\n+        Method used to encode the transformed result.\n+\n+        onehot\n+            Encode the transformed result with one-hot encoding\n+            and return a sparse matrix. Ignored features are always\n+            stacked to the right.\n+        onehot-dense\n+            Encode the transformed result with one-hot encoding\n+            and return a dense array. Ignored features are always\n+            stacked to the right.\n+        ordinal\n+            Return the bin identifier encoded as an integer value.\n+\n+    strategy : {'uniform', 'quantile', 'kmeans'}, (default='quantile')\n+        Strategy used to define the widths of the bins.\n+\n+        uniform\n+            All bins in each feature have identical widths.\n+        quantile\n+            All bins in each feature have the same number of points.\n+        kmeans\n+            Values in each bin have the same nearest center of a 1D k-means\n+            cluster.\n+\n+    Attributes\n+    ----------\n+    n_bins_ : int array, shape (n_features,)\n+        Number of bins per feature.\n+\n+    bin_edges_ : array of arrays, shape (n_features, )\n+        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``\n+        Ignored features will have empty arrays.\n+\n+    Examples\n+    --------\n+    >>> X = [[-2, 1, -4,   -1],\n+    ...      [-1, 2, -3, -0.5],\n+    ...      [ 0, 3, -2,  0.5],\n+    ...      [ 1, 4, -1,    2]]\n+    >>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n+    >>> est.fit(X)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n+    KBinsDiscretizer(...)\n+    >>> Xt = est.transform(X)\n+    >>> Xt  # doctest: +SKIP\n+    array([[ 0., 0., 0., 0.],\n+           [ 1., 1., 1., 0.],\n+           [ 2., 2., 2., 1.],\n+           [ 2., 2., 2., 2.]])\n+\n+    Sometimes it may be useful to convert the data back into the original\n+    feature space. The ``inverse_transform`` function converts the binned\n+    data into the original feature space. Each value will be equal to the mean\n+    of the two bin edges.\n+\n+    >>> est.bin_edges_[0]\n+    array([-2., -1.,  0.,  1.])\n+    >>> est.inverse_transform(Xt)\n+    array([[-1.5,  1.5, -3.5, -0.5],\n+           [-0.5,  2.5, -2.5, -0.5],\n+           [ 0.5,  3.5, -1.5,  0.5],\n+           [ 0.5,  3.5, -1.5,  1.5]])\n+\n+    Notes\n+    -----\n+    In bin edges for feature ``i``, the first and last values are used only for\n+    ``inverse_transform``. During transform, bin edges are extended to::\n+\n+      np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])\n+\n+    You can combine ``KBinsDiscretizer`` with\n+    :class:`sklearn.compose.ColumnTransformer` if you only want to preprocess\n+    part of the features.\n+\n+    See also\n+    --------\n+     sklearn.preprocessing.Binarizer : class used to bin values as ``0`` or\n+        ``1`` based on a parameter ``threshold``.\n+    \"\"\"\n+\n+    def __init__(self, n_bins=5, encode='onehot', strategy='quantile'):\n+        self.n_bins = n_bins\n+        self.encode = encode\n+        self.strategy = strategy\n+\n+    def fit(self, X, y=None):\n+        \"\"\"Fits the estimator.\n+\n+        Parameters\n+        ----------\n+        X : numeric array-like, shape (n_samples, n_features)\n+            Data to be discretized.\n+\n+        y : ignored\n+\n+        Returns\n+        -------\n+        self\n+        \"\"\"\n+        X = check_array(X, dtype='numeric')\n+\n+        valid_encode = ('onehot', 'onehot-dense', 'ordinal')\n+        if self.encode not in valid_encode:\n+            raise ValueError(\"Valid options for 'encode' are {}. \"\n+                             \"Got encode={!r} instead.\"\n+                             .format(valid_encode, self.encode))\n+        valid_strategy = ('uniform', 'quantile', 'kmeans')\n+        if self.strategy not in valid_strategy:\n+            raise ValueError(\"Valid options for 'strategy' are {}. \"\n+                             \"Got strategy={!r} instead.\"\n+                             .format(valid_strategy, self.strategy))\n+\n+        n_features = X.shape[1]\n+        n_bins = self._validate_n_bins(n_features)\n+\n+        bin_edges = np.zeros(n_features, dtype=object)\n+        for jj in range(n_features):\n+            column = X[:, jj]\n+            col_min, col_max = column.min(), column.max()\n+\n+            if col_min == col_max:\n+                warnings.warn(\"Feature %d is constant and will be \"\n+                              \"replaced with 0.\" % jj)\n+                n_bins[jj] = 1\n+                bin_edges[jj] = np.array([-np.inf, np.inf])\n+                continue\n+\n+            if self.strategy == 'uniform':\n+                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)\n+\n+            elif self.strategy == 'quantile':\n+                quantiles = np.linspace(0, 100, n_bins[jj] + 1)\n+                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))\n+\n+            elif self.strategy == 'kmeans':\n+                from ..cluster import KMeans  # fixes import loops\n+\n+                # Deterministic initialization with uniform spacing\n+                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)\n+                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5\n+\n+                # 1D k-means procedure\n+                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\n+                centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n+                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\n+                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\n+\n+        self.bin_edges_ = bin_edges\n+        self.n_bins_ = n_bins\n+\n+        if 'onehot' in self.encode:\n+            self._encoder = OneHotEncoder(\n+                categories=[np.arange(i) for i in self.n_bins_],\n+                sparse=self.encode == 'onehot')\n+            # Fit the OneHotEncoder with toy datasets\n+            # so that it's ready for use after the KBinsDiscretizer is fitted\n+            self._encoder.fit(np.zeros((1, len(self.n_bins_)), dtype=int))\n+\n+        return self\n+\n+    def _validate_n_bins(self, n_features):\n+        \"\"\"Returns n_bins_, the number of bins per feature.\n+        \"\"\"\n+        orig_bins = self.n_bins\n+        if isinstance(orig_bins, numbers.Number):\n+            if not isinstance(orig_bins, (numbers.Integral, np.integer)):\n+                raise ValueError(\"{} received an invalid n_bins type. \"\n+                                 \"Received {}, expected int.\"\n+                                 .format(KBinsDiscretizer.__name__,\n+                                         type(orig_bins).__name__))\n+            if orig_bins < 2:\n+                raise ValueError(\"{} received an invalid number \"\n+                                 \"of bins. Received {}, expected at least 2.\"\n+                                 .format(KBinsDiscretizer.__name__, orig_bins))\n+            return np.full(n_features, orig_bins, dtype=np.int)\n+\n+        n_bins = check_array(orig_bins, dtype=np.int, copy=True,\n+                             ensure_2d=False)\n+\n+        if n_bins.ndim > 1 or n_bins.shape[0] != n_features:\n+            raise ValueError(\"n_bins must be a scalar or array \"\n+                             \"of shape (n_features,).\")\n+\n+        bad_nbins_value = (n_bins < 2) | (n_bins != orig_bins)\n+\n+        violating_indices = np.where(bad_nbins_value)[0]\n+        if violating_indices.shape[0] > 0:\n+            indices = \", \".join(str(i) for i in violating_indices)\n+            raise ValueError(\"{} received an invalid number \"\n+                             \"of bins at indices {}. Number of bins \"\n+                             \"must be at least 2, and must be an int.\"\n+                             .format(KBinsDiscretizer.__name__, indices))\n+        return n_bins\n+\n+    def transform(self, X):\n+        \"\"\"Discretizes the data.\n+\n+        Parameters\n+        ----------\n+        X : numeric array-like, shape (n_samples, n_features)\n+            Data to be discretized.\n+\n+        Returns\n+        -------\n+        Xt : numeric array-like or sparse matrix\n+            Data in the binned space.\n+        \"\"\"\n+        check_is_fitted(self, [\"bin_edges_\"])\n+\n+        Xt = check_array(X, copy=True, dtype=FLOAT_DTYPES)\n+        n_features = self.n_bins_.shape[0]\n+        if Xt.shape[1] != n_features:\n+            raise ValueError(\"Incorrect number of features. Expecting {}, \"\n+                             \"received {}.\".format(n_features, Xt.shape[1]))\n+\n+        bin_edges = self.bin_edges_\n+        for jj in range(Xt.shape[1]):\n+            # Values which are close to a bin edge are susceptible to numeric\n+            # instability. Add eps to X so these values are binned correctly\n+            # with respect to their decimal truncation. See documentation of\n+            # numpy.isclose for an explanation of ``rtol`` and ``atol``.\n+            rtol = 1.e-5\n+            atol = 1.e-8\n+            eps = atol + rtol * np.abs(Xt[:, jj])\n+            Xt[:, jj] = np.digitize(Xt[:, jj] + eps, bin_edges[jj][1:])\n+        np.clip(Xt, 0, self.n_bins_ - 1, out=Xt)\n+\n+        if self.encode == 'ordinal':\n+            return Xt\n+\n+        return self._encoder.transform(Xt)\n+\n+    def inverse_transform(self, Xt):\n+        \"\"\"Transforms discretized data back to original feature space.\n+\n+        Note that this function does not regenerate the original data\n+        due to discretization rounding.\n+\n+        Parameters\n+        ----------\n+        Xt : numeric array-like, shape (n_sample, n_features)\n+            Transformed data in the binned space.\n+\n+        Returns\n+        -------\n+        Xinv : numeric array-like\n+            Data in the original feature space.\n+        \"\"\"\n+        check_is_fitted(self, [\"bin_edges_\"])\n+\n+        if 'onehot' in self.encode:\n+            Xt = self._encoder.inverse_transform(Xt)\n+\n+        Xinv = check_array(Xt, copy=True, dtype=FLOAT_DTYPES)\n+        n_features = self.n_bins_.shape[0]\n+        if Xinv.shape[1] != n_features:\n+            raise ValueError(\"Incorrect number of features. Expecting {}, \"\n+                             \"received {}.\".format(n_features, Xinv.shape[1]))\n+\n+        for jj in range(n_features):\n+            bin_edges = self.bin_edges_[jj]\n+            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5\n+            Xinv[:, jj] = bin_centers[np.int_(Xinv[:, jj])]\n+\n+        return Xinv\ndiff --git a/sklearn/preprocessing/tests/test_discretization.py b/sklearn/preprocessing/tests/test_discretization.py\nindex c2e307e85..54ae5a401 100644\n--- a/sklearn/preprocessing/tests/test_discretization.py\n+++ b/sklearn/preprocessing/tests/test_discretization.py\n@@ -247,3 +247,30 @@ def test_overwrite():\n     Xinv = est.inverse_transform(Xt)\n     assert_array_equal(Xt, Xt_before)\n     assert_array_equal(Xinv, np.array([[0.5], [1.5], [2.5], [2.5]]))\n+\n+\n+def test_kmeans_sorted_centers():\n+    \"\"\"Test that kmeans strategy produces sorted bin edges.\n+    \n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/13195\n+    \"\"\"\n+    X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)\n+    \n+    # This specific case would produce unsorted centers without the fix\n+    est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')\n+    est.fit(X)\n+    \n+    # Check that bin edges are sorted\n+    assert np.all(np.diff(est.bin_edges_[0]) >= 0)\n+    \n+    # Transform should not raise ValueError\n+    Xt = est.transform(X)\n+    \n+    # Test with multiple features as well\n+    X_multi = np.random.RandomState(42).randn(100, 3)\n+    est_multi = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')\n+    est_multi.fit(X_multi)\n+    \n+    for i in range(X_multi.shape[1]):\n+        assert np.all(np.diff(est_multi.bin_edges_[i]) >= 0)\ndiff --git a/test_comprehensive.py b/test_comprehensive.py\nnew file mode 100644\nindex 000000000..918623218\n--- /dev/null\n+++ b/test_comprehensive.py\n@@ -0,0 +1,54 @@\n+import numpy as np\n+from sklearn.preprocessing import KBinsDiscretizer\n+from sklearn.utils.testing import assert_array_equal, assert_array_almost_equal\n+\n+# Test 1: Original failing case\n+print(\"Test 1: Original failing case...\")\n+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)\n+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')\n+Xt = est.fit_transform(X)\n+print(f\"  Passed! Transformed: {Xt.ravel()}\")\n+print(f\"  Bin edges: {est.bin_edges_[0]}\")\n+\n+# Test 2: Multiple features\n+print(\"\\nTest 2: Multiple features...\")\n+X = np.random.randn(100, 3)\n+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')\n+Xt = est.fit_transform(X)\n+for i in range(3):\n+    assert np.all(np.diff(est.bin_edges_[i]) >= 0), f\"Feature {i} bin edges not sorted!\"\n+print(\"  Passed! All features have sorted bin edges\")\n+\n+# Test 3: Different encoding options\n+print(\"\\nTest 3: Different encoding options...\")\n+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)\n+for encode in ['ordinal', 'onehot', 'onehot-dense']:\n+    est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode=encode)\n+    Xt = est.fit_transform(X)\n+    print(f\"  {encode}: shape = {Xt.shape}, passed!\")\n+\n+# Test 4: Edge case with many bins\n+print(\"\\nTest 4: Edge case with many bins...\")\n+X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n+est = KBinsDiscretizer(n_bins=8, strategy='kmeans', encode='ordinal')\n+Xt = est.fit_transform(X)\n+print(f\"  Passed! Bin edges sorted: {np.all(np.diff(est.bin_edges_[0]) >= 0)}\")\n+\n+# Test 5: Compare with other strategies (they should still work)\n+print(\"\\nTest 5: Other strategies still work...\")\n+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)\n+for strategy in ['uniform', 'quantile', 'kmeans']:\n+    est = KBinsDiscretizer(n_bins=5, strategy=strategy, encode='ordinal')\n+    Xt = est.fit_transform(X)\n+    print(f\"  {strategy}: passed!\")\n+\n+# Test 6: Inverse transform\n+print(\"\\nTest 6: Inverse transform...\")\n+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)\n+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')\n+Xt = est.fit_transform(X)\n+X_inv = est.inverse_transform(Xt)\n+print(f\"  Original shape: {X.shape}, Inverse shape: {X_inv.shape}\")\n+print(\"  Passed!\")\n+\n+print(\"\\nAll tests passed!\")\ndiff --git a/test_final_verification.py b/test_final_verification.py\nnew file mode 100644\nindex 000000000..2cc0c8173\n--- /dev/null\n+++ b/test_final_verification.py\n@@ -0,0 +1,24 @@\n+import numpy as np\n+from sklearn.preprocessing import KBinsDiscretizer\n+\n+# The exact test case from the issue\n+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)\n+\n+# with 5 bins (previously would fail)\n+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')\n+Xt = est.fit_transform(X)\n+\n+print(\"Success! KBinsDiscretizer with kmeans strategy works correctly.\")\n+print(f\"Transformed data: {Xt.ravel()}\")\n+print(f\"Bin edges: {est.bin_edges_[0]}\")\n+print(f\"Bin edges are sorted: {np.all(np.diff(est.bin_edges_[0]) >= 0)}\")\n+\n+# Also test the mentioned productive case scenario\n+# (bins of order log_2(number of unique values))\n+unique_values = 100\n+X_prod = np.random.RandomState(42).choice(unique_values, size=1000).reshape(-1, 1)\n+n_bins = int(np.log2(unique_values))\n+print(f\"\\nTesting productive scenario: {unique_values} unique values, {n_bins} bins\")\n+est_prod = KBinsDiscretizer(n_bins=n_bins, strategy='kmeans', encode='ordinal')\n+Xt_prod = est_prod.fit_transform(X_prod)\n+print(f\"Success! Bin edges are sorted: {np.all(np.diff(est_prod.bin_edges_[0]) >= 0)}\")\ndiff --git a/test_kbins_issue.py b/test_kbins_issue.py\nnew file mode 100644\nindex 000000000..45bffa8ca\n--- /dev/null\n+++ b/test_kbins_issue.py\n@@ -0,0 +1,21 @@\n+import numpy as np\n+from sklearn.preprocessing import KBinsDiscretizer\n+\n+# Test case from the issue\n+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)\n+\n+# with 5 bins\n+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')\n+\n+print(\"Testing original code...\")\n+try:\n+    Xt = est.fit_transform(X)\n+    print(\"No error occurred\")\n+    print(f\"Transformed data: {Xt.ravel()}\")\n+except ValueError as e:\n+    print(f\"Error: {e}\")\n+    \n+# Let's check the bin edges after fit\n+est.fit(X)\n+print(f\"\\nBin edges: {est.bin_edges_[0]}\")\n+print(f\"Are bin edges sorted? {np.all(np.diff(est.bin_edges_[0]) >= 0)}\")\n", "gold_standard_diff": "From 32837055a077b9e3b79183660098a57e4a858aba Mon Sep 17 00:00:00 2001\nFrom: SandroCasagrande <SandroCasagrande@users.noreply.github.com>\nDate: Mon, 11 Feb 2019 21:52:03 +0100\nSubject: [PATCH] Test and fix to ensure sorted bin_edges from kmeans strategy\n of KBinsDiscretizer\n\n---\n doc/whats_new/v0.20.rst                          |  4 ++++\n sklearn/preprocessing/_discretization.py         |  2 ++\n .../preprocessing/tests/test_discretization.py   | 16 +++++++++++-----\n 3 files changed, 17 insertions(+), 5 deletions(-)\n\ndiff --git a/doc/whats_new/v0.20.rst b/doc/whats_new/v0.20.rst\nindex 3483b173dcb16..4daae88572b48 100644\n--- a/doc/whats_new/v0.20.rst\n+++ b/doc/whats_new/v0.20.rst\n@@ -55,6 +55,10 @@ Changelog\n :mod:`sklearn.preprocessing`\n ............................\n \n+- |Fix| Fixed a bug in :class:`preprocessing.KBinsDiscretizer` where\n+  ``strategy='kmeans'`` fails with an error during transformation due to unsorted\n+  bin edges. :issue:`13134` by :user:`Sandro Casagrande <SandroCasagrande>`.\n+\n - |Fix| Fixed a bug in :class:`preprocessing.OneHotEncoder` where the\n   deprecation of ``categorical_features`` was handled incorrectly in\n   combination with ``handle_unknown='ignore'``.\ndiff --git a/sklearn/preprocessing/_discretization.py b/sklearn/preprocessing/_discretization.py\nindex dd969c12b2833..35d654399dc27 100644\n--- a/sklearn/preprocessing/_discretization.py\n+++ b/sklearn/preprocessing/_discretization.py\n@@ -172,6 +172,8 @@ def fit(self, X, y=None):\n                 # 1D k-means procedure\n                 km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)\n                 centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n+                # Must sort, centers may be unsorted even with sorted init\n+                centers.sort()\n                 bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5\n                 bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]\n \ndiff --git a/sklearn/preprocessing/tests/test_discretization.py b/sklearn/preprocessing/tests/test_discretization.py\nindex c2e307e85d51e..81dc6fa5668a5 100644\n--- a/sklearn/preprocessing/tests/test_discretization.py\n+++ b/sklearn/preprocessing/tests/test_discretization.py\n@@ -185,11 +185,12 @@ def test_invalid_strategy_option():\n \n \n @pytest.mark.parametrize(\n-    'strategy, expected_2bins, expected_3bins',\n-    [('uniform', [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 2, 2]),\n-     ('kmeans', [0, 0, 0, 0, 1, 1], [0, 0, 1, 1, 2, 2]),\n-     ('quantile', [0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2])])\n-def test_nonuniform_strategies(strategy, expected_2bins, expected_3bins):\n+    'strategy, expected_2bins, expected_3bins, expected_5bins',\n+    [('uniform', [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 2, 2], [0, 0, 1, 1, 4, 4]),\n+     ('kmeans', [0, 0, 0, 0, 1, 1], [0, 0, 1, 1, 2, 2], [0, 0, 1, 2, 3, 4]),\n+     ('quantile', [0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2], [0, 1, 2, 3, 4, 4])])\n+def test_nonuniform_strategies(\n+        strategy, expected_2bins, expected_3bins, expected_5bins):\n     X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)\n \n     # with 2 bins\n@@ -202,6 +203,11 @@ def test_nonuniform_strategies(strategy, expected_2bins, expected_3bins):\n     Xt = est.fit_transform(X)\n     assert_array_equal(expected_3bins, Xt.ravel())\n \n+    # with 5 bins\n+    est = KBinsDiscretizer(n_bins=5, strategy=strategy, encode='ordinal')\n+    Xt = est.fit_transform(X)\n+    assert_array_equal(expected_5bins, Xt.ravel())\n+\n \n @pytest.mark.parametrize('strategy', ['uniform', 'kmeans', 'quantile'])\n @pytest.mark.parametrize('encode', ['ordinal', 'onehot', 'onehot-dense'])\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/13135"}