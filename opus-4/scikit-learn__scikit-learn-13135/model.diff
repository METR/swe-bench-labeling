diff --git a/sklearn/preprocessing/_discretization.py b/sklearn/preprocessing/_discretization.py
index dd969c12b..362d4b7d3 100644
--- a/sklearn/preprocessing/_discretization.py
+++ b/sklearn/preprocessing/_discretization.py
@@ -172,6 +172,8 @@ class KBinsDiscretizer(BaseEstimator, TransformerMixin):
                 # 1D k-means procedure
                 km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)
                 centers = km.fit(column[:, None]).cluster_centers_[:, 0]
+                # Sort centers to ensure bin_edges are ordered
+                centers = np.sort(centers)
                 bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5
                 bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]
 
diff --git a/sklearn/preprocessing/_discretization.py.backup b/sklearn/preprocessing/_discretization.py.backup
new file mode 100644
index 000000000..dd969c12b
--- /dev/null
+++ b/sklearn/preprocessing/_discretization.py.backup
@@ -0,0 +1,295 @@
+# -*- coding: utf-8 -*-
+
+# Author: Henry Lin <hlin117@gmail.com>
+#         Tom Dupr√© la Tour
+
+# License: BSD
+
+
+import numbers
+import numpy as np
+import warnings
+
+from . import OneHotEncoder
+
+from ..base import BaseEstimator, TransformerMixin
+from ..utils.validation import check_array
+from ..utils.validation import check_is_fitted
+from ..utils.validation import FLOAT_DTYPES
+
+
+class KBinsDiscretizer(BaseEstimator, TransformerMixin):
+    """Bin continuous data into intervals.
+
+    Read more in the :ref:`User Guide <preprocessing_discretization>`.
+
+    Parameters
+    ----------
+    n_bins : int or array-like, shape (n_features,) (default=5)
+        The number of bins to produce. Raises ValueError if ``n_bins < 2``.
+
+    encode : {'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')
+        Method used to encode the transformed result.
+
+        onehot
+            Encode the transformed result with one-hot encoding
+            and return a sparse matrix. Ignored features are always
+            stacked to the right.
+        onehot-dense
+            Encode the transformed result with one-hot encoding
+            and return a dense array. Ignored features are always
+            stacked to the right.
+        ordinal
+            Return the bin identifier encoded as an integer value.
+
+    strategy : {'uniform', 'quantile', 'kmeans'}, (default='quantile')
+        Strategy used to define the widths of the bins.
+
+        uniform
+            All bins in each feature have identical widths.
+        quantile
+            All bins in each feature have the same number of points.
+        kmeans
+            Values in each bin have the same nearest center of a 1D k-means
+            cluster.
+
+    Attributes
+    ----------
+    n_bins_ : int array, shape (n_features,)
+        Number of bins per feature.
+
+    bin_edges_ : array of arrays, shape (n_features, )
+        The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``
+        Ignored features will have empty arrays.
+
+    Examples
+    --------
+    >>> X = [[-2, 1, -4,   -1],
+    ...      [-1, 2, -3, -0.5],
+    ...      [ 0, 3, -2,  0.5],
+    ...      [ 1, 4, -1,    2]]
+    >>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
+    >>> est.fit(X)  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
+    KBinsDiscretizer(...)
+    >>> Xt = est.transform(X)
+    >>> Xt  # doctest: +SKIP
+    array([[ 0., 0., 0., 0.],
+           [ 1., 1., 1., 0.],
+           [ 2., 2., 2., 1.],
+           [ 2., 2., 2., 2.]])
+
+    Sometimes it may be useful to convert the data back into the original
+    feature space. The ``inverse_transform`` function converts the binned
+    data into the original feature space. Each value will be equal to the mean
+    of the two bin edges.
+
+    >>> est.bin_edges_[0]
+    array([-2., -1.,  0.,  1.])
+    >>> est.inverse_transform(Xt)
+    array([[-1.5,  1.5, -3.5, -0.5],
+           [-0.5,  2.5, -2.5, -0.5],
+           [ 0.5,  3.5, -1.5,  0.5],
+           [ 0.5,  3.5, -1.5,  1.5]])
+
+    Notes
+    -----
+    In bin edges for feature ``i``, the first and last values are used only for
+    ``inverse_transform``. During transform, bin edges are extended to::
+
+      np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])
+
+    You can combine ``KBinsDiscretizer`` with
+    :class:`sklearn.compose.ColumnTransformer` if you only want to preprocess
+    part of the features.
+
+    See also
+    --------
+     sklearn.preprocessing.Binarizer : class used to bin values as ``0`` or
+        ``1`` based on a parameter ``threshold``.
+    """
+
+    def __init__(self, n_bins=5, encode='onehot', strategy='quantile'):
+        self.n_bins = n_bins
+        self.encode = encode
+        self.strategy = strategy
+
+    def fit(self, X, y=None):
+        """Fits the estimator.
+
+        Parameters
+        ----------
+        X : numeric array-like, shape (n_samples, n_features)
+            Data to be discretized.
+
+        y : ignored
+
+        Returns
+        -------
+        self
+        """
+        X = check_array(X, dtype='numeric')
+
+        valid_encode = ('onehot', 'onehot-dense', 'ordinal')
+        if self.encode not in valid_encode:
+            raise ValueError("Valid options for 'encode' are {}. "
+                             "Got encode={!r} instead."
+                             .format(valid_encode, self.encode))
+        valid_strategy = ('uniform', 'quantile', 'kmeans')
+        if self.strategy not in valid_strategy:
+            raise ValueError("Valid options for 'strategy' are {}. "
+                             "Got strategy={!r} instead."
+                             .format(valid_strategy, self.strategy))
+
+        n_features = X.shape[1]
+        n_bins = self._validate_n_bins(n_features)
+
+        bin_edges = np.zeros(n_features, dtype=object)
+        for jj in range(n_features):
+            column = X[:, jj]
+            col_min, col_max = column.min(), column.max()
+
+            if col_min == col_max:
+                warnings.warn("Feature %d is constant and will be "
+                              "replaced with 0." % jj)
+                n_bins[jj] = 1
+                bin_edges[jj] = np.array([-np.inf, np.inf])
+                continue
+
+            if self.strategy == 'uniform':
+                bin_edges[jj] = np.linspace(col_min, col_max, n_bins[jj] + 1)
+
+            elif self.strategy == 'quantile':
+                quantiles = np.linspace(0, 100, n_bins[jj] + 1)
+                bin_edges[jj] = np.asarray(np.percentile(column, quantiles))
+
+            elif self.strategy == 'kmeans':
+                from ..cluster import KMeans  # fixes import loops
+
+                # Deterministic initialization with uniform spacing
+                uniform_edges = np.linspace(col_min, col_max, n_bins[jj] + 1)
+                init = (uniform_edges[1:] + uniform_edges[:-1])[:, None] * 0.5
+
+                # 1D k-means procedure
+                km = KMeans(n_clusters=n_bins[jj], init=init, n_init=1)
+                centers = km.fit(column[:, None]).cluster_centers_[:, 0]
+                bin_edges[jj] = (centers[1:] + centers[:-1]) * 0.5
+                bin_edges[jj] = np.r_[col_min, bin_edges[jj], col_max]
+
+        self.bin_edges_ = bin_edges
+        self.n_bins_ = n_bins
+
+        if 'onehot' in self.encode:
+            self._encoder = OneHotEncoder(
+                categories=[np.arange(i) for i in self.n_bins_],
+                sparse=self.encode == 'onehot')
+            # Fit the OneHotEncoder with toy datasets
+            # so that it's ready for use after the KBinsDiscretizer is fitted
+            self._encoder.fit(np.zeros((1, len(self.n_bins_)), dtype=int))
+
+        return self
+
+    def _validate_n_bins(self, n_features):
+        """Returns n_bins_, the number of bins per feature.
+        """
+        orig_bins = self.n_bins
+        if isinstance(orig_bins, numbers.Number):
+            if not isinstance(orig_bins, (numbers.Integral, np.integer)):
+                raise ValueError("{} received an invalid n_bins type. "
+                                 "Received {}, expected int."
+                                 .format(KBinsDiscretizer.__name__,
+                                         type(orig_bins).__name__))
+            if orig_bins < 2:
+                raise ValueError("{} received an invalid number "
+                                 "of bins. Received {}, expected at least 2."
+                                 .format(KBinsDiscretizer.__name__, orig_bins))
+            return np.full(n_features, orig_bins, dtype=np.int)
+
+        n_bins = check_array(orig_bins, dtype=np.int, copy=True,
+                             ensure_2d=False)
+
+        if n_bins.ndim > 1 or n_bins.shape[0] != n_features:
+            raise ValueError("n_bins must be a scalar or array "
+                             "of shape (n_features,).")
+
+        bad_nbins_value = (n_bins < 2) | (n_bins != orig_bins)
+
+        violating_indices = np.where(bad_nbins_value)[0]
+        if violating_indices.shape[0] > 0:
+            indices = ", ".join(str(i) for i in violating_indices)
+            raise ValueError("{} received an invalid number "
+                             "of bins at indices {}. Number of bins "
+                             "must be at least 2, and must be an int."
+                             .format(KBinsDiscretizer.__name__, indices))
+        return n_bins
+
+    def transform(self, X):
+        """Discretizes the data.
+
+        Parameters
+        ----------
+        X : numeric array-like, shape (n_samples, n_features)
+            Data to be discretized.
+
+        Returns
+        -------
+        Xt : numeric array-like or sparse matrix
+            Data in the binned space.
+        """
+        check_is_fitted(self, ["bin_edges_"])
+
+        Xt = check_array(X, copy=True, dtype=FLOAT_DTYPES)
+        n_features = self.n_bins_.shape[0]
+        if Xt.shape[1] != n_features:
+            raise ValueError("Incorrect number of features. Expecting {}, "
+                             "received {}.".format(n_features, Xt.shape[1]))
+
+        bin_edges = self.bin_edges_
+        for jj in range(Xt.shape[1]):
+            # Values which are close to a bin edge are susceptible to numeric
+            # instability. Add eps to X so these values are binned correctly
+            # with respect to their decimal truncation. See documentation of
+            # numpy.isclose for an explanation of ``rtol`` and ``atol``.
+            rtol = 1.e-5
+            atol = 1.e-8
+            eps = atol + rtol * np.abs(Xt[:, jj])
+            Xt[:, jj] = np.digitize(Xt[:, jj] + eps, bin_edges[jj][1:])
+        np.clip(Xt, 0, self.n_bins_ - 1, out=Xt)
+
+        if self.encode == 'ordinal':
+            return Xt
+
+        return self._encoder.transform(Xt)
+
+    def inverse_transform(self, Xt):
+        """Transforms discretized data back to original feature space.
+
+        Note that this function does not regenerate the original data
+        due to discretization rounding.
+
+        Parameters
+        ----------
+        Xt : numeric array-like, shape (n_sample, n_features)
+            Transformed data in the binned space.
+
+        Returns
+        -------
+        Xinv : numeric array-like
+            Data in the original feature space.
+        """
+        check_is_fitted(self, ["bin_edges_"])
+
+        if 'onehot' in self.encode:
+            Xt = self._encoder.inverse_transform(Xt)
+
+        Xinv = check_array(Xt, copy=True, dtype=FLOAT_DTYPES)
+        n_features = self.n_bins_.shape[0]
+        if Xinv.shape[1] != n_features:
+            raise ValueError("Incorrect number of features. Expecting {}, "
+                             "received {}.".format(n_features, Xinv.shape[1]))
+
+        for jj in range(n_features):
+            bin_edges = self.bin_edges_[jj]
+            bin_centers = (bin_edges[1:] + bin_edges[:-1]) * 0.5
+            Xinv[:, jj] = bin_centers[np.int_(Xinv[:, jj])]
+
+        return Xinv
diff --git a/sklearn/preprocessing/tests/test_discretization.py b/sklearn/preprocessing/tests/test_discretization.py
index c2e307e85..54ae5a401 100644
--- a/sklearn/preprocessing/tests/test_discretization.py
+++ b/sklearn/preprocessing/tests/test_discretization.py
@@ -247,3 +247,30 @@ def test_overwrite():
     Xinv = est.inverse_transform(Xt)
     assert_array_equal(Xt, Xt_before)
     assert_array_equal(Xinv, np.array([[0.5], [1.5], [2.5], [2.5]]))
+
+
+def test_kmeans_sorted_centers():
+    """Test that kmeans strategy produces sorted bin edges.
+    
+    Non-regression test for:
+    https://github.com/scikit-learn/scikit-learn/issues/13195
+    """
+    X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)
+    
+    # This specific case would produce unsorted centers without the fix
+    est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')
+    est.fit(X)
+    
+    # Check that bin edges are sorted
+    assert np.all(np.diff(est.bin_edges_[0]) >= 0)
+    
+    # Transform should not raise ValueError
+    Xt = est.transform(X)
+    
+    # Test with multiple features as well
+    X_multi = np.random.RandomState(42).randn(100, 3)
+    est_multi = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')
+    est_multi.fit(X_multi)
+    
+    for i in range(X_multi.shape[1]):
+        assert np.all(np.diff(est_multi.bin_edges_[i]) >= 0)
diff --git a/test_comprehensive.py b/test_comprehensive.py
new file mode 100644
index 000000000..918623218
--- /dev/null
+++ b/test_comprehensive.py
@@ -0,0 +1,54 @@
+import numpy as np
+from sklearn.preprocessing import KBinsDiscretizer
+from sklearn.utils.testing import assert_array_equal, assert_array_almost_equal
+
+# Test 1: Original failing case
+print("Test 1: Original failing case...")
+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)
+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')
+Xt = est.fit_transform(X)
+print(f"  Passed! Transformed: {Xt.ravel()}")
+print(f"  Bin edges: {est.bin_edges_[0]}")
+
+# Test 2: Multiple features
+print("\nTest 2: Multiple features...")
+X = np.random.randn(100, 3)
+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')
+Xt = est.fit_transform(X)
+for i in range(3):
+    assert np.all(np.diff(est.bin_edges_[i]) >= 0), f"Feature {i} bin edges not sorted!"
+print("  Passed! All features have sorted bin edges")
+
+# Test 3: Different encoding options
+print("\nTest 3: Different encoding options...")
+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)
+for encode in ['ordinal', 'onehot', 'onehot-dense']:
+    est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode=encode)
+    Xt = est.fit_transform(X)
+    print(f"  {encode}: shape = {Xt.shape}, passed!")
+
+# Test 4: Edge case with many bins
+print("\nTest 4: Edge case with many bins...")
+X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)
+est = KBinsDiscretizer(n_bins=8, strategy='kmeans', encode='ordinal')
+Xt = est.fit_transform(X)
+print(f"  Passed! Bin edges sorted: {np.all(np.diff(est.bin_edges_[0]) >= 0)}")
+
+# Test 5: Compare with other strategies (they should still work)
+print("\nTest 5: Other strategies still work...")
+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)
+for strategy in ['uniform', 'quantile', 'kmeans']:
+    est = KBinsDiscretizer(n_bins=5, strategy=strategy, encode='ordinal')
+    Xt = est.fit_transform(X)
+    print(f"  {strategy}: passed!")
+
+# Test 6: Inverse transform
+print("\nTest 6: Inverse transform...")
+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)
+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')
+Xt = est.fit_transform(X)
+X_inv = est.inverse_transform(Xt)
+print(f"  Original shape: {X.shape}, Inverse shape: {X_inv.shape}")
+print("  Passed!")
+
+print("\nAll tests passed!")
diff --git a/test_final_verification.py b/test_final_verification.py
new file mode 100644
index 000000000..2cc0c8173
--- /dev/null
+++ b/test_final_verification.py
@@ -0,0 +1,24 @@
+import numpy as np
+from sklearn.preprocessing import KBinsDiscretizer
+
+# The exact test case from the issue
+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)
+
+# with 5 bins (previously would fail)
+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')
+Xt = est.fit_transform(X)
+
+print("Success! KBinsDiscretizer with kmeans strategy works correctly.")
+print(f"Transformed data: {Xt.ravel()}")
+print(f"Bin edges: {est.bin_edges_[0]}")
+print(f"Bin edges are sorted: {np.all(np.diff(est.bin_edges_[0]) >= 0)}")
+
+# Also test the mentioned productive case scenario
+# (bins of order log_2(number of unique values))
+unique_values = 100
+X_prod = np.random.RandomState(42).choice(unique_values, size=1000).reshape(-1, 1)
+n_bins = int(np.log2(unique_values))
+print(f"\nTesting productive scenario: {unique_values} unique values, {n_bins} bins")
+est_prod = KBinsDiscretizer(n_bins=n_bins, strategy='kmeans', encode='ordinal')
+Xt_prod = est_prod.fit_transform(X_prod)
+print(f"Success! Bin edges are sorted: {np.all(np.diff(est_prod.bin_edges_[0]) >= 0)}")
diff --git a/test_kbins_issue.py b/test_kbins_issue.py
new file mode 100644
index 000000000..45bffa8ca
--- /dev/null
+++ b/test_kbins_issue.py
@@ -0,0 +1,21 @@
+import numpy as np
+from sklearn.preprocessing import KBinsDiscretizer
+
+# Test case from the issue
+X = np.array([0, 0.5, 2, 3, 9, 10]).reshape(-1, 1)
+
+# with 5 bins
+est = KBinsDiscretizer(n_bins=5, strategy='kmeans', encode='ordinal')
+
+print("Testing original code...")
+try:
+    Xt = est.fit_transform(X)
+    print("No error occurred")
+    print(f"Transformed data: {Xt.ravel()}")
+except ValueError as e:
+    print(f"Error: {e}")
+    
+# Let's check the bin edges after fit
+est.fit(X)
+print(f"\nBin edges: {est.bin_edges_[0]}")
+print(f"Are bin edges sorted? {np.all(np.diff(est.bin_edges_[0]) >= 0)}")
