{"instance_id": "scikit-learn__scikit-learn-13496", "score": "C", "diff": "diff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd3625..c665370ab 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -120,6 +120,10 @@ class IsolationForest(BaseBagging, OutlierMixin):\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n \n     Attributes\n     ----------\n@@ -173,7 +177,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -187,7 +192,8 @@ class IsolationForest(BaseBagging, OutlierMixin):\n             max_features=max_features,\n             n_jobs=n_jobs,\n             random_state=random_state,\n-            verbose=verbose)\n+            verbose=verbose,\n+            warm_start=warm_start)\n \n         self.behaviour = behaviour\n         self.contamination = contamination\ndiff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py\nindex 67ba2d7f9..6c1d27b2d 100644\n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -345,6 +345,52 @@ def test_iforest_chunks_works1(\n     assert mocked_get_chunk.call_count == n_predict_calls\n \n \n+@pytest.mark.filterwarnings('ignore:default contamination')\n+@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')\n+def test_iforest_warm_start():\n+    \"\"\"Test IsolationForest warm_start implementation.\"\"\"\n+    X = iris.data\n+    # Use same random state to ensure reproducibility\n+    rng = check_random_state(42)\n+    \n+    # Fit a forest with warm_start=True, then increase n_estimators\n+    clf_ws = IsolationForest(n_estimators=50, random_state=rng,\n+                            warm_start=True)  \n+    clf_ws.fit(X)\n+    assert len(clf_ws.estimators_) == 50\n+    \n+    # Store decision function for first 50 trees\n+    decision_50 = clf_ws.decision_function(X)\n+    \n+    # Add 50 more estimators  \n+    clf_ws.set_params(n_estimators=100)\n+    clf_ws.fit(X)\n+    assert len(clf_ws.estimators_) == 100\n+    \n+    # Store decision function for 100 trees\n+    decision_100 = clf_ws.decision_function(X)\n+    \n+    # Decision function should have changed after adding trees\n+    assert not np.allclose(decision_50, decision_100)\n+    \n+    # Fit a new forest directly with 100 estimators\n+    clf_no_ws = IsolationForest(n_estimators=100, random_state=rng,\n+                                warm_start=False)\n+    clf_no_ws.fit(X)\n+    \n+    # Check that both have the same number of estimators\n+    assert len(clf_no_ws.estimators_) == len(clf_ws.estimators_)\n+    \n+    # Test that reducing n_estimators raises an error  \n+    clf_ws.set_params(n_estimators=10)\n+    assert_raises(ValueError, clf_ws.fit, X)\n+    \n+    # Test that warm start with the same n_estimators warns\n+    clf_ws.set_params(n_estimators=100)\n+    with pytest.warns(UserWarning, match=\"Warm-start fitting without\"):\n+        clf_ws.fit(X)\n+\n+\n # idem with chunk_size = 5 rows\n @patch(\n     \"sklearn.ensemble.iforest.get_chunk_n_rows\",\n", "gold_standard_diff": "diff --git a/doc/modules/outlier_detection.rst b/doc/modules/outlier_detection.rst\nindex b27b0c8a59643..c061feb0b1d7c 100644\n--- a/doc/modules/outlier_detection.rst\n+++ b/doc/modules/outlier_detection.rst\n@@ -252,6 +252,19 @@ This algorithm is illustrated below.\n    :align: center\n    :scale: 75%\n \n+.. _iforest_warm_start:\n+\n+The :class:`ensemble.IsolationForest` supports ``warm_start=True`` which\n+allows you to add more trees to an already fitted model::\n+\n+  >>> from sklearn.ensemble import IsolationForest\n+  >>> import numpy as np\n+  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\n+  >>> clf = IsolationForest(n_estimators=10, warm_start=True)\n+  >>> clf.fit(X)  # fit 10 trees  # doctest: +SKIP\n+  >>> clf.set_params(n_estimators=20)  # add 10 more trees  # doctest: +SKIP\n+  >>> clf.fit(X)  # fit the added trees  # doctest: +SKIP\n+\n .. topic:: Examples:\n \n    * See :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py` for\ndiff --git a/doc/whats_new/v0.21.rst b/doc/whats_new/v0.21.rst\nindex d9ae08ed5342e..cdb6b0960f535 100644\n--- a/doc/whats_new/v0.21.rst\n+++ b/doc/whats_new/v0.21.rst\n@@ -158,6 +158,10 @@ Support for Python 3.4 and below has been officially dropped.\n - |Enhancement| Minimized the validation of X in\n   :class:`ensemble.AdaBoostClassifier` and :class:`ensemble.AdaBoostRegressor`\n   :issue:`13174` by :user:`Christos Aridas <chkoar>`.\n+  \n+- |Enhancement| :class:`ensemble.IsolationForest` now exposes ``warm_start``\n+  parameter, allowing iterative addition of trees to an isolation \n+  forest. :issue:`13496` by :user:`Peter Marko <petibear>`.\n \n - |Efficiency| Make :class:`ensemble.IsolationForest` more memory efficient\n   by avoiding keeping in memory each tree prediction. :issue:`13260` by\ndiff --git a/sklearn/ensemble/iforest.py b/sklearn/ensemble/iforest.py\nindex 8a1bd36259e48..eac3519c7d9e2 100644\n--- a/sklearn/ensemble/iforest.py\n+++ b/sklearn/ensemble/iforest.py\n@@ -120,6 +120,12 @@ class IsolationForest(BaseBagging, OutlierMixin):\n     verbose : int, optional (default=0)\n         Controls the verbosity of the tree building process.\n \n+    warm_start : bool, optional (default=False)\n+        When set to ``True``, reuse the solution of the previous call to fit\n+        and add more estimators to the ensemble, otherwise, just fit a whole\n+        new forest. See :term:`the Glossary <warm_start>`.\n+\n+        .. versionadded:: 0.21\n \n     Attributes\n     ----------\n@@ -173,7 +179,8 @@ def __init__(self,\n                  n_jobs=None,\n                  behaviour='old',\n                  random_state=None,\n-                 verbose=0):\n+                 verbose=0,\n+                 warm_start=False):\n         super().__init__(\n             base_estimator=ExtraTreeRegressor(\n                 max_features=1,\n@@ -185,6 +192,7 @@ def __init__(self,\n             n_estimators=n_estimators,\n             max_samples=max_samples,\n             max_features=max_features,\n+            warm_start=warm_start,\n             n_jobs=n_jobs,\n             random_state=random_state,\n             verbose=verbose)\ndiff --git a/sklearn/ensemble/tests/test_iforest.py b/sklearn/ensemble/tests/test_iforest.py\nindex 67ba2d7f933e3..9ab679ef66e05 100644\n--- a/sklearn/ensemble/tests/test_iforest.py\n+++ b/sklearn/ensemble/tests/test_iforest.py\n@@ -295,6 +295,28 @@ def test_score_samples():\n                        clf2.score_samples([[2., 2.]]))\n \n \n+@pytest.mark.filterwarnings('ignore:default contamination')\n+@pytest.mark.filterwarnings('ignore:behaviour=\"old\"')\n+def test_iforest_warm_start():\n+    \"\"\"Test iterative addition of iTrees to an iForest \"\"\"\n+\n+    rng = check_random_state(0)\n+    X = rng.randn(20, 2)\n+\n+    # fit first 10 trees\n+    clf = IsolationForest(n_estimators=10, max_samples=20,\n+                          random_state=rng, warm_start=True)\n+    clf.fit(X)\n+    # remember the 1st tree\n+    tree_1 = clf.estimators_[0]\n+    # fit another 10 trees\n+    clf.set_params(n_estimators=20)\n+    clf.fit(X)\n+    # expecting 20 fitted trees and no overwritten trees\n+    assert len(clf.estimators_) == 20\n+    assert clf.estimators_[0] is tree_1\n+\n+\n @pytest.mark.filterwarnings('ignore:default contamination')\n @pytest.mark.filterwarnings('ignore:behaviour=\"old\"')\n def test_deprecation():\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/13496"}