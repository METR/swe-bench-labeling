{"instance_id": "pytest-dev__pytest-7490", "score": "C", "diff": "diff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996..be260a33f 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -262,6 +262,9 @@ def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\n     outcome = yield\n     rep = outcome.get_result()\n     xfailed = item._store.get(xfailed_key, None)\n+    # Re-evaluate xfail marks to catch dynamically added markers during test execution\n+    if call.when == \"call\" and not item.config.option.runxfail:\n+        xfailed = evaluate_xfail_marks(item)\n     # unittest special case, see setting of unexpectedsuccess_key\n     if unexpectedsuccess_key in item._store and rep.when == \"call\":\n         reason = item._store[unexpectedsuccess_key]\ndiff --git a/test_both_methods.py b/test_both_methods.py\nnew file mode 100644\nindex 000000000..3aaee9751\n--- /dev/null\n+++ b/test_both_methods.py\n@@ -0,0 +1,14 @@\n+import pytest\n+\n+\n+def test_add_marker_method(request):\n+    \"\"\"Test using request.node.add_marker (the method in the issue)\"\"\"\n+    mark = pytest.mark.xfail(reason=\"dynamically added via add_marker\")\n+    request.node.add_marker(mark)\n+    assert 0\n+\n+\n+def test_applymarker_method(request):\n+    \"\"\"Test using request.applymarker (the method in existing tests)\"\"\"\n+    request.applymarker(pytest.mark.xfail(reason=\"dynamically added via applymarker\"))\n+    assert 0\ndiff --git a/test_dynamic_xfail.py b/test_dynamic_xfail.py\nnew file mode 100644\nindex 000000000..2b477f1ee\n--- /dev/null\n+++ b/test_dynamic_xfail.py\n@@ -0,0 +1,7 @@\n+import pytest\n+\n+\n+def test_xfail_test(request):\n+    mark = pytest.mark.xfail(reason=\"xfail\")\n+    request.node.add_marker(mark)\n+    assert 0\ndiff --git a/test_dynamic_xfail_comprehensive.py b/test_dynamic_xfail_comprehensive.py\nnew file mode 100644\nindex 000000000..c460b2a07\n--- /dev/null\n+++ b/test_dynamic_xfail_comprehensive.py\n@@ -0,0 +1,57 @@\n+import pytest\n+\n+\n+# Test 1: Basic dynamic xfail during test execution\n+def test_dynamic_xfail_basic(request):\n+    mark = pytest.mark.xfail(reason=\"dynamically added xfail\")\n+    request.node.add_marker(mark)\n+    assert 0\n+\n+\n+# Test 2: Dynamic xfail with strict=True\n+def test_dynamic_xfail_strict(request):\n+    mark = pytest.mark.xfail(reason=\"strict xfail\", strict=True)\n+    request.node.add_marker(mark)\n+    # This should pass, but since strict=True, it should be marked as XPASS(strict) -> failed\n+    assert 1\n+\n+\n+# Test 3: Dynamic xfail with raises parameter\n+def test_dynamic_xfail_raises(request):\n+    mark = pytest.mark.xfail(reason=\"expecting ValueError\", raises=ValueError)\n+    request.node.add_marker(mark)\n+    raise ValueError(\"Expected error\")\n+\n+\n+# Test 4: Dynamic xfail with wrong exception (should fail)\n+def test_dynamic_xfail_wrong_exception(request):\n+    mark = pytest.mark.xfail(reason=\"expecting ValueError\", raises=ValueError)\n+    request.node.add_marker(mark)\n+    raise TypeError(\"Wrong error type\")\n+\n+\n+# Test 5: Static xfail should still work\n+@pytest.mark.xfail(reason=\"static xfail\")\n+def test_static_xfail():\n+    assert 0\n+\n+\n+# Test 6: No xfail marker, should fail normally\n+def test_no_xfail():\n+    assert 0\n+\n+\n+# Test 7: Dynamic xfail that unexpectedly passes\n+def test_dynamic_xfail_unexpected_pass(request):\n+    mark = pytest.mark.xfail(reason=\"expected to fail\")\n+    request.node.add_marker(mark)\n+    assert 1  # This passes, so it should be marked as XPASS\n+\n+\n+# Test 8: Combination of static and dynamic xfail (dynamic should not override)\n+@pytest.mark.xfail(reason=\"static xfail\", strict=True)\n+def test_static_and_dynamic_xfail(request):\n+    # Adding another xfail dynamically - the static one should take precedence\n+    mark = pytest.mark.xfail(reason=\"dynamic xfail\", strict=False)\n+    request.node.add_marker(mark)\n+    assert 1  # Should be XPASS(strict) due to static marker\ndiff --git a/test_edge_cases.py b/test_edge_cases.py\nnew file mode 100644\nindex 000000000..6b191c977\n--- /dev/null\n+++ b/test_edge_cases.py\n@@ -0,0 +1,30 @@\n+import pytest\n+\n+\n+# Test adding xfail in a fixture\n+@pytest.fixture\n+def xfail_fixture(request):\n+    request.node.add_marker(pytest.mark.xfail(reason=\"xfail from fixture\"))\n+    return \"fixture_value\"\n+\n+\n+def test_xfail_from_fixture(xfail_fixture):\n+    \"\"\"Xfail marker added via fixture\"\"\"\n+    assert 0\n+\n+\n+# Test conditional xfail\n+def test_conditional_xfail(request):\n+    \"\"\"Add xfail conditionally based on some runtime condition\"\"\"\n+    runtime_condition = True  # Simulate a runtime condition\n+    if runtime_condition:\n+        request.node.add_marker(pytest.mark.xfail(reason=\"runtime condition met\"))\n+    assert 0\n+\n+\n+# Test multiple xfail markers\n+def test_multiple_xfails(request):\n+    \"\"\"Add multiple xfail markers - last one should take effect\"\"\"\n+    request.node.add_marker(pytest.mark.xfail(reason=\"first xfail\"))\n+    request.node.add_marker(pytest.mark.xfail(reason=\"second xfail\", strict=True))\n+    assert 1  # This passes, so with strict=True it should fail\ndiff --git a/test_issue_example.py b/test_issue_example.py\nnew file mode 100644\nindex 000000000..61892ea7d\n--- /dev/null\n+++ b/test_issue_example.py\n@@ -0,0 +1,8 @@\n+# Exact test from the issue\n+import pytest\n+\n+\n+def test_xfail_test(request):\n+    mark = pytest.mark.xfail(reason=\"xfail\")\n+    request.node.add_marker(mark)\n+    assert 0\n", "gold_standard_diff": "From 78f2dc08fa998622bb52edd992e04e09918712da Mon Sep 17 00:00:00 2001\nFrom: Ran Benita <ran@unusedvar.com>\nDate: Mon, 13 Jul 2020 23:17:39 +0300\nSubject: [PATCH 1/2] skipping: slight simplification\n\n---\n src/_pytest/skipping.py | 4 +---\n 1 file changed, 1 insertion(+), 3 deletions(-)\n\ndiff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 335e10996a2..24c89eb6dea 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -231,11 +231,9 @@ def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:\n \n @hookimpl(tryfirst=True)\n def pytest_runtest_setup(item: Item) -> None:\n-    item._store[skipped_by_mark_key] = False\n-\n     skipped = evaluate_skip_marks(item)\n+    item._store[skipped_by_mark_key] = skipped is not None\n     if skipped:\n-        item._store[skipped_by_mark_key] = True\n         skip(skipped.reason)\n \n     if not item.config.option.runxfail:\n\nFrom ccad10a82908d7a12cd6024e00be11af413edf1c Mon Sep 17 00:00:00 2001\nFrom: Ran Benita <ran@unusedvar.com>\nDate: Mon, 13 Jul 2020 21:34:07 +0300\nSubject: [PATCH 2/2] skipping: fix dynamic xfail mark added in runtest not\n respected\n\nIf a test runtest phase (not setup) dynamically adds a pytest.mark.xfail\nmark to the item, it should be respected, but it wasn't. This regressed\nin 3e6fe92b7ea3c120e8024a970bf37a7c6c137714 (not released).\n\nFix it by just always refreshing the mark if needed. This is mostly what\nwas done before but in a more roundabout way.\n---\n src/_pytest/skipping.py  | 17 ++++++++++-------\n testing/test_skipping.py | 28 ++++++++++++++++++++++++++++\n 2 files changed, 38 insertions(+), 7 deletions(-)\n\ndiff --git a/src/_pytest/skipping.py b/src/_pytest/skipping.py\nindex 24c89eb6dea..e333e78df9b 100644\n--- a/src/_pytest/skipping.py\n+++ b/src/_pytest/skipping.py\n@@ -236,10 +236,9 @@ def pytest_runtest_setup(item: Item) -> None:\n     if skipped:\n         skip(skipped.reason)\n \n-    if not item.config.option.runxfail:\n-        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n-        if xfailed and not xfailed.run:\n-            xfail(\"[NOTRUN] \" + xfailed.reason)\n+    item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+    if xfailed and not item.config.option.runxfail and not xfailed.run:\n+        xfail(\"[NOTRUN] \" + xfailed.reason)\n \n \n @hookimpl(hookwrapper=True)\n@@ -248,12 +247,16 @@ def pytest_runtest_call(item: Item) -> Generator[None, None, None]:\n     if xfailed is None:\n         item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n \n-    if not item.config.option.runxfail:\n-        if xfailed and not xfailed.run:\n-            xfail(\"[NOTRUN] \" + xfailed.reason)\n+    if xfailed and not item.config.option.runxfail and not xfailed.run:\n+        xfail(\"[NOTRUN] \" + xfailed.reason)\n \n     yield\n \n+    # The test run may have added an xfail mark dynamically.\n+    xfailed = item._store.get(xfailed_key, None)\n+    if xfailed is None:\n+        item._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)\n+\n \n @hookimpl(hookwrapper=True)\n def pytest_runtest_makereport(item: Item, call: CallInfo[None]):\ndiff --git a/testing/test_skipping.py b/testing/test_skipping.py\nindex 8fceb37aa71..61de0b3e177 100644\n--- a/testing/test_skipping.py\n+++ b/testing/test_skipping.py\n@@ -1,6 +1,7 @@\n import sys\n \n import pytest\n+from _pytest.pytester import Testdir\n from _pytest.runner import runtestprotocol\n from _pytest.skipping import evaluate_skip_marks\n from _pytest.skipping import evaluate_xfail_marks\n@@ -425,6 +426,33 @@ def test_this2(arg):\n         result = testdir.runpytest(p)\n         result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n \n+    def test_dynamic_xfail_set_during_runtest_failed(self, testdir: Testdir) -> None:\n+        # Issue #7486.\n+        p = testdir.makepyfile(\n+            \"\"\"\n+            import pytest\n+            def test_this(request):\n+                request.node.add_marker(pytest.mark.xfail(reason=\"xfail\"))\n+                assert 0\n+        \"\"\"\n+        )\n+        result = testdir.runpytest(p)\n+        result.assert_outcomes(xfailed=1)\n+\n+    def test_dynamic_xfail_set_during_runtest_passed_strict(\n+        self, testdir: Testdir\n+    ) -> None:\n+        # Issue #7486.\n+        p = testdir.makepyfile(\n+            \"\"\"\n+            import pytest\n+            def test_this(request):\n+                request.node.add_marker(pytest.mark.xfail(reason=\"xfail\", strict=True))\n+        \"\"\"\n+        )\n+        result = testdir.runpytest(p)\n+        result.assert_outcomes(failed=1)\n+\n     @pytest.mark.parametrize(\n         \"expected, actual, matchline\",\n         [\n", "gold_standard_pr_link": "https://github.com/pytest-dev/pytest/pull/7490"}