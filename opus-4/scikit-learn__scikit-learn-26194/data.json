{"instance_id": "scikit-learn__scikit-learn-26194", "score": "I", "diff": "diff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead923..e9fed0b49 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1084,6 +1084,11 @@ def roc_curve(\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n     thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    \n+    # When y_score contains probability estimates (all values in [0, 1]),\n+    # clip the artificial first threshold to 1 to maintain semantic meaning\n+    if len(thresholds) > 1 and np.all((thresholds[1:] >= 0) & (thresholds[1:] <= 1)):\n+        thresholds[0] = np.clip(thresholds[0], None, 1)\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c3..3f21356b8 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,13 +418,24 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+    # When y_score contains probability estimates (all in [0,1]), \n+    # the first threshold is capped at 1.0\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+    # When y_score contains probability estimates (all in [0,1]), \n+    # the first threshold is capped at 1.0\n+    assert_array_almost_equal(thresholds, [1.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+    \n+    # Test with scores outside [0, 1] range (e.g., decision function output)\n+    y_true = [0, 0, 1, 1]\n+    y_score = [-2.0, -1.0, 1.0, 2.0]\n+    tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n+    # When y_score contains non-probability values, no capping occurs\n+    assert_array_almost_equal(thresholds, [3.0, 2.0, 1.0, -2.0])\n \n \n def test_roc_curve_fpr_tpr_increasing():\n@@ -439,6 +450,23 @@ def test_roc_curve_fpr_tpr_increasing():\n     assert (np.diff(tpr) < 0).sum() == 0\n \n \n+def test_roc_curve_with_probability_estimates():\n+    # Test that thresholds do not exceed 1 when y_score contains probabilities\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)  # Probability estimates in [0, 1]\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    \n+    # All thresholds should be <= 1 when y_score are probabilities\n+    assert thresholds.max() <= 1, (\n+        f\"Thresholds exceed 1 with probability estimates: max={thresholds.max()}\"\n+    )\n+    # Also check that all thresholds are non-negative\n+    assert thresholds.min() >= 0, (\n+        f\"Thresholds are negative: min={thresholds.min()}\"\n+    )\n+\n+\n def test_auc():\n     # Test Area Under Curve (AUC) computation\n     x = [0, 1]\ndiff --git a/test_comprehensive_roc_fix.py b/test_comprehensive_roc_fix.py\nnew file mode 100644\nindex 000000000..63c6415c1\n--- /dev/null\n+++ b/test_comprehensive_roc_fix.py\n@@ -0,0 +1,71 @@\n+import numpy as np\n+from sklearn.metrics import roc_curve\n+\n+def test_comprehensive_roc_fix():\n+    \"\"\"Test that the fix correctly handles various cases\"\"\"\n+    \n+    # Case 1: Probability estimates in [0, 1] - should be capped at 1\n+    print(\"Case 1: Probability estimates\")\n+    y_true = [0, 0, 1, 1]\n+    y_score = [0.1, 0.4, 0.35, 0.8]\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    print(f\"  y_score: {y_score}\")\n+    print(f\"  thresholds: {thresholds}\")\n+    print(f\"  max threshold: {thresholds.max()}\")\n+    assert thresholds.max() <= 1, f\"Expected max threshold <= 1, got {thresholds.max()}\"\n+    \n+    # Case 2: Decision function scores outside [0, 1] - should NOT be capped\n+    print(\"\\nCase 2: Decision function scores\")\n+    y_true = [0, 0, 1, 1]\n+    y_score = [-0.5, 0.5, 1.5, 2.5]\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    print(f\"  y_score: {y_score}\")\n+    print(f\"  thresholds: {thresholds}\")\n+    print(f\"  max threshold: {thresholds.max()}\")\n+    assert thresholds.max() > 1, f\"Expected max threshold > 1, got {thresholds.max()}\"\n+    \n+    # Case 3: Edge case - all scores are 0\n+    print(\"\\nCase 3: All scores are 0\")\n+    y_true = [0, 0, 1, 1]\n+    y_score = [0.0, 0.0, 0.0, 0.0]\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    print(f\"  y_score: {y_score}\")\n+    print(f\"  thresholds: {thresholds}\")\n+    print(f\"  max threshold: {thresholds.max()}\")\n+    assert thresholds.max() <= 1, f\"Expected max threshold <= 1, got {thresholds.max()}\"\n+    \n+    # Case 4: Edge case - all scores are 1\n+    print(\"\\nCase 4: All scores are 1\")\n+    y_true = [0, 0, 1, 1]\n+    y_score = [1.0, 1.0, 1.0, 1.0]\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    print(f\"  y_score: {y_score}\")\n+    print(f\"  thresholds: {thresholds}\")\n+    print(f\"  max threshold: {thresholds.max()}\")\n+    # In this case, the first threshold would be 1 + 1 = 2, but should be capped at 1\n+    assert thresholds.max() <= 1, f\"Expected max threshold <= 1, got {thresholds.max()}\"\n+    \n+    # Case 5: Mixed but still in [0, 1]\n+    print(\"\\nCase 5: Mixed probability scores\")\n+    y_true = [1, 0, 0, 1, 0, 1, 0, 1]\n+    y_score = [0.9, 0.1, 0.2, 0.8, 0.3, 0.7, 0.4, 0.6]\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    print(f\"  y_score: {y_score}\")\n+    print(f\"  thresholds: {thresholds}\")\n+    print(f\"  max threshold: {thresholds.max()}\")\n+    assert thresholds.max() <= 1, f\"Expected max threshold <= 1, got {thresholds.max()}\"\n+    \n+    # Case 6: Some scores exactly at boundaries\n+    print(\"\\nCase 6: Scores at boundaries\")\n+    y_true = [0, 1, 0, 1]\n+    y_score = [0.0, 1.0, 0.5, 0.5]\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    print(f\"  y_score: {y_score}\")\n+    print(f\"  thresholds: {thresholds}\")\n+    print(f\"  max threshold: {thresholds.max()}\")\n+    assert thresholds.max() <= 1, f\"Expected max threshold <= 1, got {thresholds.max()}\"\n+    \n+    print(\"\\nAll tests passed!\")\n+\n+if __name__ == \"__main__\":\n+    test_comprehensive_roc_fix()\ndiff --git a/test_roc_curve_correctness.py b/test_roc_curve_correctness.py\nnew file mode 100644\nindex 000000000..8b1378917\n--- /dev/null\n+++ b/test_roc_curve_correctness.py\n@@ -0,0 +1 @@\n+\ndiff --git a/test_roc_threshold_issue.py b/test_roc_threshold_issue.py\nnew file mode 100644\nindex 000000000..ccbf0869b\n--- /dev/null\n+++ b/test_roc_threshold_issue.py\n@@ -0,0 +1,21 @@\n+import numpy as np\n+from sklearn.metrics import roc_curve\n+\n+def test_roc_curve_with_probability_estimates():\n+    rng = np.random.RandomState(42)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n+    \n+    print(\"y_true:\", y_true)\n+    print(\"y_score:\", y_score)\n+    print(\"thresholds:\", thresholds)\n+    print(\"max threshold:\", thresholds.max())\n+    print(\"min threshold:\", thresholds.min())\n+    \n+    # Check if thresholds are within [0, 1] range\n+    assert np.logical_and(thresholds <= 1, thresholds >= 0).all(), \\\n+        f\"Thresholds out of range [0, 1]: min={thresholds.min()}, max={thresholds.max()}\"\n+\n+if __name__ == \"__main__\":\n+    test_roc_curve_with_probability_estimates()\n", "gold_standard_diff": "diff --git a/doc/modules/model_evaluation.rst b/doc/modules/model_evaluation.rst\nindex 537f23e49d2dc..c0b0ab67de786 100644\n--- a/doc/modules/model_evaluation.rst\n+++ b/doc/modules/model_evaluation.rst\n@@ -1366,7 +1366,7 @@ function::\n     >>> tpr\n     array([0. , 0.5, 0.5, 1. , 1. ])\n     >>> thresholds\n-    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n+    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\n \n Compared to metrics such as the subset accuracy, the Hamming loss, or the\n F1 score, ROC doesn't require optimizing a threshold for each label.\ndiff --git a/doc/whats_new/v1.3.rst b/doc/whats_new/v1.3.rst\nindex e47c74a54edd6..6a5c0812a8622 100644\n--- a/doc/whats_new/v1.3.rst\n+++ b/doc/whats_new/v1.3.rst\n@@ -366,6 +366,11 @@ Changelog\n - |API| The `eps` parameter of the :func:`log_loss` has been deprecated and will be\n   removed in 1.5. :pr:`25299` by :user:`Omar Salman <OmarManzoor>`.\n \n+- |Fix| In :func:`metrics.roc_curve`, use the threshold value `np.inf` instead of\n+  arbritrary `max(y_score) + 1`. This threshold is associated with the ROC curve point\n+  `tpr=0` and `fpr=0`.\n+  :pr:`26194` by :user:`Guillaume Lemaitre <glemaitre>`.\n+\n :mod:`sklearn.model_selection`\n ..............................\n \ndiff --git a/sklearn/metrics/_ranking.py b/sklearn/metrics/_ranking.py\nindex 0cdead9233898..7a3b7f0cc2663 100644\n--- a/sklearn/metrics/_ranking.py\n+++ b/sklearn/metrics/_ranking.py\n@@ -1016,10 +1016,10 @@ def roc_curve(\n         Increasing true positive rates such that element `i` is the true\n         positive rate of predictions with score >= `thresholds[i]`.\n \n-    thresholds : ndarray of shape = (n_thresholds,)\n+    thresholds : ndarray of shape (n_thresholds,)\n         Decreasing thresholds on the decision function used to compute\n         fpr and tpr. `thresholds[0]` represents no instances being predicted\n-        and is arbitrarily set to `max(y_score) + 1`.\n+        and is arbitrarily set to `np.inf`.\n \n     See Also\n     --------\n@@ -1036,6 +1036,10 @@ def roc_curve(\n     are reversed upon returning them to ensure they correspond to both ``fpr``\n     and ``tpr``, which are sorted in reversed order during their calculation.\n \n+    An arbritrary threshold is added for the case `tpr=0` and `fpr=0` to\n+    ensure that the curve starts at `(0, 0)`. This threshold corresponds to the\n+    `np.inf`.\n+\n     References\n     ----------\n     .. [1] `Wikipedia entry for the Receiver operating characteristic\n@@ -1056,7 +1060,7 @@ def roc_curve(\n     >>> tpr\n     array([0. , 0.5, 0.5, 1. , 1. ])\n     >>> thresholds\n-    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n+    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\n     \"\"\"\n     fps, tps, thresholds = _binary_clf_curve(\n         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight\n@@ -1083,7 +1087,8 @@ def roc_curve(\n     # to make sure that the curve starts at (0, 0)\n     tps = np.r_[0, tps]\n     fps = np.r_[0, fps]\n-    thresholds = np.r_[thresholds[0] + 1, thresholds]\n+    # get dtype of `y_score` even if it is an array-like\n+    thresholds = np.r_[np.inf, thresholds]\n \n     if fps[-1] <= 0:\n         warnings.warn(\ndiff --git a/sklearn/metrics/tests/test_ranking.py b/sklearn/metrics/tests/test_ranking.py\nindex f38f118c38c0a..d40e48456fe9d 100644\n--- a/sklearn/metrics/tests/test_ranking.py\n+++ b/sklearn/metrics/tests/test_ranking.py\n@@ -418,13 +418,13 @@ def test_roc_curve_drop_intermediate():\n     y_true = [0, 0, 0, 0, 1, 1]\n     y_score = [0.0, 0.2, 0.5, 0.6, 0.7, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.7, 0.0])\n+    assert_array_almost_equal(thresholds, [np.inf, 1.0, 0.7, 0.0])\n \n     # Test dropping thresholds with repeating scores\n     y_true = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n     y_score = [0.0, 0.1, 0.6, 0.6, 0.7, 0.8, 0.9, 0.6, 0.7, 0.8, 0.9, 0.9, 1.0]\n     tpr, fpr, thresholds = roc_curve(y_true, y_score, drop_intermediate=True)\n-    assert_array_almost_equal(thresholds, [2.0, 1.0, 0.9, 0.7, 0.6, 0.0])\n+    assert_array_almost_equal(thresholds, [np.inf, 1.0, 0.9, 0.7, 0.6, 0.0])\n \n \n def test_roc_curve_fpr_tpr_increasing():\n@@ -2199,3 +2199,17 @@ def test_ranking_metric_pos_label_types(metric, classes):\n         assert not np.isnan(metric_1).any()\n         assert not np.isnan(metric_2).any()\n         assert not np.isnan(thresholds).any()\n+\n+\n+def test_roc_curve_with_probablity_estimates(global_random_seed):\n+    \"\"\"Check that thresholds do not exceed 1.0 when `y_score` is a probability\n+    estimate.\n+\n+    Non-regression test for:\n+    https://github.com/scikit-learn/scikit-learn/issues/26193\n+    \"\"\"\n+    rng = np.random.RandomState(global_random_seed)\n+    y_true = rng.randint(0, 2, size=10)\n+    y_score = rng.rand(10)\n+    _, _, thresholds = roc_curve(y_true, y_score)\n+    assert np.isinf(thresholds[0])\n", "gold_standard_pr_link": "https://github.com/scikit-learn/scikit-learn/pull/26194"}